2025-05-04 13:09:06,815 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:09:06,819 - __main__ - INFO - RAM: 使用中 12.8GB / 95.4GB (13.4%)
2025-05-04 13:09:06,892 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:09:06,892 - __main__ - INFO - Using device: cuda
2025-05-04 13:09:06,892 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:09:06,893 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:09:06,893 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:09:06,893 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:16:02,776 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:16:02,787 - __main__ - INFO - RAM: 使用中 13.7GB / 95.4GB (14.4%)
2025-05-04 13:16:02,830 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:16:02,831 - __main__ - INFO - Using device: cuda
2025-05-04 13:16:02,831 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:16:02,832 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:16:02,832 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:16:02,832 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:16:03,721 - __main__ - INFO - Initializing student model
2025-05-04 13:16:04,650 - distillation - INFO - RAM使用状況: 15.5% (使用中: 14.8GB, 空き: 80.6GB)
2025-05-04 13:16:04,651 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:16:04,651 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:16:04,653 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:16:06,061 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:29:48,824 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:29:48,835 - __main__ - INFO - RAM: 使用中 14.3GB / 95.4GB (15.0%)
2025-05-04 13:29:48,836 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:29:48,836 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:29:56,484 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:29:56,484 - __main__ - INFO - Using device: cpu
2025-05-04 13:29:56,485 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:29:56,978 - __main__ - INFO - Initializing student model
2025-05-04 13:29:57,810 - distillation - INFO - RAM使用状況: 16.0% (使用中: 15.2GB, 空き: 80.2GB)
2025-05-04 13:29:57,811 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:29:57,812 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:29:57,869 - xformers - WARNING - WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.6.0+cpu)
    Python  3.10.11 (you have 3.10.6)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-05-04 13:30:01,264 - distillation - INFO - xformersが使用可能です
2025-05-04 13:30:01,265 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:30:01,273 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:30:01,274 - distillation - INFO - Qwen2モデル用にxformersでパフォーマンス最適化
2025-05-04 13:30:02,057 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
operator torchvision::nms does not exist
2025-05-04 13:30:02,058 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
operator torchvision::nms does not exist
2025-05-04 13:42:02,715 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:42:02,720 - __main__ - INFO - RAM: 使用中 14.7GB / 95.4GB (15.4%)
2025-05-04 13:42:02,789 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:42:02,789 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:42:02,790 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:42:20,169 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:42:20,169 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:42:20,169 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-04 13:42:20,170 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-04 13:42:23,020 - __main__ - INFO - Using device: cuda
2025-05-04 13:42:23,020 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:42:23,020 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:42:23,020 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:42:23,020 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:42:24,009 - __main__ - INFO - Initializing student model
2025-05-04 13:42:24,888 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:42:24,899 - distillation - INFO - RAM使用状況: 16.4% (使用中: 15.6GB, 空き: 79.8GB)
2025-05-04 13:42:24,900 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:42:24,900 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 13:42:24,906 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:42:29,070 - distillation - INFO - xformersが使用可能です
2025-05-04 13:42:29,071 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:42:29,080 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:42:29,513 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:42:29,513 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 13:42:29,514 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 13:42:29,515 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 13:42:29,515 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:42:29,516 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:42:29,517 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 13:42:59,736 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:42:59,736 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 13:42:59,736 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 13:43:05,919 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 13:43:05,920 - __main__ - INFO - 代替モデルの候補:
2025-05-04 13:43:05,920 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:43:05,920 - __main__ - INFO - 2. meta-llama/Llama-2-7b-hf
2025-05-04 13:43:05,920 - __main__ - INFO - 3. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:43:16,861 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:43:16,867 - __main__ - INFO - RAM: 使用中 14.6GB / 95.4GB (15.3%)
2025-05-04 13:43:16,926 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:43:16,926 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:43:16,927 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:43:19,199 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:43:19,199 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:43:19,199 - __main__ - INFO - Using device: cuda
2025-05-04 13:43:19,200 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:43:19,200 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:43:19,200 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:43:19,200 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:43:19,709 - __main__ - INFO - Initializing student model
2025-05-04 13:43:20,576 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:43:20,580 - distillation - INFO - RAM使用状況: 16.4% (使用中: 15.6GB, 空き: 79.7GB)
2025-05-04 13:43:20,580 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:43:20,581 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 13:43:20,581 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:43:21,804 - distillation - INFO - xformersが使用可能です
2025-05-04 13:43:21,804 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:43:21,810 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:43:22,202 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:43:22,202 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 13:43:22,203 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 13:43:22,203 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 13:43:22,204 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:43:22,204 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:43:22,204 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 13:45:59,750 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:45:59,751 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 13:45:59,751 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 13:46:01,458 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 13:46:01,458 - __main__ - INFO - 代替モデルの候補:
2025-05-04 13:46:01,458 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:46:01,458 - __main__ - INFO - 2. meta-llama/Llama-2-7b-hf
2025-05-04 13:46:01,458 - __main__ - INFO - 3. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:54:20,894 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:54:20,898 - __main__ - INFO - RAM: 使用中 15.5GB / 95.4GB (16.2%)
2025-05-04 13:54:20,925 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:54:20,925 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:54:20,926 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:54:34,799 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:54:34,799 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:54:34,799 - __main__ - INFO - Using device: cuda
2025-05-04 13:54:34,800 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:54:34,800 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:54:34,800 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:54:34,800 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:54:35,492 - __main__ - INFO - Initializing student model
2025-05-04 13:54:36,394 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:54:36,399 - distillation - INFO - RAM使用状況: 17.3% (使用中: 16.5GB, 空き: 78.9GB)
2025-05-04 13:54:36,399 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:54:36,399 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 13:54:36,400 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:54:37,562 - distillation - INFO - xformersが使用可能です
2025-05-04 13:54:37,562 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:54:37,568 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:54:37,955 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:54:37,955 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 13:54:37,956 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 13:54:37,957 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 13:54:37,958 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:54:37,958 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:54:37,959 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:00:10,236 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:00:10,246 - __main__ - INFO - RAM: 使用中 15.8GB / 95.4GB (16.6%)
2025-05-04 14:00:10,305 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:00:10,305 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:00:10,306 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 14:00:14,455 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 14:00:14,456 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:00:14,456 - __main__ - INFO - Using device: cuda
2025-05-04 14:00:14,456 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:00:14,456 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:00:14,456 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:00:14,457 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:00:15,291 - __main__ - INFO - Initializing student model
2025-05-04 14:00:16,128 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:00:16,141 - distillation - INFO - RAM使用状況: 17.6% (使用中: 16.8GB, 空き: 78.6GB)
2025-05-04 14:00:16,141 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:00:16,142 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:00:16,142 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:00:17,285 - distillation - INFO - xformersが使用可能です
2025-05-04 14:00:17,286 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:00:17,294 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:00:17,686 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:00:17,687 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:00:17,687 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:00:17,687 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:00:17,688 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:00:17,688 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:00:17,688 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:00:47,649 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:00:47,649 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:00:47,649 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:00:48,025 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:00:48,025 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:00:48,025 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:00:48,026 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:00:48,026 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:00:48,026 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:00:48,026 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:00:48,027 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:00:48,027 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:00:48,027 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 14:00:57,897 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:00:57,902 - __main__ - INFO - RAM: 使用中 15.7GB / 95.4GB (16.5%)
2025-05-04 14:00:57,966 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:00:57,966 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:00:57,967 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 14:01:02,685 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 14:01:02,685 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:02,686 - __main__ - INFO - Using device: cuda
2025-05-04 14:01:02,686 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:01:02,686 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:01:02,686 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:01:02,687 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:01:03,227 - __main__ - INFO - Initializing student model
2025-05-04 14:01:04,106 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:04,110 - distillation - INFO - RAM使用状況: 17.6% (使用中: 16.8GB, 空き: 78.6GB)
2025-05-04 14:01:04,111 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:01:04,111 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:01:04,112 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:01:05,280 - distillation - INFO - xformersが使用可能です
2025-05-04 14:01:05,281 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:01:05,282 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:01:05,676 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:05,676 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:01:05,677 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:01:05,677 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:01:05,678 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:05,678 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:05,679 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:01:08,356 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:08,357 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:01:08,357 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:01:10,095 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:01:10,095 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:01:10,096 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:10,096 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:01:10,096 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:01:10,096 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:01:10,096 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:01:10,096 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:01:10,097 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:10,097 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 14:01:25,878 - __main__ - INFO - 代替モデル microsoft/Phi-4-reasoning を使用します
2025-05-04 14:01:25,878 - __main__ - INFO - Phi-4-reasoning用に設定を最適化しています
2025-05-04 14:01:32,252 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:01:32,258 - __main__ - INFO - RAM: 使用中 16.9GB / 95.4GB (17.7%)
2025-05-04 14:01:32,259 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:01:32,259 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:01:32,260 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:32,260 - __main__ - INFO - Using device: cuda
2025-05-04 14:01:32,260 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:01:32,260 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:01:32,261 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:01:32,261 - __main__ - INFO - Loading tokenizer for microsoft/Phi-4-reasoning
2025-05-04 14:01:36,381 - __main__ - INFO - Initializing student model
2025-05-04 14:01:37,000 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:37,011 - distillation - INFO - RAM使用状況: 18.5% (使用中: 17.6GB, 空き: 77.8GB)
2025-05-04 14:01:37,011 - distillation - INFO - Loading teacher model: microsoft/Phi-4-reasoning
2025-05-04 14:01:37,011 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:01:37,012 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:01:37,013 - distillation - INFO - xformersが使用可能です
2025-05-04 14:01:37,013 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:01:37,015 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:01:37,520 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.phi3.modeling_phi3 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:37,520 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:01:37,520 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:01:37,520 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:01:37,522 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:37,522 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:37,522 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:01:53,566 - __main__ - ERROR - Error initializing distiller with microsoft/Phi-4-reasoning: Failed to import transformers.models.phi3.modeling_phi3 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:53,566 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:01:53,567 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:01:55,276 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:01:55,277 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:01:55,277 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:55,277 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:01:55,277 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:01:55,278 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:01:55,278 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:01:55,278 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:01:55,278 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:55,278 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 14:01:58,564 - __main__ - INFO - 代替モデル elyza/Llama-3-ELYZA-JP-8B を使用します
2025-05-04 14:01:58,564 - __main__ - INFO - Llama-3-ELYZA-JP-8B用に設定を最適化しています
2025-05-04 14:02:04,151 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:02:04,157 - __main__ - INFO - RAM: 使用中 17.6GB / 95.4GB (18.5%)
2025-05-04 14:02:04,157 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:02:04,158 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:02:04,158 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:02:04,158 - __main__ - INFO - Using device: cuda
2025-05-04 14:02:04,159 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:02:04,159 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:02:04,159 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:02:04,159 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:02:04,679 - __main__ - INFO - Initializing student model
2025-05-04 14:02:05,410 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:02:05,422 - distillation - INFO - RAM使用状況: 19.4% (使用中: 18.5GB, 空き: 76.9GB)
2025-05-04 14:02:05,423 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:02:05,424 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:02:05,424 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:02:05,425 - distillation - INFO - xformersが使用可能です
2025-05-04 14:02:05,426 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:02:05,427 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:02:05,665 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:02:05,665 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:02:05,666 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:02:05,666 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:02:05,667 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:02:05,667 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:02:05,668 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:02:15,418 - __main__ - ERROR - Error initializing distiller with elyza/Llama-3-ELYZA-JP-8B: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:02:15,419 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:02:15,419 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:02:17,484 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:02:17,484 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:02:17,484 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:02:17,484 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:02:17,484 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:02:17,485 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:02:17,485 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:02:17,485 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:02:17,485 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:02:17,485 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 23:55:20,349 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 23:55:20,362 - __main__ - INFO - RAM: 使用中 17.7GB / 95.4GB (18.5%)
2025-05-04 23:55:20,566 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 23:55:20,566 - __main__ - INFO - Windows環境を検出しました
2025-05-04 23:55:20,568 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 23:56:56,869 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 23:56:56,870 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 23:56:56,871 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-04 23:56:56,872 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-04 23:59:39,194 - __main__ - INFO - Using device: cuda
2025-05-04 23:59:39,194 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 23:59:39,194 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 23:59:39,195 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 23:59:39,195 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 23:59:39,832 - __main__ - INFO - Initializing student model
2025-05-04 23:59:40,700 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 23:59:40,705 - distillation - INFO - RAM使用状況: 19.6% (使用中: 18.7GB, 空き: 76.7GB)
2025-05-04 23:59:40,706 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 23:59:40,706 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 23:59:40,707 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 23:59:43,072 - distillation - INFO - xformersが使用可能です
2025-05-04 23:59:43,072 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 23:59:43,074 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 00:00:33,056 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 00:01:10,430 - distillation - INFO - Teacher model loaded successfully
2025-05-05 00:01:10,431 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 00:01:10,433 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 00:01:10,434 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 00:09:49,643 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 00:09:49,655 - __main__ - INFO - RAM: 使用中 17.4GB / 95.4GB (18.2%)
2025-05-05 00:09:49,720 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 00:09:49,721 - __main__ - INFO - Windows環境を検出しました
2025-05-05 00:09:49,722 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 00:10:41,676 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 00:10:41,677 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:10:41,677 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 00:10:41,677 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 00:10:58,617 - __main__ - INFO - Using device: cuda
2025-05-05 00:10:58,617 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 00:10:58,617 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 00:10:58,617 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 00:10:58,618 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:10:59,225 - __main__ - INFO - Initializing student model
2025-05-05 00:11:00,125 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:11:00,129 - distillation - INFO - RAM使用状況: 19.2% (使用中: 18.3GB, 空き: 77.0GB)
2025-05-05 00:11:00,130 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:11:00,130 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 00:11:00,131 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 00:11:01,919 - distillation - INFO - xformersが使用可能です
2025-05-05 00:11:01,920 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 00:11:01,927 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 00:11:48,156 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 00:12:27,918 - distillation - INFO - Teacher model loaded successfully
2025-05-05 00:12:27,921 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 00:12:27,922 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 00:12:27,923 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 00:16:31,872 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 00:16:31,879 - __main__ - INFO - RAM: 使用中 17.4GB / 95.4GB (18.3%)
2025-05-05 00:16:31,942 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 00:16:31,942 - __main__ - INFO - Windows環境を検出しました
2025-05-05 00:16:31,943 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 00:17:39,959 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 00:17:39,959 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:17:39,959 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 00:17:39,960 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 00:17:42,620 - __main__ - INFO - Using device: cuda
2025-05-05 00:17:42,621 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 00:17:42,621 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 00:17:42,621 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 00:17:42,621 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:17:43,225 - __main__ - INFO - Initializing student model
2025-05-05 00:17:44,137 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:17:44,142 - distillation - INFO - RAM使用状況: 19.4% (使用中: 18.5GB, 空き: 76.9GB)
2025-05-05 00:17:44,142 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:17:44,142 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 00:17:44,143 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 00:17:45,808 - distillation - INFO - xformersが使用可能です
2025-05-05 00:17:45,808 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 00:17:45,810 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 00:18:33,005 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 00:19:12,726 - distillation - INFO - Teacher model loaded successfully
2025-05-05 00:19:12,728 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 00:19:12,728 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 00:19:12,729 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 00:19:12,730 - distillation - INFO - Generating 4000 questions for distillation
2025-05-05 00:19:12,730 - distillation - INFO - Generated 0 questions
2025-05-05 00:19:12,743 - distillation - INFO - Generated 100 questions
2025-05-05 00:19:12,745 - distillation - INFO - Generated 200 questions
2025-05-05 00:19:12,752 - distillation - INFO - Generated 300 questions
2025-05-05 00:19:12,754 - distillation - INFO - Generated 400 questions
2025-05-05 00:19:12,756 - distillation - INFO - Generated 500 questions
2025-05-05 00:19:12,758 - distillation - INFO - Generated 600 questions
2025-05-05 00:19:12,760 - distillation - INFO - Generated 700 questions
2025-05-05 00:19:12,762 - distillation - INFO - Generated 800 questions
2025-05-05 00:19:12,764 - distillation - INFO - Generated 900 questions
2025-05-05 00:19:12,765 - distillation - INFO - Generated 1000 questions
2025-05-05 00:19:12,767 - distillation - INFO - Generated 1100 questions
2025-05-05 00:19:12,769 - distillation - INFO - Generated 1200 questions
2025-05-05 00:19:12,770 - distillation - INFO - Generated 1300 questions
2025-05-05 00:19:12,772 - distillation - INFO - Generated 1400 questions
2025-05-05 00:19:12,774 - distillation - INFO - Generated 1500 questions
2025-05-05 00:19:12,775 - distillation - INFO - Generated 1600 questions
2025-05-05 00:19:12,777 - distillation - INFO - Generated 1700 questions
2025-05-05 00:19:12,779 - distillation - INFO - Generated 1800 questions
2025-05-05 00:19:12,780 - distillation - INFO - Generated 1900 questions
2025-05-05 00:19:12,782 - distillation - INFO - Generated 2000 questions
2025-05-05 00:19:12,784 - distillation - INFO - Generated 2100 questions
2025-05-05 00:19:12,785 - distillation - INFO - Generated 2200 questions
2025-05-05 00:19:12,787 - distillation - INFO - Generated 2300 questions
2025-05-05 00:19:12,789 - distillation - INFO - Generated 2400 questions
2025-05-05 00:19:12,791 - distillation - INFO - Generated 2500 questions
2025-05-05 00:19:12,792 - distillation - INFO - Generated 2600 questions
2025-05-05 00:19:12,794 - distillation - INFO - Generated 2700 questions
2025-05-05 00:19:12,796 - distillation - INFO - Generated 2800 questions
2025-05-05 00:19:12,798 - distillation - INFO - Generated 2900 questions
2025-05-05 00:19:12,799 - distillation - INFO - Generated 3000 questions
2025-05-05 00:19:12,801 - distillation - INFO - Generated 3100 questions
2025-05-05 00:19:12,803 - distillation - INFO - Generated 3200 questions
2025-05-05 00:19:12,804 - distillation - INFO - Generated 3300 questions
2025-05-05 00:19:12,806 - distillation - INFO - Generated 3400 questions
2025-05-05 00:19:12,808 - distillation - INFO - Generated 3500 questions
2025-05-05 00:19:12,810 - distillation - INFO - Generated 3600 questions
2025-05-05 00:19:12,811 - distillation - INFO - Generated 3700 questions
2025-05-05 00:19:12,813 - distillation - INFO - Generated 3800 questions
2025-05-05 00:19:12,816 - distillation - INFO - Generated 3900 questions
2025-05-05 00:19:12,820 - distillation - INFO - Generated questions saved to questions.txt
2025-05-05 00:19:12,821 - distillation - INFO - Using PyTorch nightly version 2.7.0.dev20250309+cu128 for data generation
2025-05-05 00:19:12,821 - distillation - INFO - Processing batch 1/1000
2025-05-05 00:38:38,045 - distillation - INFO - Processing batch 2/1000
2025-05-05 19:14:17,645 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 19:14:17,650 - __main__ - INFO - RAM: 使用中 14.1GB / 95.4GB (14.8%)
2025-05-05 19:14:17,837 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 19:14:17,837 - __main__ - INFO - Windows環境を検出しました
2025-05-05 19:14:17,837 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 19:14:25,984 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 19:14:25,985 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:14:25,985 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 19:14:25,985 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 19:14:30,172 - __main__ - INFO - Using device: cuda
2025-05-05 19:14:30,172 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 19:14:30,172 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 19:14:30,173 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 19:14:30,173 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:14:30,741 - __main__ - INFO - Initializing student model
2025-05-05 19:14:31,582 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:14:31,588 - distillation - INFO - RAM使用状況: 15.9% (使用中: 15.2GB, 空き: 80.2GB)
2025-05-05 19:14:31,588 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:14:31,588 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 19:14:31,589 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 19:14:33,962 - distillation - INFO - xformersが使用可能です
2025-05-05 19:14:33,963 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 19:14:33,971 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 19:15:20,838 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 19:16:03,972 - distillation - INFO - Teacher model loaded successfully
2025-05-05 19:16:03,974 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 19:16:03,975 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 19:16:03,976 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 19:16:03,977 - distillation - INFO - Generating 4000 questions for distillation
2025-05-05 19:16:03,977 - distillation - INFO - Generated 0 questions
2025-05-05 19:16:03,982 - distillation - INFO - Generated 100 questions
2025-05-05 19:16:03,984 - distillation - INFO - Generated 200 questions
2025-05-05 19:16:03,986 - distillation - INFO - Generated 300 questions
2025-05-05 19:16:03,989 - distillation - INFO - Generated 400 questions
2025-05-05 19:16:03,991 - distillation - INFO - Generated 500 questions
2025-05-05 19:16:03,994 - distillation - INFO - Generated 600 questions
2025-05-05 19:16:03,995 - distillation - INFO - Generated 700 questions
2025-05-05 19:16:03,997 - distillation - INFO - Generated 800 questions
2025-05-05 19:16:04,000 - distillation - INFO - Generated 900 questions
2025-05-05 19:16:04,002 - distillation - INFO - Generated 1000 questions
2025-05-05 19:16:04,004 - distillation - INFO - Generated 1100 questions
2025-05-05 19:16:04,006 - distillation - INFO - Generated 1200 questions
2025-05-05 19:16:04,007 - distillation - INFO - Generated 1300 questions
2025-05-05 19:16:04,009 - distillation - INFO - Generated 1400 questions
2025-05-05 19:16:04,015 - distillation - INFO - Generated 1500 questions
2025-05-05 19:16:04,017 - distillation - INFO - Generated 1600 questions
2025-05-05 19:16:04,019 - distillation - INFO - Generated 1700 questions
2025-05-05 19:16:04,022 - distillation - INFO - Generated 1800 questions
2025-05-05 19:16:04,024 - distillation - INFO - Generated 1900 questions
2025-05-05 19:16:04,026 - distillation - INFO - Generated 2000 questions
2025-05-05 19:16:04,028 - distillation - INFO - Generated 2100 questions
2025-05-05 19:16:04,034 - distillation - INFO - Generated 2200 questions
2025-05-05 19:16:04,036 - distillation - INFO - Generated 2300 questions
2025-05-05 19:16:04,037 - distillation - INFO - Generated 2400 questions
2025-05-05 19:16:04,039 - distillation - INFO - Generated 2500 questions
2025-05-05 19:16:04,041 - distillation - INFO - Generated 2600 questions
2025-05-05 19:16:04,043 - distillation - INFO - Generated 2700 questions
2025-05-05 19:16:04,045 - distillation - INFO - Generated 2800 questions
2025-05-05 19:16:04,046 - distillation - INFO - Generated 2900 questions
2025-05-05 19:16:04,048 - distillation - INFO - Generated 3000 questions
2025-05-05 19:16:04,050 - distillation - INFO - Generated 3100 questions
2025-05-05 19:16:04,052 - distillation - INFO - Generated 3200 questions
2025-05-05 19:16:04,054 - distillation - INFO - Generated 3300 questions
2025-05-05 19:16:04,056 - distillation - INFO - Generated 3400 questions
2025-05-05 19:16:04,058 - distillation - INFO - Generated 3500 questions
2025-05-05 19:16:04,061 - distillation - INFO - Generated 3600 questions
2025-05-05 19:16:04,062 - distillation - INFO - Generated 3700 questions
2025-05-05 19:16:04,064 - distillation - INFO - Generated 3800 questions
2025-05-05 19:16:04,065 - distillation - INFO - Generated 3900 questions
2025-05-05 19:16:04,069 - distillation - INFO - Generated questions saved to questions.txt
2025-05-05 19:16:04,070 - distillation - INFO - Using PyTorch nightly version 2.7.0.dev20250309+cu128 for data generation
2025-05-05 19:16:04,070 - distillation - INFO - Processing batch 1/1000
2025-05-05 19:18:38,923 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-05 19:37:24,506 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 19:37:24,517 - __main__ - INFO - RAM: 使用中 10.4GB / 95.4GB (10.9%)
2025-05-05 19:37:24,541 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 19:37:24,541 - __main__ - INFO - Windows環境を検出しました
2025-05-05 19:37:24,542 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 19:37:39,543 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 19:37:39,543 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:37:39,544 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 19:37:39,544 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 19:37:43,980 - __main__ - INFO - Using device: cuda
2025-05-05 19:37:43,980 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 19:37:43,981 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 19:37:43,981 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 19:37:43,981 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:37:44,867 - __main__ - INFO - Initializing student model
2025-05-05 19:37:45,706 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:37:45,717 - distillation - INFO - RAM使用状況: 12.0% (使用中: 11.5GB, 空き: 83.9GB)
2025-05-05 19:37:45,718 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:37:45,718 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 19:37:45,719 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 19:37:47,683 - distillation - INFO - xformersが使用可能です
2025-05-05 19:37:47,684 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 19:37:47,686 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 19:38:27,984 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 19:39:11,408 - distillation - INFO - Teacher model loaded successfully
2025-05-05 19:39:11,421 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 19:39:11,422 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 19:39:11,428 - distillation - INFO - Using PyTorch nightly version 2.7.0.dev20250309+cu128 for data generation
2025-05-05 19:39:11,428 - distillation - INFO - Processing batch 1/1000
2025-05-05 19:57:18,123 - distillation - INFO - Processing batch 2/1000
2025-05-05 20:15:05,750 - distillation - INFO - Processing batch 3/1000
2025-05-05 20:32:52,518 - distillation - INFO - Processing batch 4/1000
2025-05-05 20:50:39,044 - distillation - INFO - Processing batch 5/1000
2025-05-05 21:08:24,119 - distillation - INFO - Processing batch 6/1000
2025-05-05 21:26:09,554 - distillation - INFO - Processing batch 7/1000
2025-05-05 21:44:03,229 - distillation - INFO - Processing batch 8/1000
2025-05-05 22:01:57,091 - distillation - INFO - Processing batch 9/1000
2025-05-05 22:19:02,389 - distillation - INFO - Processing batch 10/1000
2025-05-05 22:37:00,271 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_40
2025-05-05 22:37:00,272 - distillation - INFO - Processing batch 11/1000
2025-05-05 22:55:04,181 - distillation - INFO - Processing batch 12/1000
2025-05-05 23:13:06,050 - distillation - INFO - Processing batch 13/1000
2025-05-05 23:31:00,684 - distillation - INFO - Processing batch 14/1000
2025-05-05 23:49:00,736 - distillation - INFO - Processing batch 15/1000
2025-05-06 00:07:04,566 - distillation - INFO - Processing batch 16/1000
2025-05-06 00:25:07,218 - distillation - INFO - Processing batch 17/1000
2025-05-06 00:43:08,792 - distillation - INFO - Processing batch 18/1000
2025-05-06 01:01:12,135 - distillation - INFO - Processing batch 19/1000
2025-05-06 01:19:10,856 - distillation - INFO - Processing batch 20/1000
2025-05-06 01:37:10,916 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_80
2025-05-06 01:37:10,916 - distillation - INFO - Processing batch 21/1000
2025-05-06 01:55:11,228 - distillation - INFO - Processing batch 22/1000
2025-05-06 02:13:01,497 - distillation - INFO - Processing batch 23/1000
2025-05-06 02:31:00,071 - distillation - INFO - Processing batch 24/1000
2025-05-06 02:49:03,913 - distillation - INFO - Processing batch 25/1000
2025-05-06 03:06:59,700 - distillation - INFO - Processing batch 26/1000
2025-05-06 03:25:02,820 - distillation - INFO - Processing batch 27/1000
2025-05-06 03:43:02,734 - distillation - INFO - Processing batch 28/1000
2025-05-06 04:01:01,968 - distillation - INFO - Processing batch 29/1000
2025-05-06 04:19:03,270 - distillation - INFO - Processing batch 30/1000
2025-05-06 04:37:02,355 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_120
2025-05-06 04:37:02,356 - distillation - INFO - Processing batch 31/1000
2025-05-06 04:54:59,146 - distillation - INFO - Processing batch 32/1000
2025-05-06 05:12:50,872 - distillation - INFO - Processing batch 33/1000
2025-05-06 05:30:46,946 - distillation - INFO - Processing batch 34/1000
2025-05-06 05:48:38,766 - distillation - INFO - Processing batch 35/1000
2025-05-06 06:06:32,509 - distillation - INFO - Processing batch 36/1000
2025-05-06 06:24:28,888 - distillation - INFO - Processing batch 37/1000
2025-05-06 06:42:24,129 - distillation - INFO - Processing batch 38/1000
2025-05-06 07:00:20,255 - distillation - INFO - Processing batch 39/1000
2025-05-06 07:18:15,042 - distillation - INFO - Processing batch 40/1000
2025-05-06 07:36:08,224 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_160
2025-05-06 07:36:08,224 - distillation - INFO - Processing batch 41/1000
2025-05-06 07:54:03,341 - distillation - INFO - Processing batch 42/1000
2025-05-06 08:11:58,898 - distillation - INFO - Processing batch 43/1000
2025-05-06 08:29:58,144 - distillation - INFO - Processing batch 44/1000
2025-05-06 08:47:54,606 - distillation - INFO - Processing batch 45/1000
2025-05-06 09:05:49,136 - distillation - INFO - Processing batch 46/1000
2025-05-06 09:23:38,117 - distillation - INFO - Processing batch 47/1000
2025-05-06 09:41:49,085 - distillation - INFO - Processing batch 48/1000
2025-05-06 10:00:04,392 - distillation - INFO - Processing batch 49/1000
2025-05-06 10:17:59,313 - distillation - INFO - Processing batch 50/1000
2025-05-06 10:35:54,840 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_200
2025-05-06 10:35:54,840 - distillation - INFO - Processing batch 51/1000
2025-05-06 10:53:45,497 - distillation - INFO - Processing batch 52/1000
2025-05-06 11:11:28,831 - distillation - INFO - Processing batch 53/1000
2025-05-06 11:29:22,668 - distillation - INFO - Processing batch 54/1000
2025-05-06 11:47:17,464 - distillation - INFO - Processing batch 55/1000
2025-05-06 12:05:15,315 - distillation - INFO - Processing batch 56/1000
2025-05-06 12:23:22,105 - distillation - INFO - Processing batch 57/1000
2025-05-06 12:41:22,010 - distillation - INFO - Processing batch 58/1000
2025-05-06 12:59:17,226 - distillation - INFO - Processing batch 59/1000
2025-05-06 13:17:12,326 - distillation - INFO - Processing batch 60/1000
2025-05-06 13:35:15,258 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_240
2025-05-06 13:35:15,258 - distillation - INFO - Processing batch 61/1000
2025-05-06 13:53:14,368 - distillation - INFO - Processing batch 62/1000
2025-05-06 14:11:34,199 - distillation - INFO - Processing batch 63/1000
2025-05-06 14:30:00,707 - distillation - INFO - Processing batch 64/1000
2025-05-06 14:48:23,825 - distillation - INFO - Processing batch 65/1000
2025-05-06 15:06:47,462 - distillation - INFO - Processing batch 66/1000
2025-05-06 15:25:04,602 - distillation - INFO - Processing batch 67/1000
2025-05-06 15:43:28,362 - distillation - INFO - Processing batch 68/1000
2025-05-06 16:01:49,094 - distillation - INFO - Processing batch 69/1000
2025-05-06 16:20:08,201 - distillation - INFO - Processing batch 70/1000
2025-05-06 16:38:17,477 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_280
2025-05-06 16:38:17,477 - distillation - INFO - Processing batch 71/1000
2025-05-06 16:56:31,797 - distillation - INFO - Processing batch 72/1000
2025-05-06 17:14:50,958 - distillation - INFO - Processing batch 73/1000
2025-05-06 17:33:00,482 - distillation - INFO - Processing batch 74/1000
2025-05-06 17:51:15,509 - distillation - INFO - Processing batch 75/1000
2025-05-06 18:09:28,788 - distillation - INFO - Processing batch 76/1000
2025-05-06 18:27:33,361 - distillation - INFO - Processing batch 77/1000
2025-05-06 18:45:40,977 - distillation - INFO - Processing batch 78/1000
2025-05-06 19:03:46,914 - distillation - INFO - Processing batch 79/1000
2025-05-06 19:21:56,799 - distillation - INFO - Processing batch 80/1000
2025-05-06 19:40:11,666 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_320
2025-05-06 19:40:11,666 - distillation - INFO - Processing batch 81/1000
2025-05-06 19:58:24,708 - distillation - INFO - Processing batch 82/1000
2025-05-06 20:16:36,995 - distillation - INFO - Processing batch 83/1000
2025-05-06 20:34:42,898 - distillation - INFO - Processing batch 84/1000
2025-05-06 20:52:50,202 - distillation - INFO - Processing batch 85/1000
2025-05-06 21:10:59,550 - distillation - INFO - Processing batch 86/1000
2025-05-06 21:29:01,038 - distillation - INFO - Processing batch 87/1000
2025-05-06 21:46:56,073 - distillation - INFO - Processing batch 88/1000
2025-05-06 22:04:45,909 - distillation - INFO - Processing batch 89/1000
2025-05-06 22:22:47,506 - distillation - INFO - Processing batch 90/1000
2025-05-06 22:40:36,106 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_360
2025-05-06 22:40:36,106 - distillation - INFO - Processing batch 91/1000
2025-05-06 22:58:29,342 - distillation - INFO - Processing batch 92/1000
2025-05-06 23:16:19,824 - distillation - INFO - Processing batch 93/1000
2025-05-06 23:34:17,410 - distillation - INFO - Processing batch 94/1000
2025-05-06 23:52:11,085 - distillation - INFO - Processing batch 95/1000
2025-05-07 00:10:09,091 - distillation - INFO - Processing batch 96/1000
2025-05-07 00:28:01,853 - distillation - INFO - Processing batch 97/1000
2025-05-07 00:45:52,383 - distillation - INFO - Processing batch 98/1000
2025-05-07 01:03:48,757 - distillation - INFO - Processing batch 99/1000
2025-05-07 01:21:45,692 - distillation - INFO - Processing batch 100/1000
2025-05-07 01:39:42,952 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_400
2025-05-07 01:39:42,952 - distillation - INFO - Processing batch 101/1000
2025-05-07 01:57:34,036 - distillation - INFO - Processing batch 102/1000
2025-05-07 02:15:26,233 - distillation - INFO - Processing batch 103/1000
2025-05-07 02:33:21,448 - distillation - INFO - Processing batch 104/1000
2025-05-07 02:51:09,351 - distillation - INFO - Processing batch 105/1000
2025-05-07 03:09:00,313 - distillation - INFO - Processing batch 106/1000
2025-05-07 03:26:57,110 - distillation - INFO - Processing batch 107/1000
2025-05-07 03:44:50,271 - distillation - INFO - Processing batch 108/1000
2025-05-07 04:02:40,550 - distillation - INFO - Processing batch 109/1000
2025-05-07 04:20:36,906 - distillation - INFO - Processing batch 110/1000
2025-05-07 04:38:30,735 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_440
2025-05-07 04:38:30,735 - distillation - INFO - Processing batch 111/1000
2025-05-07 04:56:24,160 - distillation - INFO - Processing batch 112/1000
2025-05-07 05:14:18,763 - distillation - INFO - Processing batch 113/1000
2025-05-07 05:32:12,446 - distillation - INFO - Processing batch 114/1000
2025-05-07 05:50:00,351 - distillation - INFO - Processing batch 115/1000
2025-05-07 06:07:51,306 - distillation - INFO - Processing batch 116/1000
2025-05-07 06:25:47,696 - distillation - INFO - Processing batch 117/1000
2025-05-07 06:43:40,886 - distillation - INFO - Processing batch 118/1000
2025-05-07 07:02:13,096 - distillation - INFO - Processing batch 119/1000
2025-05-07 07:20:36,455 - distillation - INFO - Processing batch 120/1000
2025-05-07 07:38:58,804 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_480
2025-05-07 07:38:58,805 - distillation - INFO - Processing batch 121/1000
2025-05-07 07:57:25,406 - distillation - INFO - Processing batch 122/1000
2025-05-07 08:15:50,119 - distillation - INFO - Processing batch 123/1000
2025-05-07 08:34:14,433 - distillation - INFO - Processing batch 124/1000
2025-05-07 08:52:33,865 - distillation - INFO - Processing batch 125/1000
2025-05-07 09:10:34,732 - distillation - INFO - Processing batch 126/1000
2025-05-07 09:28:33,676 - distillation - INFO - Processing batch 127/1000
2025-05-07 09:46:38,534 - distillation - INFO - Processing batch 128/1000
2025-05-07 10:04:41,866 - distillation - INFO - Processing batch 129/1000
2025-05-07 10:22:42,467 - distillation - INFO - Processing batch 130/1000
2025-05-07 10:40:42,439 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_520
2025-05-07 10:40:42,439 - distillation - INFO - Processing batch 131/1000
2025-05-07 10:58:42,599 - distillation - INFO - Processing batch 132/1000
2025-05-07 11:16:46,743 - distillation - INFO - Processing batch 133/1000
2025-05-07 11:35:55,770 - distillation - INFO - Processing batch 134/1000
2025-05-07 12:08:20,362 - distillation - INFO - Processing batch 135/1000
2025-05-07 12:35:58,525 - distillation - INFO - Processing batch 136/1000
2025-05-07 13:03:44,376 - distillation - INFO - Processing batch 137/1000
2025-05-07 13:31:36,560 - distillation - INFO - Processing batch 138/1000
2025-05-07 14:00:22,402 - distillation - INFO - Processing batch 139/1000
2025-05-07 14:28:32,200 - distillation - INFO - Processing batch 140/1000
2025-05-07 14:57:37,643 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_560
2025-05-07 14:57:37,644 - distillation - INFO - Processing batch 141/1000
2025-05-07 15:20:30,480 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 19:52:06,120 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 19:52:06,125 - __main__ - INFO - RAM: 使用中 26.3GB / 95.4GB (27.6%)
2025-05-07 19:52:06,161 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 19:52:06,161 - __main__ - WARNING - LMstudioと通常のteacher_modelの両方が指定されています。LMstudioが優先されます。
2025-05-07 19:52:06,162 - __main__ - INFO - Windows環境を検出しました
2025-05-07 19:52:06,162 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 19:52:12,225 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 19:52:12,225 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 19:52:12,226 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 19:52:12,229 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 19:52:14,777 - __main__ - INFO - Using device: cuda
2025-05-07 19:52:14,777 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 19:52:14,777 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 19:52:14,778 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 19:52:14,778 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-07 19:52:15,543 - __main__ - INFO - Initializing student model
2025-05-07 19:52:16,575 - distillation - INFO - RAM使用状況: 28.6% (使用中: 27.3GB, 空き: 68.1GB)
2025-05-07 19:52:16,739 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 19:52:18,769 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 19:52:18,770 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 19:52:18,830 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 20:34:34,990 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 20:34:34,995 - __main__ - INFO - RAM: 使用中 27.5GB / 95.4GB (28.8%)
2025-05-07 20:34:35,046 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 20:34:35,047 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 20:34:35,047 - __main__ - INFO - Windows環境を検出しました
2025-05-07 20:34:35,047 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 20:34:41,657 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 20:34:41,658 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 20:34:41,658 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 20:34:41,658 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 20:34:45,754 - __main__ - INFO - Using device: cuda
2025-05-07 20:34:45,754 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 20:34:45,755 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 20:34:45,755 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 20:34:50,144 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 20:34:50,144 - __main__ - INFO - Initializing student model
2025-05-07 20:34:50,461 - distillation - INFO - RAM使用状況: 29.3% (使用中: 28.0GB, 空き: 67.4GB)
2025-05-07 20:34:50,462 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 20:34:52,495 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 20:34:52,495 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 20:34:52,557 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:03:18,558 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:03:18,564 - __main__ - INFO - RAM: 使用中 31.5GB / 95.4GB (33.0%)
2025-05-07 21:03:18,581 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:03:18,581 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:03:18,582 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:03:18,582 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:03:21,157 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:03:21,157 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:03:21,157 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:03:21,158 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:03:36,734 - __main__ - INFO - Using device: cuda
2025-05-07 21:03:36,734 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:03:36,734 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:03:36,735 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:03:37,198 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:03:37,199 - __main__ - INFO - Initializing student model
2025-05-07 21:03:37,537 - distillation - INFO - RAM使用状況: 33.3% (使用中: 31.8GB, 空き: 63.6GB)
2025-05-07 21:03:37,537 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:03:39,566 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:03:39,566 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:03:39,630 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:06:19,831 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:06:19,835 - __main__ - INFO - RAM: 使用中 31.6GB / 95.4GB (33.1%)
2025-05-07 21:06:19,856 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:06:19,856 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:06:19,856 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:06:19,856 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:06:31,422 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:06:31,422 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:06:31,422 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:06:34,441 - __main__ - INFO - Using device: cuda
2025-05-07 21:06:34,441 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:06:34,442 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:06:34,442 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:06:34,862 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:06:34,863 - __main__ - INFO - Initializing student model
2025-05-07 21:06:35,197 - distillation - INFO - RAM使用状況: 33.6% (使用中: 32.0GB, 空き: 63.3GB)
2025-05-07 21:06:35,197 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:06:37,228 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:06:37,228 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:06:37,287 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:11:00,282 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:11:00,290 - __main__ - INFO - RAM: 使用中 31.7GB / 95.4GB (33.2%)
2025-05-07 21:11:00,314 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:11:00,314 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:11:00,314 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:11:00,315 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:11:29,397 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:11:29,398 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:11:29,398 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:11:29,398 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:11:30,593 - __main__ - INFO - Using device: cuda
2025-05-07 21:11:30,593 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:11:30,593 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:11:30,594 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:11:31,088 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:11:31,088 - __main__ - INFO - Initializing student model
2025-05-07 21:11:31,418 - distillation - INFO - RAM使用状況: 33.5% (使用中: 32.0GB, 空き: 63.4GB)
2025-05-07 21:11:31,418 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:11:33,475 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:11:33,476 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:11:33,535 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:34:06,490 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:34:06,495 - __main__ - INFO - RAM: 使用中 31.9GB / 95.4GB (33.4%)
2025-05-07 21:34:06,553 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:34:06,553 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:34:06,553 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:34:06,553 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:34:10,157 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:34:10,158 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:34:10,158 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:34:13,240 - __main__ - INFO - Using device: cuda
2025-05-07 21:34:13,240 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:34:13,241 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:34:13,241 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:34:13,627 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:34:13,627 - __main__ - INFO - Initializing student model
2025-05-07 21:34:13,968 - distillation - INFO - RAM使用状況: 33.7% (使用中: 32.1GB, 空き: 63.2GB)
2025-05-07 21:34:13,969 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:34:16,011 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:34:16,012 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:34:16,074 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:35:02,313 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:35:02,318 - __main__ - INFO - RAM: 使用中 31.9GB / 95.4GB (33.5%)
2025-05-07 21:35:02,338 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:35:02,338 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:35:02,338 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:35:02,338 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:35:05,086 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:35:05,086 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:35:05,088 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:35:05,088 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:35:06,356 - __main__ - INFO - Using device: cuda
2025-05-07 21:35:06,357 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:35:06,357 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:35:06,357 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:35:07,107 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:35:07,107 - __main__ - INFO - Initializing student model
2025-05-07 21:35:07,431 - distillation - INFO - RAM使用状況: 34.0% (使用中: 32.4GB, 空き: 63.0GB)
2025-05-07 21:35:07,431 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234/v1/chat/completions
2025-05-07 21:35:07,444 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:35:07,444 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234/v1/chat/completions: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:35:07,505 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:36:33,533 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:36:33,538 - __main__ - INFO - RAM: 使用中 31.8GB / 95.4GB (33.4%)
2025-05-07 21:36:33,556 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:36:33,556 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:36:33,556 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:36:33,556 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:36:35,632 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:36:35,632 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:36:35,632 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:36:35,632 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:36:36,644 - __main__ - INFO - Using device: cuda
2025-05-07 21:36:36,645 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:36:36,645 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:36:36,645 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:36:37,055 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:36:37,055 - __main__ - INFO - Initializing student model
2025-05-07 21:36:37,391 - distillation - INFO - RAM使用状況: 33.8% (使用中: 32.3GB, 空き: 63.1GB)
2025-05-07 21:36:37,391 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234
2025-05-07 21:36:37,405 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:36:37,405 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:36:37,465 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:37:42,954 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:37:42,958 - __main__ - INFO - RAM: 使用中 31.8GB / 95.4GB (33.3%)
2025-05-07 21:37:42,979 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:37:42,979 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:37:42,980 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:37:42,980 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:37:45,549 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:37:45,550 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:37:45,550 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:37:45,550 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:37:46,483 - __main__ - INFO - Using device: cuda
2025-05-07 21:37:46,483 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:37:46,483 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:37:46,484 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:37:47,205 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:37:47,205 - __main__ - INFO - Initializing student model
2025-05-07 21:37:47,540 - distillation - INFO - RAM使用状況: 33.8% (使用中: 32.2GB, 空き: 63.2GB)
2025-05-07 21:37:47,540 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234
2025-05-07 21:37:47,549 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:37:47,550 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:37:47,610 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:39:34,981 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:39:34,986 - __main__ - INFO - RAM: 使用中 31.7GB / 95.4GB (33.2%)
2025-05-07 21:39:35,005 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:39:35,006 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:39:35,006 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:39:35,006 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:39:37,028 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:39:37,028 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:39:37,028 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:39:37,028 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:39:37,673 - __main__ - INFO - Using device: cuda
2025-05-07 21:39:37,674 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:39:37,674 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:39:37,674 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:39:38,419 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:39:38,420 - __main__ - INFO - Initializing student model
2025-05-07 21:39:38,775 - distillation - INFO - RAM使用状況: 33.7% (使用中: 32.1GB, 空き: 63.3GB)
2025-05-07 21:39:38,775 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234
2025-05-07 21:39:38,784 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:39:38,785 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:39:38,845 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:20:01,467 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:20:01,472 - __main__ - INFO - RAM: 使用中 27.9GB / 95.4GB (29.3%)
2025-05-07 22:20:01,532 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:20:01,533 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:20:01,533 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:20:01,533 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:20:01,533 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:20:01,533 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:20:01,534 - __main__ - INFO - Using device: cuda
2025-05-07 22:20:01,535 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:20:01,535 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:20:01,535 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:20:01,535 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:20:02,548 - __main__ - ERROR - Failed to load tokenizer for GGUF model: The repository for Qwen/Qwen-14B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-14B.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-05-07 22:20:02,610 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:20:19,970 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:20:19,983 - __main__ - INFO - RAM: 使用中 27.9GB / 95.4GB (29.3%)
2025-05-07 22:20:20,029 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:20:20,030 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:20:20,030 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:20:20,030 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:20:20,030 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:20:20,030 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:20:27,110 - __main__ - INFO - Using device: cuda
2025-05-07 22:20:27,110 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:20:27,110 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:20:27,110 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:20:27,110 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:20:27,376 - __main__ - ERROR - Failed to load tokenizer for GGUF model: The repository for Qwen/Qwen-14B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-14B.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-05-07 22:20:27,442 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:29:43,677 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:29:43,686 - __main__ - INFO - RAM: 使用中 28.0GB / 95.4GB (29.4%)
2025-05-07 22:29:43,738 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:29:43,739 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:29:43,739 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:29:43,740 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:29:43,740 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:29:43,740 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:30:15,574 - __main__ - INFO - Using device: cuda
2025-05-07 22:30:15,574 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:30:15,574 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:30:15,574 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:30:15,575 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:30:15,947 - __main__ - ERROR - Failed to load tokenizer for GGUF model: The repository for Qwen/Qwen-14B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-14B.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-05-07 22:30:16,010 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:35:40,990 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:35:41,000 - __main__ - INFO - RAM: 使用中 28.3GB / 95.4GB (29.7%)
2025-05-07 22:35:41,047 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:35:41,047 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:35:41,048 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:35:41,048 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:35:41,048 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:35:41,049 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:35:46,721 - __main__ - INFO - Using device: cuda
2025-05-07 22:35:46,721 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:35:46,722 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:35:46,722 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:35:46,722 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:35:49,858 - __main__ - INFO - Initializing student model
2025-05-07 22:35:51,483 - distillation - INFO - RAM使用状況: 30.8% (使用中: 29.4GB, 空き: 66.0GB)
2025-05-07 22:35:51,483 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 22:35:52,684 - distillation - INFO - GGUF model loaded successfully
2025-05-07 22:35:52,684 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 22:35:52,685 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 22:35:52,718 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 22:35:52,719 - distillation - INFO - Processing batch 1/1000
2025-05-07 22:35:52,719 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 22:36:10,368 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:41:20,611 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:41:20,619 - __main__ - INFO - RAM: 使用中 28.5GB / 95.4GB (29.8%)
2025-05-07 22:41:20,673 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:41:20,673 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:41:20,675 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:41:20,675 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:41:20,675 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:41:20,676 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:41:26,420 - __main__ - INFO - Using device: cuda
2025-05-07 22:41:26,421 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:41:26,421 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:41:26,421 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:41:26,421 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:41:27,030 - __main__ - INFO - Initializing student model
2025-05-07 22:41:28,756 - distillation - INFO - RAM使用状況: 30.9% (使用中: 29.5GB, 空き: 65.9GB)
2025-05-07 22:41:28,756 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 22:41:28,756 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1)
2025-05-07 22:41:30,009 - distillation - INFO - GGUF model loaded successfully
2025-05-07 22:41:30,010 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 22:41:30,010 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 22:41:30,034 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 22:41:30,034 - distillation - INFO - Processing batch 1/1000
2025-05-07 22:41:30,035 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 22:41:50,198 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:55:16,946 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:55:16,950 - __main__ - INFO - RAM: 使用中 29.1GB / 95.4GB (30.5%)
2025-05-07 22:55:17,008 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:55:17,008 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:55:17,009 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:55:17,009 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:55:17,010 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:55:17,010 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:55:20,490 - __main__ - INFO - Using device: cuda
2025-05-07 22:55:20,491 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:55:20,491 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:55:20,491 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:55:20,491 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:55:21,119 - __main__ - INFO - Initializing student model
2025-05-07 22:55:23,179 - distillation - INFO - RAM使用状況: 31.7% (使用中: 30.2GB, 空き: 65.2GB)
2025-05-07 22:55:23,180 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 22:55:23,180 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1)
2025-05-07 22:55:24,434 - distillation - INFO - GGUF model loaded successfully
2025-05-07 22:55:24,434 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 22:55:24,435 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 22:55:24,459 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 22:55:24,462 - distillation - INFO - Processing batch 1/1000
2025-05-07 22:55:24,462 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 22:55:29,995 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:02:21,921 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:02:21,930 - __main__ - INFO - RAM: 使用中 29.5GB / 95.4GB (31.0%)
2025-05-07 23:02:21,993 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:02:21,993 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:02:21,993 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:02:21,993 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:02:21,994 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:02:21,994 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:02:24,326 - __main__ - INFO - Using device: cuda
2025-05-07 23:02:24,327 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:24,327 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:02:24,327 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:02:24,327 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-07 23:02:24,328 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:02:25,043 - __main__ - INFO - Initializing student model
2025-05-07 23:02:27,280 - distillation - INFO - RAM使用状況: 32.1% (使用中: 30.6GB, 空き: 64.8GB)
2025-05-07 23:02:27,281 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:02:27,281 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1, n_gpu=1, n_batch=512)
2025-05-07 23:02:27,281 - distillation - INFO - GGUF: 使用GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:30,524 - distillation - INFO - GGUF: GPUモード動作確認完了
2025-05-07 23:02:30,524 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:02:30,525 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:02:30,525 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:02:30,556 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:02:30,556 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:02:30,556 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:02:34,891 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:02:48,102 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:02:48,110 - __main__ - INFO - RAM: 使用中 29.5GB / 95.4GB (31.0%)
2025-05-07 23:02:48,169 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:02:48,169 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:02:48,170 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:02:48,171 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:02:48,171 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:02:48,171 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:02:53,166 - __main__ - INFO - Using device: cuda
2025-05-07 23:02:53,167 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:53,167 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:02:53,167 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:02:53,167 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-07 23:02:53,168 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:02:53,872 - __main__ - INFO - Initializing student model
2025-05-07 23:02:55,609 - distillation - INFO - RAM使用状況: 32.1% (使用中: 30.6GB, 空き: 64.8GB)
2025-05-07 23:02:55,610 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:02:55,610 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1, n_gpu=1, n_batch=512)
2025-05-07 23:02:55,610 - distillation - INFO - GGUF: 使用GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:58,785 - distillation - INFO - GGUF: GPUモード動作確認完了
2025-05-07 23:02:58,785 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:02:58,786 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:02:58,786 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:02:58,815 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:02:58,816 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:02:58,816 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:06:10,187 - distillation - INFO - Generating GGUF response for question 2/4000
2025-05-07 23:07:09,075 - distillation - INFO - Generating GGUF response for question 3/4000
2025-05-07 23:08:28,585 - distillation - INFO - Generating GGUF response for question 4/4000
2025-05-07 23:09:21,577 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:09:44,042 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:09:44,052 - __main__ - INFO - RAM: 使用中 29.6GB / 95.4GB (31.0%)
2025-05-07 23:09:44,108 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:09:44,109 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:09:44,109 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:09:44,109 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:09:44,109 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:09:44,109 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:09:47,119 - __main__ - INFO - Using device: cuda
2025-05-07 23:09:47,120 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:09:47,120 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:09:47,120 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:09:47,120 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-07 23:09:47,121 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:09:47,790 - __main__ - INFO - Initializing student model
2025-05-07 23:09:49,496 - distillation - INFO - RAM使用状況: 32.1% (使用中: 30.6GB, 空き: 64.8GB)
2025-05-07 23:09:49,496 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:09:49,497 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=-1, n_gpu=1, n_batch=512
2025-05-07 23:09:49,497 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:09:49,497 - distillation - INFO - CUDA バージョン: 12.8
2025-05-07 23:09:49,507 - distillation - INFO - cuDNN バージョン: 90701
2025-05-07 23:09:49,507 - distillation - INFO - CUDA デバイス数: 1
2025-05-07 23:09:49,507 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-07 23:09:49,507 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': -1, 'n_batch': 512, 'offload_kqv': True, 'main_gpu': 0}
2025-05-07 23:09:50,745 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-07 23:09:54,380 - distillation - INFO - テスト推論完了: 処理時間 3.63秒
2025-05-07 23:09:54,390 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-07 23:09:54,390 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-07 23:09:54,390 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:09:54,391 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:09:54,391 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000001C7FD665F90>>
2025-05-07 23:09:54,391 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-07 23:09:54,391 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:09:54,391 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:09:54,415 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:09:54,415 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:09:54,415 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:10:01,881 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:15:15,163 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:15:15,174 - __main__ - INFO - RAM: 使用中 29.6GB / 95.4GB (31.0%)
2025-05-07 23:15:15,233 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:15:15,233 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:15:15,234 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:15:15,234 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:15:15,234 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:15:15,235 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:15:20,929 - __main__ - INFO - Using device: cuda
2025-05-07 23:15:20,930 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:15:20,930 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:15:20,930 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:15:20,930 - __main__ - INFO - GGUF GPU設定: GPU層数=40, GPU数=1, バッチサイズ=512
2025-05-07 23:15:20,930 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:15:21,535 - __main__ - INFO - Initializing student model
2025-05-07 23:15:23,198 - distillation - INFO - RAM使用状況: 32.2% (使用中: 30.7GB, 空き: 64.7GB)
2025-05-07 23:15:23,198 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:15:23,199 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=40, n_gpu=1, n_batch=512
2025-05-07 23:15:23,199 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:15:23,199 - distillation - INFO - CUDA バージョン: 12.8
2025-05-07 23:15:23,199 - distillation - INFO - cuDNN バージョン: 90701
2025-05-07 23:15:23,199 - distillation - INFO - CUDA デバイス数: 1
2025-05-07 23:15:23,199 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-07 23:15:23,199 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': 40, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-07 23:15:24,391 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-07 23:15:28,047 - distillation - INFO - テスト推論完了: 処理時間 3.66秒
2025-05-07 23:15:28,048 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-07 23:15:28,048 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-07 23:15:28,048 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:15:28,048 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:15:28,048 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000002190DDF5F90>>
2025-05-07 23:15:28,048 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-07 23:15:28,049 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:15:28,049 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:15:28,082 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:15:28,082 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:15:28,082 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:15:36,354 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:03:51,109 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:03:51,114 - __main__ - INFO - RAM: 使用中 25.6GB / 95.4GB (26.9%)
2025-05-08 08:03:51,163 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:03:51,163 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-08 08:03:51,164 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-08 08:03:51,164 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:03:51,164 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:03:51,164 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:03:53,553 - __main__ - INFO - Using device: cuda
2025-05-08 08:03:53,554 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:03:53,554 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:03:53,554 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:03:53,554 - __main__ - INFO - GGUF GPU設定: GPU層数=40, GPU数=1, バッチサイズ=512
2025-05-08 08:03:53,555 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-08 08:03:54,659 - __main__ - INFO - Initializing student model
2025-05-08 08:03:55,573 - distillation - INFO - RAM使用状況: 28.0% (使用中: 26.7GB, 空き: 68.7GB)
2025-05-08 08:03:55,574 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:03:55,575 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=40, n_gpu=1, n_batch=512
2025-05-08 08:03:55,576 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:03:55,577 - distillation - INFO - CUDA バージョン: 12.8
2025-05-08 08:03:55,578 - distillation - INFO - cuDNN バージョン: 90701
2025-05-08 08:03:55,579 - distillation - INFO - CUDA デバイス数: 1
2025-05-08 08:03:55,579 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-08 08:03:55,579 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': 40, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-08 08:03:56,566 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-08 08:03:58,827 - distillation - INFO - テスト推論完了: 処理時間 2.26秒
2025-05-08 08:03:58,837 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-08 08:03:58,837 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-08 08:03:58,837 - distillation - INFO - GGUF model loaded successfully
2025-05-08 08:03:58,837 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:03:58,837 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x00000262A77F2080>>
2025-05-08 08:03:58,838 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-08 08:03:58,838 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:03:58,838 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:03:58,864 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:03:58,864 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:03:58,864 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-08 08:04:14,282 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:06:55,277 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:06:55,285 - __main__ - INFO - RAM: 使用中 25.9GB / 95.4GB (27.2%)
2025-05-08 08:06:55,307 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:06:55,307 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:06:55,307 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:06:55,307 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:07:03,100 - __main__ - INFO - Using device: cuda
2025-05-08 08:07:03,100 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:07:03,101 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:07:03,101 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:07:03,101 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:07:04,071 - __main__ - INFO - Initializing student model
2025-05-08 08:07:04,821 - distillation - INFO - RAM使用状況: 28.1% (使用中: 26.8GB, 空き: 68.6GB)
2025-05-08 08:07:04,821 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:07:04,824 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:07:04,825 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:07:04,852 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:07:04,852 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:07:05,019 - __main__ - CRITICAL - 予期しないエラーが発生しました: name 'outputs' is not defined
2025-05-08 08:07:05,022 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 491, in prepare_distillation_data
    for j, output in enumerate(outputs):
NameError: name 'outputs' is not defined

2025-05-08 08:07:05,078 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:10:03,281 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:10:03,286 - __main__ - INFO - RAM: 使用中 26.2GB / 95.4GB (27.5%)
2025-05-08 08:10:03,314 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:10:03,315 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:10:03,315 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:10:03,315 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:10:06,158 - __main__ - INFO - Using device: cuda
2025-05-08 08:10:06,158 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:10:06,158 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:10:06,159 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:10:06,159 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:10:07,014 - __main__ - INFO - Initializing student model
2025-05-08 08:10:07,745 - distillation - INFO - RAM使用状況: 28.4% (使用中: 27.1GB, 空き: 68.3GB)
2025-05-08 08:10:07,745 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:10:07,748 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:10:07,749 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:10:07,777 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:10:07,777 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:10:07,915 - __main__ - CRITICAL - 予期しないエラーが発生しました: 'KnowledgeDistiller' object has no attribute 'teacher_model'
2025-05-08 08:10:07,916 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 489, in prepare_distillation_data
    outputs = self.teacher_model.generate(
AttributeError: 'KnowledgeDistiller' object has no attribute 'teacher_model'

2025-05-08 08:10:07,971 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:15:54,727 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:15:54,732 - __main__ - INFO - RAM: 使用中 24.0GB / 95.4GB (25.2%)
2025-05-08 08:15:54,753 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:15:54,753 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:15:54,753 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:15:54,753 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:16:26,566 - __main__ - INFO - Using device: cuda
2025-05-08 08:16:26,567 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:16:26,567 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:16:26,567 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:16:26,567 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:16:27,482 - __main__ - INFO - Initializing student model
2025-05-08 08:16:29,908 - distillation - INFO - RAM使用状況: 26.0% (使用中: 24.8GB, 空き: 70.6GB)
2025-05-08 08:16:29,908 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:16:29,910 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:17:05,420 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:17:05,422 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:17:05,424 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:17:05,454 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:17:05,454 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:17:28,377 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:17:38,764 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:17:38,771 - __main__ - INFO - RAM: 使用中 24.7GB / 95.4GB (25.9%)
2025-05-08 08:17:38,789 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:17:38,789 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-08 08:17:38,789 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-08 08:17:38,789 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:17:38,789 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:17:38,791 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:17:55,853 - __main__ - INFO - Using device: cuda
2025-05-08 08:17:55,853 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:17:55,854 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:17:55,854 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:17:55,854 - __main__ - INFO - GGUF GPU設定: GPU層数=40, GPU数=1, バッチサイズ=512
2025-05-08 08:17:55,854 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-08 08:17:57,088 - __main__ - INFO - Initializing student model
2025-05-08 08:17:57,914 - distillation - INFO - RAM使用状況: 27.0% (使用中: 25.8GB, 空き: 69.6GB)
2025-05-08 08:17:57,914 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:17:57,916 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=40, n_gpu=1, n_batch=512
2025-05-08 08:17:57,917 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:17:57,917 - distillation - INFO - CUDA バージョン: 12.8
2025-05-08 08:17:57,918 - distillation - INFO - cuDNN バージョン: 90701
2025-05-08 08:17:57,918 - distillation - INFO - CUDA デバイス数: 1
2025-05-08 08:17:57,918 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-08 08:17:57,918 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': 40, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-08 08:17:58,868 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-08 08:18:01,274 - distillation - INFO - テスト推論完了: 処理時間 2.40秒
2025-05-08 08:18:01,275 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-08 08:18:01,275 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-08 08:18:01,275 - distillation - INFO - GGUF model loaded successfully
2025-05-08 08:18:01,275 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:18:01,275 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000002B163D41F30>>
2025-05-08 08:18:01,275 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-08 08:18:01,276 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:18:01,276 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:18:01,307 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:18:01,307 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:18:01,307 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-08 08:18:15,947 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:20:22,430 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:20:22,437 - __main__ - INFO - RAM: 使用中 24.7GB / 95.4GB (25.9%)
2025-05-08 08:20:22,471 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:20:22,472 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:20:22,472 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:20:22,473 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:20:27,321 - __main__ - INFO - Using device: cuda
2025-05-08 08:20:27,321 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:20:27,322 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:20:27,322 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:20:27,322 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:20:28,163 - __main__ - INFO - Initializing student model
2025-05-08 08:20:28,881 - distillation - INFO - RAM使用状況: 26.8% (使用中: 25.6GB, 空き: 69.8GB)
2025-05-08 08:20:28,881 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:20:28,889 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:20:40,800 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:20:40,800 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:20:40,801 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:20:40,826 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:20:40,826 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:21:33,064 - distillation - INFO - Processing output 1/4 in batch 1
2025-05-08 08:21:33,064 - distillation - INFO - Processing output 2/4 in batch 1
2025-05-08 08:21:33,067 - distillation - INFO - Processing output 3/4 in batch 1
2025-05-08 08:21:33,068 - distillation - INFO - Processing output 4/4 in batch 1
2025-05-08 08:21:33,221 - distillation - INFO - Processing batch 2/1000
2025-05-08 08:22:07,485 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:28:34,756 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:28:34,762 - __main__ - INFO - RAM: 使用中 25.3GB / 95.4GB (26.5%)
2025-05-08 08:28:34,791 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:28:34,792 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-08 08:28:34,793 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-08 08:28:34,794 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:28:34,794 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:28:34,795 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:28:39,804 - __main__ - INFO - Using device: cuda
2025-05-08 08:28:39,805 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:28:39,805 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:28:39,805 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:28:39,805 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-08 08:28:39,805 - __main__ - INFO - デフォルトのLLaMAトークナイザーを使用します
2025-05-08 08:28:43,308 - __main__ - INFO - Initializing student model
2025-05-08 08:28:43,557 - distillation - INFO - RAM使用状況: 26.8% (使用中: 25.6GB, 空き: 69.8GB)
2025-05-08 08:28:43,557 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\Llama-3-ELYZA-JP-8B-gguf\Llama-3-ELYZA-JP-8B-Q8_0.gguf
2025-05-08 08:28:43,558 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=-1, n_gpu=1, n_batch=512
2025-05-08 08:28:43,559 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:28:43,560 - distillation - INFO - CUDA バージョン: 12.8
2025-05-08 08:28:43,561 - distillation - INFO - cuDNN バージョン: 90701
2025-05-08 08:28:43,561 - distillation - INFO - CUDA デバイス数: 1
2025-05-08 08:28:43,561 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-08 08:28:43,562 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': -1, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-08 08:28:46,152 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-08 08:28:48,002 - distillation - INFO - テスト推論完了: 処理時間 1.85秒
2025-05-08 08:28:48,002 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-08 08:28:48,002 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-08 08:28:48,002 - distillation - INFO - GGUF model loaded successfully
2025-05-08 08:28:48,002 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\Llama-3-ELYZA-JP-8B-gguf\Llama-3-ELYZA-JP-8B-Q8_0.gguf
2025-05-08 08:28:48,003 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000001BB43C97730>>
2025-05-08 08:28:48,003 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-08 08:28:48,003 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:28:48,003 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:28:48,032 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:28:48,032 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:28:48,032 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-08 08:28:55,907 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:31:05,615 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:31:05,621 - __main__ - INFO - RAM: 使用中 25.3GB / 95.4GB (26.5%)
2025-05-08 08:31:05,650 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:31:05,650 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:31:05,651 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:31:05,651 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:31:11,419 - __main__ - INFO - Using device: cuda
2025-05-08 08:31:11,419 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:31:11,420 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:31:11,420 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:31:11,420 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:31:12,268 - __main__ - INFO - Initializing student model
2025-05-08 08:31:12,973 - distillation - INFO - RAM使用状況: 27.4% (使用中: 26.1GB, 空き: 69.2GB)
2025-05-08 08:31:12,974 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:31:12,977 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:31:25,058 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:31:25,061 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:31:25,062 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:31:25,089 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:31:25,089 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:31:55,173 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:32:13,003 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:32:13,009 - __main__ - INFO - RAM: 使用中 25.3GB / 95.4GB (26.5%)
2025-05-08 08:32:13,031 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:32:13,032 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:32:13,032 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:32:13,033 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:32:14,985 - __main__ - INFO - Using device: cuda
2025-05-08 08:32:14,985 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:32:14,986 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:32:14,986 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:32:14,986 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:32:15,847 - __main__ - INFO - Initializing student model
2025-05-08 08:32:16,564 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.2GB, 空き: 69.2GB)
2025-05-08 08:32:16,564 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:32:16,568 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:32:28,531 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:32:28,534 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:32:28,535 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:32:28,537 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:32:28,537 - __main__ - CRITICAL - 予期しないエラーが発生しました: expected str, bytes or os.PathLike object, not NoneType
2025-05-08 08:32:28,539 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 446, in prepare_distillation_data
    with open(questions_file, 'r', encoding='utf-8') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

2025-05-08 08:32:28,701 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:35:32,619 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:35:32,626 - __main__ - INFO - RAM: 使用中 25.4GB / 95.4GB (26.6%)
2025-05-08 08:35:32,648 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:35:32,648 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:35:32,648 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:35:32,648 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:35:34,297 - __main__ - INFO - Using device: cuda
2025-05-08 08:35:34,298 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:35:34,298 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:35:34,298 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:35:34,298 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:35:34,786 - __main__ - INFO - Initializing student model
2025-05-08 08:35:35,506 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.2GB, 空き: 69.2GB)
2025-05-08 08:35:35,506 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:35:35,509 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:35:47,521 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:35:47,523 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:35:47,524 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:35:47,525 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:35:47,525 - __main__ - CRITICAL - 予期しないエラーが発生しました: expected str, bytes or os.PathLike object, not NoneType
2025-05-08 08:35:47,528 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 446, in prepare_distillation_data
    with open(questions_file, 'r', encoding='utf-8') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

2025-05-08 08:35:47,683 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:36:10,955 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:36:10,961 - __main__ - INFO - RAM: 使用中 25.4GB / 95.4GB (26.6%)
2025-05-08 08:36:10,996 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:36:10,996 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:36:10,996 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:36:10,996 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:36:12,929 - __main__ - INFO - Using device: cuda
2025-05-08 08:36:12,929 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:36:12,930 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:36:12,930 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:36:12,930 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:36:13,475 - __main__ - INFO - Initializing student model
2025-05-08 08:36:14,219 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.2GB, 空き: 69.2GB)
2025-05-08 08:36:14,219 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:36:14,221 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:36:25,977 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:36:25,977 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:36:25,978 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:36:25,978 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:36:25,979 - __main__ - CRITICAL - 予期しないエラーが発生しました: expected str, bytes or os.PathLike object, not NoneType
2025-05-08 08:36:25,981 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 446, in prepare_distillation_data
    with open(questions_file, 'r', encoding='utf-8') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

2025-05-08 08:36:26,132 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:37:15,608 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:37:15,616 - __main__ - INFO - RAM: 使用中 25.4GB / 95.4GB (26.6%)
2025-05-08 08:37:15,652 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:37:15,653 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:37:15,653 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:37:15,654 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:37:17,479 - __main__ - INFO - Using device: cuda
2025-05-08 08:37:17,479 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:37:17,479 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:37:17,479 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:37:17,479 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:37:18,010 - __main__ - INFO - Initializing student model
2025-05-08 08:37:18,694 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.3GB, 空き: 69.1GB)
2025-05-08 08:37:18,695 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:37:18,697 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:37:30,446 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:37:30,446 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:37:30,446 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:37:30,488 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:37:30,489 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:37:59,283 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:43:36,439 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:43:36,448 - __main__ - INFO - RAM: 使用中 25.5GB / 95.4GB (26.8%)
2025-05-08 08:43:36,499 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:43:36,499 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:43:36,500 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:43:36,500 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:43:38,291 - __main__ - INFO - Using device: cuda
2025-05-08 08:43:38,292 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:43:38,292 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:43:38,292 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:43:38,292 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:43:39,182 - __main__ - INFO - Initializing student model
2025-05-08 08:43:39,880 - distillation - INFO - RAM使用状況: 27.7% (使用中: 26.4GB, 空き: 69.0GB)
2025-05-08 08:43:39,880 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:43:39,882 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:43:51,721 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:43:51,724 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:43:51,725 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:43:51,726 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:43:51,727 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 12:53:50,580 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 12:54:03,500 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 12:54:03,505 - __main__ - INFO - RAM: 使用中 25.6GB / 95.4GB (26.8%)
2025-05-08 12:54:03,526 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 12:54:03,526 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 12:54:03,526 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 12:54:03,526 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 12:54:05,523 - __main__ - INFO - Using device: cuda
2025-05-08 12:54:05,523 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 12:54:05,523 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 12:54:05,523 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 12:54:05,524 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:54:06,048 - __main__ - INFO - Initializing student model
2025-05-08 12:54:06,750 - distillation - INFO - RAM使用状況: 27.7% (使用中: 26.5GB, 空き: 68.9GB)
2025-05-08 12:54:06,751 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:54:06,752 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 12:54:18,546 - distillation - INFO - Teacher model loaded successfully
2025-05-08 12:54:18,549 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 12:54:18,550 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 12:54:18,551 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 12:54:18,551 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 12:54:51,793 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 12:56:19,563 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 12:56:19,573 - __main__ - INFO - RAM: 使用中 25.5GB / 95.4GB (26.7%)
2025-05-08 12:56:19,606 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 12:56:19,606 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 12:56:19,607 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 12:56:19,607 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 12:56:27,473 - __main__ - INFO - Using device: cuda
2025-05-08 12:56:27,474 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 12:56:27,475 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 12:56:27,475 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 12:56:27,475 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:56:28,039 - __main__ - INFO - Initializing student model
2025-05-08 12:56:29,275 - distillation - INFO - RAM使用状況: 27.7% (使用中: 26.4GB, 空き: 69.0GB)
2025-05-08 12:56:29,275 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:56:29,277 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 12:56:33,295 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 12:56:43,022 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 12:56:43,032 - __main__ - INFO - RAM: 使用中 25.5GB / 95.4GB (26.7%)
2025-05-08 12:56:43,069 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 12:56:43,069 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 12:56:43,070 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 12:56:43,070 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 12:57:23,487 - __main__ - INFO - Using device: cuda
2025-05-08 12:57:23,488 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 12:57:23,488 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 12:57:23,488 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 12:57:23,489 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:57:24,383 - __main__ - INFO - Initializing student model
2025-05-08 12:57:25,482 - distillation - INFO - RAM使用状況: 27.6% (使用中: 26.3GB, 空き: 69.1GB)
2025-05-08 12:57:25,482 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:57:25,484 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 12:57:37,506 - distillation - INFO - Teacher model loaded successfully
2025-05-08 12:57:37,508 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 12:57:37,509 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 12:57:37,510 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 12:57:37,511 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 12:57:50,872 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 13:07:50,003 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 13:07:50,012 - __main__ - INFO - RAM: 使用中 27.5GB / 95.4GB (28.8%)
2025-05-08 13:07:50,032 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 13:07:50,033 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 13:07:50,033 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 13:07:50,033 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 13:11:50,045 - __main__ - INFO - Using device: cuda
2025-05-08 13:11:50,046 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 13:11:50,046 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 13:11:50,046 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 13:11:50,046 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:11:50,858 - __main__ - INFO - Initializing student model
2025-05-08 13:11:51,623 - distillation - INFO - RAM使用状況: 29.9% (使用中: 28.5GB, 空き: 66.8GB)
2025-05-08 13:11:51,623 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:11:51,624 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 13:12:05,082 - distillation - INFO - Teacher model loaded successfully
2025-05-08 13:12:05,082 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 13:12:05,083 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 13:12:05,083 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 13:12:05,083 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 13:12:05,083 - distillation - INFO - 基本的な質問テーマ数: 32
2025-05-08 13:16:06,327 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 13:45:49,039 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 13:45:49,046 - __main__ - INFO - RAM: 使用中 26.6GB / 95.4GB (27.9%)
2025-05-08 13:45:49,069 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 13:45:49,069 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 13:45:49,069 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 13:45:49,069 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 13:45:50,824 - __main__ - INFO - Using device: cuda
2025-05-08 13:45:50,825 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 13:45:50,825 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 13:45:50,825 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 13:45:50,825 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:45:51,362 - __main__ - INFO - Initializing student model
2025-05-08 13:45:53,042 - distillation - INFO - RAM使用状況: 28.8% (使用中: 27.5GB, 空き: 67.9GB)
2025-05-08 13:45:53,042 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:45:53,044 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 13:46:05,668 - distillation - INFO - Teacher model loaded successfully
2025-05-08 13:46:05,671 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 13:46:05,672 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 13:46:05,672 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 13:46:05,672 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 13:46:05,673 - distillation - INFO - 基本的な質問テーマ数: 30
2025-05-08 13:46:05,673 - distillation - INFO - 質問生成パターン数: 3
2025-05-08 13:46:05,673 - distillation - INFO - 基本的な質問テーマから質問を生成中...
2025-05-08 13:46:05,673 - distillation - INFO - 追加の質問を生成します。現在: 90/4000
2025-05-08 13:46:05,674 - distillation - INFO - 質問をさらに生成します。現在: 180/4000
2025-05-08 13:46:05,676 - distillation - INFO - 生成された質問数: 508
2025-05-08 13:46:05,676 - distillation - WARNING - 要求された 4000 個の質問を生成できませんでした。実際の生成数: 508
2025-05-08 13:46:05,677 - distillation - INFO - 質問をファイルに書き込み中: questions.txt
2025-05-08 13:46:05,678 - distillation - INFO - 質問ファイルの書き込みが完了しました: questions.txt
2025-05-08 13:46:05,680 - distillation - INFO - Filtered questions: 356 out of 508 original questions
2025-05-08 13:46:05,680 - distillation - INFO - Processing batch 1/89
2025-05-08 13:47:01,219 - distillation - INFO - Processing output 1/4 in batch 1
2025-05-08 13:47:01,220 - distillation - INFO - Processing output 2/4 in batch 1
2025-05-08 13:47:01,224 - distillation - INFO - Processing output 3/4 in batch 1
2025-05-08 13:47:01,227 - distillation - INFO - Processing output 4/4 in batch 1
2025-05-08 13:47:01,479 - distillation - INFO - Processing batch 2/89
2025-05-08 13:47:44,846 - distillation - INFO - Processing output 1/4 in batch 2
2025-05-08 13:47:44,848 - distillation - INFO - Processing output 2/4 in batch 2
2025-05-08 13:47:44,849 - distillation - INFO - Processing output 3/4 in batch 2
2025-05-08 13:47:44,851 - distillation - INFO - Processing output 4/4 in batch 2
2025-05-08 13:47:45,091 - distillation - INFO - Processing batch 3/89
2025-05-08 13:48:21,678 - distillation - INFO - Processing output 1/4 in batch 3
2025-05-08 13:48:21,682 - distillation - INFO - Processing output 2/4 in batch 3
2025-05-08 13:48:21,685 - distillation - INFO - Processing output 3/4 in batch 3
2025-05-08 13:48:21,688 - distillation - INFO - Processing output 4/4 in batch 3
2025-05-08 13:48:21,897 - distillation - INFO - Processing batch 4/89
2025-05-08 13:49:28,331 - distillation - INFO - Processing output 1/4 in batch 4
2025-05-08 13:49:28,332 - distillation - INFO - Processing output 2/4 in batch 4
2025-05-08 13:49:28,332 - distillation - INFO - Processing output 3/4 in batch 4
2025-05-08 13:49:28,333 - distillation - INFO - Processing output 4/4 in batch 4
2025-05-08 13:49:28,482 - distillation - INFO - Processing batch 5/89
2025-05-08 13:50:38,081 - distillation - INFO - Processing output 1/4 in batch 5
2025-05-08 13:50:38,083 - distillation - INFO - Processing output 2/4 in batch 5
2025-05-08 13:50:38,084 - distillation - INFO - Processing output 3/4 in batch 5
2025-05-08 13:50:38,085 - distillation - INFO - Processing output 4/4 in batch 5
2025-05-08 13:50:38,308 - distillation - INFO - Processing batch 6/89
2025-05-08 13:51:39,498 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 14:03:43,763 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 14:03:43,772 - __main__ - INFO - RAM: 使用中 29.1GB / 95.4GB (30.5%)
2025-05-08 14:03:43,791 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 14:03:43,791 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 14:03:43,792 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 14:03:43,792 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 14:03:51,989 - __main__ - INFO - Using device: cuda
2025-05-08 14:03:51,990 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 14:03:51,991 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 14:03:51,991 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 14:03:51,991 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:03:52,808 - __main__ - INFO - Initializing student model
2025-05-08 14:03:54,206 - distillation - INFO - RAM使用状況: 31.4% (使用中: 30.0GB, 空き: 65.4GB)
2025-05-08 14:03:54,207 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:03:54,208 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 14:04:07,088 - distillation - INFO - Teacher model loaded successfully
2025-05-08 14:04:07,091 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 14:04:07,092 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 14:04:07,099 - distillation - INFO - Filtered questions: 356 out of 508 original questions
2025-05-08 14:04:07,099 - distillation - INFO - Processing batch 1/89
2025-05-08 14:04:13,961 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 14:06:05,516 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 14:06:05,521 - __main__ - INFO - RAM: 使用中 29.0GB / 95.4GB (30.4%)
2025-05-08 14:06:05,545 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 14:06:05,546 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 14:06:05,546 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 14:06:05,546 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 14:06:37,622 - __main__ - INFO - Using device: cuda
2025-05-08 14:06:37,623 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 14:06:37,623 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 14:06:37,623 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 14:06:37,623 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:06:38,479 - __main__ - INFO - Initializing student model
2025-05-08 14:06:39,565 - distillation - INFO - RAM使用状況: 31.4% (使用中: 30.0GB, 空き: 65.4GB)
2025-05-08 14:06:39,566 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:06:39,567 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 14:06:52,100 - distillation - INFO - Teacher model loaded successfully
2025-05-08 14:06:52,101 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 14:06:52,102 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 14:06:52,105 - distillation - INFO - Filtered questions: 356 out of 508 original questions
2025-05-08 14:06:52,105 - distillation - INFO - Processing batch 1/89
2025-05-08 14:07:44,344 - distillation - INFO - Processing output 1/4 in batch 1
2025-05-08 14:07:44,345 - distillation - INFO - Processing output 2/4 in batch 1
2025-05-08 14:07:44,346 - distillation - INFO - Processing output 3/4 in batch 1
2025-05-08 14:07:44,346 - distillation - INFO - Processing output 4/4 in batch 1
2025-05-08 14:07:44,519 - distillation - INFO - Processing batch 2/89
2025-05-08 14:08:50,635 - distillation - INFO - Processing output 1/4 in batch 2
2025-05-08 14:08:50,637 - distillation - INFO - Processing output 2/4 in batch 2
2025-05-08 14:08:50,638 - distillation - INFO - Processing output 3/4 in batch 2
2025-05-08 14:08:50,640 - distillation - INFO - Processing output 4/4 in batch 2
2025-05-08 14:08:50,811 - distillation - INFO - Processing batch 3/89
2025-05-08 14:09:22,472 - distillation - INFO - Processing output 1/4 in batch 3
2025-05-08 14:09:22,475 - distillation - INFO - Processing output 2/4 in batch 3
2025-05-08 14:09:22,476 - distillation - INFO - Processing output 3/4 in batch 3
2025-05-08 14:09:22,477 - distillation - INFO - Processing output 4/4 in batch 3
2025-05-08 14:09:22,688 - distillation - INFO - Processing batch 4/89
2025-05-08 14:10:31,277 - distillation - INFO - Processing output 1/4 in batch 4
2025-05-08 14:10:31,278 - distillation - INFO - Processing output 2/4 in batch 4
2025-05-08 14:10:31,278 - distillation - INFO - Processing output 3/4 in batch 4
2025-05-08 14:10:31,279 - distillation - INFO - Processing output 4/4 in batch 4
2025-05-08 14:10:31,430 - distillation - INFO - Processing batch 5/89
2025-05-08 14:11:28,295 - distillation - INFO - Processing output 1/4 in batch 5
2025-05-08 14:11:28,295 - distillation - INFO - Processing output 2/4 in batch 5
2025-05-08 14:11:28,296 - distillation - INFO - Processing output 3/4 in batch 5
2025-05-08 14:11:28,296 - distillation - INFO - Processing output 4/4 in batch 5
2025-05-08 14:11:28,460 - distillation - INFO - Processing batch 6/89
2025-05-08 14:12:27,712 - distillation - INFO - Processing output 1/4 in batch 6
2025-05-08 14:12:27,713 - distillation - INFO - Processing output 2/4 in batch 6
2025-05-08 14:12:27,714 - distillation - INFO - Processing output 3/4 in batch 6
2025-05-08 14:12:27,714 - distillation - INFO - Processing output 4/4 in batch 6
2025-05-08 14:12:27,904 - distillation - INFO - Processing batch 7/89
2025-05-08 14:13:13,704 - distillation - INFO - Processing output 1/4 in batch 7
2025-05-08 14:13:13,705 - distillation - INFO - Processing output 2/4 in batch 7
2025-05-08 14:13:13,705 - distillation - INFO - Processing output 3/4 in batch 7
2025-05-08 14:13:13,705 - distillation - INFO - Processing output 4/4 in batch 7
2025-05-08 14:13:13,862 - distillation - INFO - Processing batch 8/89
2025-05-08 14:14:21,813 - distillation - INFO - Processing output 1/4 in batch 8
2025-05-08 14:14:21,813 - distillation - INFO - Processing output 2/4 in batch 8
2025-05-08 14:14:21,814 - distillation - INFO - Processing output 3/4 in batch 8
2025-05-08 14:14:21,817 - distillation - INFO - Processing output 4/4 in batch 8
2025-05-08 14:14:21,979 - distillation - INFO - Processing batch 9/89
2025-05-08 14:15:17,375 - distillation - INFO - Processing output 1/4 in batch 9
2025-05-08 14:15:17,377 - distillation - INFO - Processing output 2/4 in batch 9
2025-05-08 14:15:17,377 - distillation - INFO - Processing output 3/4 in batch 9
2025-05-08 14:15:17,378 - distillation - INFO - Processing output 4/4 in batch 9
2025-05-08 14:15:17,534 - distillation - INFO - Processing batch 10/89
2025-05-08 14:16:15,176 - distillation - INFO - Processing output 1/4 in batch 10
2025-05-08 14:16:15,177 - distillation - INFO - Processing output 2/4 in batch 10
2025-05-08 14:16:15,177 - distillation - INFO - Processing output 3/4 in batch 10
2025-05-08 14:16:15,178 - distillation - INFO - Processing output 4/4 in batch 10
2025-05-08 14:16:15,345 - distillation - INFO - Processing batch 11/89
2025-05-08 14:17:07,582 - distillation - INFO - Processing output 1/4 in batch 11
2025-05-08 14:17:07,584 - distillation - INFO - Processing output 2/4 in batch 11
2025-05-08 14:17:07,585 - distillation - INFO - Processing output 3/4 in batch 11
2025-05-08 14:17:07,586 - distillation - INFO - Processing output 4/4 in batch 11
2025-05-08 14:17:07,746 - __main__ - CRITICAL - 予期しないエラーが発生しました: local variable 'json' referenced before assignment
2025-05-08 14:17:07,747 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 843, in prepare_distillation_data
    json.dump(distillation_data, f, ensure_ascii=False, indent=2)
UnboundLocalError: local variable 'json' referenced before assignment

2025-05-08 14:17:07,909 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 16:37:13,041 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 16:37:13,047 - __main__ - INFO - RAM: 使用中 25.9GB / 95.4GB (27.1%)
2025-05-08 16:37:13,081 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 16:37:13,081 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 16:37:13,081 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 16:37:13,081 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 16:37:20,523 - __main__ - INFO - Using device: cuda
2025-05-08 16:37:20,523 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 16:37:20,524 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 16:37:20,524 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 16:37:20,524 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 16:37:21,029 - __main__ - INFO - Initializing student model
2025-05-08 16:37:21,736 - distillation - INFO - トークナイザーのパディング方向を左側に設定します
2025-05-08 16:37:21,741 - distillation - INFO - RAM使用状況: 28.0% (使用中: 26.7GB, 空き: 68.7GB)
2025-05-08 16:37:21,741 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 16:37:21,741 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 16:37:21,742 - __main__ - CRITICAL - 予期しないエラーが発生しました: object of type 'NoneType' has no len()
2025-05-08 16:37:21,742 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 232, in prepare_distillation_data
    if num_samples < len(questions):
TypeError: object of type 'NoneType' has no len()

2025-05-08 16:37:21,804 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 16:48:26,456 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 16:48:26,461 - __main__ - INFO - RAM: 使用中 27.2GB / 95.4GB (28.6%)
2025-05-08 16:48:26,477 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 16:48:26,477 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 16:48:26,478 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 16:48:26,478 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 16:50:17,620 - __main__ - INFO - Using device: cuda
2025-05-08 16:50:17,621 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 16:50:17,621 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 16:50:17,621 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 16:50:17,621 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 16:50:18,161 - __main__ - INFO - Initializing student model
2025-05-08 16:50:19,689 - distillation - INFO - トークナイザーのパディング方向を左側に設定します
2025-05-08 16:50:19,694 - distillation - INFO - RAM使用状況: 29.4% (使用中: 28.0GB, 空き: 67.4GB)
2025-05-08 16:50:19,695 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 16:50:19,695 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 16:50:19,696 - distillation - INFO - 読み込んだ質問数: 508
2025-05-08 16:50:19,696 - distillation - WARNING - フィルタリングの結果が空になりました。元の質問を使用します。
2025-05-08 16:50:19,696 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 16:50:19,696 - distillation - INFO - 読み込んだ質問数: 508
2025-05-08 16:50:19,696 - distillation - WARNING - フィルタリングの結果が空になりました。元の質問を使用します。
2025-05-08 16:50:19,697 - __main__ - INFO - Starting distillation
2025-05-08 16:50:19,697 - __main__ - ERROR - 知識蒸留中にエラーが発生しました: 'KnowledgeDistiller' object has no attribute 'distill'
2025-05-08 16:50:19,697 - __main__ - INFO - 最後のチェックポイントから再開できる可能性があります
2025-05-08 16:50:19,774 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 17:00:59,701 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 17:00:59,706 - __main__ - INFO - RAM: 使用中 27.3GB / 95.4GB (28.6%)
2025-05-08 17:00:59,723 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 17:00:59,724 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 17:00:59,724 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 17:00:59,724 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 17:01:21,183 - __main__ - INFO - Using device: cuda
2025-05-08 17:01:21,183 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 17:01:21,183 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 17:01:21,184 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 17:01:21,184 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 17:01:22,005 - __main__ - INFO - Initializing student model
2025-05-08 17:01:23,409 - distillation - INFO - トークナイザーのパディング方向を左側に設定します
2025-05-08 17:01:23,413 - distillation - INFO - RAM使用状況: 29.3% (使用中: 28.0GB, 空き: 67.4GB)
2025-05-08 17:01:23,414 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 17:01:23,414 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 17:01:23,415 - distillation - INFO - 読み込んだ質問数: 508
2025-05-08 17:01:23,415 - distillation - INFO - フィルタリングに使用するキーワード数: 76
2025-05-08 17:01:23,416 - distillation - INFO - Filtered questions: 436 out of 508 original questions
2025-05-08 17:01:23,416 - distillation - INFO - フィルタリング後の質問数: 436
2025-05-08 17:01:23,416 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 17:01:23,417 - distillation - INFO - 読み込んだ質問数: 508
2025-05-08 17:01:23,417 - distillation - INFO - フィルタリングに使用するキーワード数: 76
2025-05-08 17:01:23,418 - distillation - INFO - Filtered questions: 436 out of 508 original questions
2025-05-08 17:01:23,418 - distillation - INFO - フィルタリング後の質問数: 436
2025-05-08 17:01:23,418 - __main__ - INFO - Starting distillation
2025-05-08 17:01:23,418 - distillation - INFO - Starting distillation process
2025-05-08 17:01:23,419 - distillation - INFO - Loading data from ./models\train_data.json
2025-05-08 17:01:23,419 - distillation - ERROR - 蒸留処理中にエラーが発生: [Errno 2] No such file or directory: './models\\train_data.json'
2025-05-08 17:01:23,419 - distillation - ERROR - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 601, in distill
    train_dataset = DistillationDataset(train_data_path, self.tokenizer,
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 52, in __init__
    with open(data_path, 'r', encoding='utf-8') as f:
FileNotFoundError: [Errno 2] No such file or directory: './models\\train_data.json'

2025-05-08 17:01:23,476 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 17:01:23,476 - __main__ - INFO - Saving model in Hugging Face format as lorinta/lal_v3
2025-05-08 17:01:23,492 - __main__ - CRITICAL - 予期しないエラーが発生しました: 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.
2025-05-08 17:01:23,494 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\serialization.py", line 869, in _check_seekable
    f.seek(f.tell())
AttributeError: 'NoneType' object has no attribute 'seek'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 511, in run_distillation
    student_model.load_state_dict(torch.load(best_model_path)["model_state_dict"])
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\serialization.py", line 1475, in load
    with _open_file_like(f, "rb") as opened_file:
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\serialization.py", line 764, in _open_file_like
    return _open_buffer_reader(name_or_buffer)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\serialization.py", line 749, in __init__
    _check_seekable(buffer)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\serialization.py", line 872, in _check_seekable
    raise_err_msg(["seek", "tell"], e)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\serialization.py", line 865, in raise_err_msg
    raise type(e)(msg)
AttributeError: 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.

2025-05-08 17:01:23,554 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 17:20:04,551 - __main__ - INFO - Generating 5000 distillation examples focusing on: h,i,g,h,s,c,h,o,o,l,,,e,l,e,c,t,r,o,n,i,c,s,,,i,t
2025-05-08 17:20:04,552 - __main__ - CRITICAL - 予期しないエラーが発生しました: name 'distiller' is not defined
2025-05-08 17:20:04,554 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 432, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 248, in run_distillation
    distiller.prepare_distillation_data(
NameError: name 'distiller' is not defined

2025-05-08 17:20:04,642 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 17:31:16,095 - __main__ - INFO - RAM: 使用中 27.2GB / 95.4GB (28.5%)
2025-05-08 17:31:16,132 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 17:31:16,132 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 17:31:16,133 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 17:31:16,133 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 17:31:25,882 - __main__ - INFO - 重点分野: highschool, electronics, it
2025-05-08 17:31:25,882 - __main__ - INFO - Using device: cuda
2025-05-08 17:31:25,883 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 17:31:25,883 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 17:31:25,883 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 17:31:25,883 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 17:31:26,385 - __main__ - INFO - パッドトークンがないため、EOSトークンをパッドトークンとして設定しました
2025-05-08 17:31:26,386 - __main__ - INFO - Initializing student model
2025-05-08 17:31:26,387 - __main__ - ERROR - 学生モデルの初期化に失敗: BrainModel.__init__() got an unexpected keyword argument 'tokenizer_name'
2025-05-08 17:31:26,387 - __main__ - CRITICAL - 学生モデルが初期化できませんでした。終了します。
2025-05-08 17:31:26,442 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 17:39:45,162 - __main__ - INFO - RAM: 使用中 27.4GB / 95.4GB (28.8%)
2025-05-08 17:39:45,182 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 17:39:45,182 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 17:39:45,183 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 17:39:45,183 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 17:39:49,679 - __main__ - INFO - 重点分野: highschool, electronics, it
2025-05-08 17:39:49,679 - __main__ - INFO - Using device: cuda
2025-05-08 17:39:49,679 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 17:39:49,680 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 17:39:49,680 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 17:39:49,680 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 17:39:50,519 - __main__ - INFO - パッドトークンがないため、EOSトークンをパッドトークンとして設定しました
2025-05-08 17:39:50,550 - __main__ - INFO - 語彙サイズ: 128256
2025-05-08 17:39:50,550 - __main__ - INFO - Initializing student model
2025-05-08 17:39:52,847 - __main__ - INFO - Initializing 4-bit quantization
2025-05-08 17:39:52,848 - __main__ - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 17:40:07,266 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-05-08 17:40:16,803 - __main__ - INFO - 教師モデルのロードが完了しました
2025-05-08 17:40:16,804 - distillation - INFO - トークナイザーのパディング方向を左側に設定します
2025-05-08 17:40:16,811 - distillation - INFO - RAM使用状況: 30.7% (使用中: 29.2GB, 空き: 66.1GB)
2025-05-08 17:40:16,812 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool, electronics, it
2025-05-08 17:40:16,812 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 17:40:16,813 - distillation - INFO - 読み込んだ質問数: 508
2025-05-08 17:40:16,813 - distillation - INFO - フィルタリングに使用するキーワード数: 76
2025-05-08 17:40:16,816 - distillation - INFO - Filtered questions: 436 out of 508 original questions
2025-05-08 17:40:16,816 - distillation - INFO - フィルタリング後の質問数: 436
2025-05-08 17:40:16,816 - distillation - INFO - Processing batch 1/218
2025-05-08 17:40:46,617 - distillation - INFO - Processing output 1/2 in batch 1
2025-05-08 17:40:46,618 - distillation - INFO - Processing output 2/2 in batch 1
2025-05-08 17:40:46,788 - distillation - INFO - Processing batch 2/218
2025-05-08 17:41:15,743 - distillation - INFO - Processing output 1/2 in batch 2
2025-05-08 17:41:15,744 - distillation - INFO - Processing output 2/2 in batch 2
2025-05-08 17:41:15,912 - distillation - INFO - Processing batch 3/218
2025-05-08 17:41:33,089 - distillation - INFO - Processing output 1/2 in batch 3
2025-05-08 17:41:33,091 - distillation - INFO - Processing output 2/2 in batch 3
2025-05-08 17:41:33,242 - distillation - INFO - Processing batch 4/218
2025-05-08 17:41:56,857 - distillation - INFO - Processing output 1/2 in batch 4
2025-05-08 17:41:56,859 - distillation - INFO - Processing output 2/2 in batch 4
2025-05-08 17:41:57,025 - distillation - INFO - Processing batch 5/218
2025-05-08 17:42:25,313 - distillation - INFO - Processing output 1/2 in batch 5
2025-05-08 17:42:25,314 - distillation - INFO - Processing output 2/2 in batch 5
2025-05-08 17:42:25,465 - distillation - INFO - Processing batch 6/218
2025-05-08 17:42:44,236 - distillation - INFO - Processing output 1/2 in batch 6
2025-05-08 17:42:44,237 - distillation - INFO - Processing output 2/2 in batch 6
2025-05-08 17:42:44,390 - distillation - INFO - Processing batch 7/218
2025-05-08 17:43:05,935 - distillation - INFO - Processing output 1/2 in batch 7
2025-05-08 17:43:05,937 - distillation - INFO - Processing output 2/2 in batch 7
2025-05-08 17:43:06,099 - distillation - INFO - Processing batch 8/218
2025-05-08 17:43:29,775 - distillation - INFO - Processing output 1/2 in batch 8
2025-05-08 17:43:29,776 - distillation - INFO - Processing output 2/2 in batch 8
2025-05-08 17:43:29,938 - distillation - INFO - Processing batch 9/218
2025-05-08 17:43:40,439 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 17:51:53,142 - __main__ - INFO - RAM: 使用中 27.4GB / 95.4GB (28.7%)
2025-05-08 17:51:53,167 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 17:51:53,168 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 17:51:53,169 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 17:51:53,169 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 17:52:00,098 - __main__ - INFO - 重点分野: highschool, electronics, it
2025-05-08 17:52:00,099 - __main__ - INFO - Using device: cuda
2025-05-08 17:52:00,099 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 17:52:00,099 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 17:52:00,099 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 17:52:00,100 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 17:52:00,956 - __main__ - INFO - パッドトークンがないため、EOSトークンをパッドトークンとして設定しました
2025-05-08 17:52:00,973 - __main__ - INFO - 語彙サイズ: 128256
2025-05-08 17:52:00,974 - __main__ - INFO - Initializing student model
2025-05-08 17:52:02,801 - __main__ - INFO - Initializing 8-bit quantization
2025-05-08 17:52:02,801 - __main__ - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 17:52:08,298 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-05-08 17:52:17,775 - __main__ - INFO - 教師モデルのロードが完了しました
2025-05-08 17:52:17,776 - distillation - INFO - トークナイザーのパディング方向を左側に設定します
2025-05-08 17:52:17,785 - distillation - INFO - RAM使用状況: 30.5% (使用中: 29.1GB, 空き: 66.3GB)
2025-05-08 17:52:17,787 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool, electronics, it
2025-05-08 17:52:17,788 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 17:52:17,789 - distillation - INFO - 読み込んだ質問数: 508
2025-05-08 17:52:17,789 - distillation - INFO - フィルタリングに使用するキーワード数: 76
2025-05-08 17:52:17,792 - distillation - INFO - Filtered questions: 436 out of 508 original questions
2025-05-08 17:52:17,792 - distillation - INFO - フィルタリング後の質問数: 436
2025-05-08 17:52:17,793 - distillation - INFO - Processing batch 1/218
2025-05-08 17:53:00,284 - distillation - INFO - Processing output 1/2 in batch 1
2025-05-08 17:53:00,286 - distillation - INFO - Processing output 2/2 in batch 1
2025-05-08 17:53:00,482 - distillation - INFO - Processing batch 2/218
2025-05-08 17:53:43,077 - distillation - INFO - Processing output 1/2 in batch 2
2025-05-08 17:53:43,078 - distillation - INFO - Processing output 2/2 in batch 2
2025-05-08 17:53:43,242 - distillation - INFO - Processing batch 3/218
2025-05-08 17:54:26,122 - distillation - INFO - Processing output 1/2 in batch 3
2025-05-08 17:54:26,123 - distillation - INFO - Processing output 2/2 in batch 3
2025-05-08 17:54:26,290 - distillation - INFO - Processing batch 4/218
2025-05-08 17:55:12,227 - distillation - INFO - Processing output 1/2 in batch 4
2025-05-08 17:55:12,227 - distillation - INFO - Processing output 2/2 in batch 4
2025-05-08 17:55:12,384 - distillation - INFO - Processing batch 5/218
2025-05-08 17:55:43,808 - distillation - INFO - Processing output 1/2 in batch 5
2025-05-08 17:55:43,810 - distillation - INFO - Processing output 2/2 in batch 5
2025-05-08 17:55:43,969 - distillation - INFO - Processing batch 6/218
2025-05-08 17:56:18,540 - distillation - INFO - Processing output 1/2 in batch 6
2025-05-08 17:56:18,541 - distillation - INFO - Processing output 2/2 in batch 6
2025-05-08 17:56:18,703 - distillation - INFO - Processing batch 7/218
2025-05-08 17:57:03,512 - distillation - INFO - Processing output 1/2 in batch 7
2025-05-08 17:57:03,513 - distillation - INFO - Processing output 2/2 in batch 7
2025-05-08 17:57:03,670 - distillation - INFO - Processing batch 8/218
2025-05-08 17:57:39,889 - distillation - INFO - Processing output 1/2 in batch 8
2025-05-08 17:57:39,889 - distillation - INFO - Processing output 2/2 in batch 8
2025-05-08 17:57:40,049 - distillation - INFO - Processing batch 9/218
2025-05-08 17:58:23,845 - distillation - INFO - Processing output 1/2 in batch 9
2025-05-08 17:58:23,846 - distillation - INFO - Processing output 2/2 in batch 9
2025-05-08 17:58:24,025 - distillation - INFO - Processing batch 10/218
2025-05-08 17:58:55,600 - distillation - INFO - Processing output 1/2 in batch 10
2025-05-08 17:58:55,601 - distillation - INFO - Processing output 2/2 in batch 10
2025-05-08 17:58:55,771 - distillation - INFO - Processing batch 11/218
2025-05-08 17:59:40,727 - distillation - INFO - Processing output 1/2 in batch 11
2025-05-08 17:59:40,727 - distillation - INFO - Processing output 2/2 in batch 11
2025-05-08 17:59:40,896 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_20
2025-05-08 17:59:40,896 - distillation - INFO - Processing batch 12/218
2025-05-08 18:00:16,645 - distillation - INFO - Processing output 1/2 in batch 12
2025-05-08 18:00:16,646 - distillation - INFO - Processing output 2/2 in batch 12
2025-05-08 18:00:16,810 - distillation - INFO - Processing batch 13/218
2025-05-08 18:00:40,639 - distillation - INFO - Processing output 1/2 in batch 13
2025-05-08 18:00:40,640 - distillation - INFO - Processing output 2/2 in batch 13
2025-05-08 18:00:40,795 - distillation - INFO - Processing batch 14/218
2025-05-08 18:01:13,000 - distillation - INFO - Processing output 1/2 in batch 14
2025-05-08 18:01:13,000 - distillation - INFO - Processing output 2/2 in batch 14
2025-05-08 18:01:13,159 - distillation - INFO - Processing batch 15/218
2025-05-08 18:01:56,363 - distillation - INFO - Processing output 1/2 in batch 15
2025-05-08 18:01:56,364 - distillation - INFO - Processing output 2/2 in batch 15
2025-05-08 18:01:56,526 - distillation - INFO - Processing batch 16/218
2025-05-08 18:02:30,176 - distillation - INFO - Processing output 1/2 in batch 16
2025-05-08 18:02:30,176 - distillation - INFO - Processing output 2/2 in batch 16
2025-05-08 18:02:30,338 - distillation - INFO - Processing batch 17/218
2025-05-08 18:03:10,535 - distillation - INFO - Processing output 1/2 in batch 17
2025-05-08 18:03:10,536 - distillation - INFO - Processing output 2/2 in batch 17
2025-05-08 18:03:10,698 - distillation - INFO - Processing batch 18/218
2025-05-08 18:03:46,883 - distillation - INFO - Processing output 1/2 in batch 18
2025-05-08 18:03:46,884 - distillation - INFO - Processing output 2/2 in batch 18
2025-05-08 18:03:47,038 - distillation - INFO - Processing batch 19/218
2025-05-08 18:04:23,400 - distillation - INFO - Processing output 1/2 in batch 19
2025-05-08 18:04:23,401 - distillation - INFO - Processing output 2/2 in batch 19
2025-05-08 18:04:23,566 - distillation - INFO - Processing batch 20/218
2025-05-08 18:04:59,773 - distillation - INFO - Processing output 1/2 in batch 20
2025-05-08 18:04:59,774 - distillation - INFO - Processing output 2/2 in batch 20
2025-05-08 18:04:59,946 - distillation - INFO - Processing batch 21/218
2025-05-08 18:05:28,822 - distillation - INFO - Processing output 1/2 in batch 21
2025-05-08 18:05:28,823 - distillation - INFO - Processing output 2/2 in batch 21
2025-05-08 18:05:28,980 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_40
2025-05-08 18:05:28,981 - distillation - INFO - Processing batch 22/218
2025-05-08 18:06:12,632 - distillation - INFO - Processing output 1/2 in batch 22
2025-05-08 18:06:12,634 - distillation - INFO - Processing output 2/2 in batch 22
2025-05-08 18:06:12,803 - distillation - INFO - Processing batch 23/218
2025-05-08 18:06:57,116 - distillation - INFO - Processing output 1/2 in batch 23
2025-05-08 18:06:57,117 - distillation - INFO - Processing output 2/2 in batch 23
2025-05-08 18:06:57,276 - distillation - INFO - Processing batch 24/218
2025-05-08 18:07:29,837 - distillation - INFO - Processing output 1/2 in batch 24
2025-05-08 18:07:29,837 - distillation - INFO - Processing output 2/2 in batch 24
2025-05-08 18:07:30,000 - distillation - INFO - Processing batch 25/218
2025-05-08 18:08:03,008 - distillation - INFO - Processing output 1/2 in batch 25
2025-05-08 18:08:03,008 - distillation - INFO - Processing output 2/2 in batch 25
2025-05-08 18:08:03,167 - distillation - INFO - Processing batch 26/218
2025-05-08 18:08:38,579 - distillation - INFO - Processing output 1/2 in batch 26
2025-05-08 18:08:38,580 - distillation - INFO - Processing output 2/2 in batch 26
2025-05-08 18:08:38,786 - distillation - INFO - Processing batch 27/218
2025-05-08 18:09:22,590 - distillation - INFO - Processing output 1/2 in batch 27
2025-05-08 18:09:22,591 - distillation - INFO - Processing output 2/2 in batch 27
2025-05-08 18:09:22,748 - distillation - INFO - Processing batch 28/218
2025-05-08 18:09:58,283 - distillation - INFO - Processing output 1/2 in batch 28
2025-05-08 18:09:58,284 - distillation - INFO - Processing output 2/2 in batch 28
2025-05-08 18:09:58,448 - distillation - INFO - Processing batch 29/218
2025-05-08 18:10:38,395 - distillation - INFO - Processing output 1/2 in batch 29
2025-05-08 18:10:38,396 - distillation - INFO - Processing output 2/2 in batch 29
2025-05-08 18:10:38,555 - distillation - INFO - Processing batch 30/218
2025-05-08 18:11:14,380 - distillation - INFO - Processing output 1/2 in batch 30
2025-05-08 18:11:14,381 - distillation - INFO - Processing output 2/2 in batch 30
2025-05-08 18:11:14,549 - distillation - INFO - Processing batch 31/218
2025-05-08 18:11:55,438 - distillation - INFO - Processing output 1/2 in batch 31
2025-05-08 18:11:55,439 - distillation - INFO - Processing output 2/2 in batch 31
2025-05-08 18:11:55,607 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_60
2025-05-08 18:11:55,608 - distillation - INFO - Processing batch 32/218
2025-05-08 18:12:28,001 - distillation - INFO - Processing output 1/2 in batch 32
2025-05-08 18:12:28,002 - distillation - INFO - Processing output 2/2 in batch 32
2025-05-08 18:12:28,163 - distillation - INFO - Processing batch 33/218
2025-05-08 18:13:09,700 - distillation - INFO - Processing output 1/2 in batch 33
2025-05-08 18:13:09,701 - distillation - INFO - Processing output 2/2 in batch 33
2025-05-08 18:13:09,865 - distillation - INFO - Processing batch 34/218
2025-05-08 18:13:52,007 - distillation - INFO - Processing output 1/2 in batch 34
2025-05-08 18:13:52,007 - distillation - INFO - Processing output 2/2 in batch 34
2025-05-08 18:13:52,164 - distillation - INFO - Processing batch 35/218
2025-05-08 18:14:34,697 - distillation - INFO - Processing output 1/2 in batch 35
2025-05-08 18:14:34,697 - distillation - INFO - Processing output 2/2 in batch 35
2025-05-08 18:14:34,862 - distillation - INFO - Processing batch 36/218
2025-05-08 18:15:04,641 - distillation - INFO - Processing output 1/2 in batch 36
2025-05-08 18:15:04,643 - distillation - INFO - Processing output 2/2 in batch 36
2025-05-08 18:15:04,807 - distillation - INFO - Processing batch 37/218
2025-05-08 18:15:50,134 - distillation - INFO - Processing output 1/2 in batch 37
2025-05-08 18:15:50,135 - distillation - INFO - Processing output 2/2 in batch 37
2025-05-08 18:15:50,296 - distillation - INFO - Processing batch 38/218
2025-05-08 18:16:35,878 - distillation - INFO - Processing output 1/2 in batch 38
2025-05-08 18:16:35,879 - distillation - INFO - Processing output 2/2 in batch 38
2025-05-08 18:16:36,042 - distillation - INFO - Processing batch 39/218
2025-05-08 18:17:07,547 - distillation - INFO - Processing output 1/2 in batch 39
2025-05-08 18:17:07,547 - distillation - INFO - Processing output 2/2 in batch 39
2025-05-08 18:17:07,714 - distillation - INFO - Processing batch 40/218
2025-05-08 18:17:52,527 - distillation - INFO - Processing output 1/2 in batch 40
2025-05-08 18:17:52,527 - distillation - INFO - Processing output 2/2 in batch 40
2025-05-08 18:17:52,686 - distillation - INFO - Processing batch 41/218
2025-05-08 18:18:38,636 - distillation - INFO - Processing output 1/2 in batch 41
2025-05-08 18:18:38,637 - distillation - INFO - Processing output 2/2 in batch 41
2025-05-08 18:18:38,804 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_80
2025-05-08 18:18:38,804 - distillation - INFO - Processing batch 42/218
2025-05-08 18:19:09,572 - distillation - INFO - Processing output 1/2 in batch 42
2025-05-08 18:19:09,574 - distillation - INFO - Processing output 2/2 in batch 42
2025-05-08 18:19:09,728 - distillation - INFO - Processing batch 43/218
2025-05-08 18:19:41,097 - distillation - INFO - Processing output 1/2 in batch 43
2025-05-08 18:19:41,097 - distillation - INFO - Processing output 2/2 in batch 43
2025-05-08 18:19:41,250 - distillation - INFO - Processing batch 44/218
2025-05-08 18:20:06,324 - distillation - INFO - Processing output 1/2 in batch 44
2025-05-08 18:20:06,325 - distillation - INFO - Processing output 2/2 in batch 44
2025-05-08 18:20:06,496 - distillation - INFO - Processing batch 45/218
2025-05-08 18:20:37,025 - distillation - INFO - Processing output 1/2 in batch 45
2025-05-08 18:20:37,026 - distillation - INFO - Processing output 2/2 in batch 45
2025-05-08 18:20:37,198 - distillation - INFO - Processing batch 46/218
2025-05-08 18:21:11,382 - distillation - INFO - Processing output 1/2 in batch 46
2025-05-08 18:21:11,383 - distillation - INFO - Processing output 2/2 in batch 46
2025-05-08 18:21:11,550 - distillation - INFO - Processing batch 47/218
2025-05-08 18:21:56,762 - distillation - INFO - Processing output 1/2 in batch 47
2025-05-08 18:21:56,763 - distillation - INFO - Processing output 2/2 in batch 47
2025-05-08 18:21:56,924 - distillation - INFO - Processing batch 48/218
2025-05-08 18:22:33,260 - distillation - INFO - Processing output 1/2 in batch 48
2025-05-08 18:22:33,261 - distillation - INFO - Processing output 2/2 in batch 48
2025-05-08 18:22:33,426 - distillation - INFO - Processing batch 49/218
2025-05-08 18:23:19,118 - distillation - INFO - Processing output 1/2 in batch 49
2025-05-08 18:23:19,119 - distillation - INFO - Processing output 2/2 in batch 49
2025-05-08 18:23:19,280 - distillation - INFO - Processing batch 50/218
2025-05-08 18:23:58,284 - distillation - INFO - Processing output 1/2 in batch 50
2025-05-08 18:23:58,285 - distillation - INFO - Processing output 2/2 in batch 50
2025-05-08 18:23:58,438 - distillation - INFO - Processing batch 51/218
2025-05-08 18:24:40,903 - distillation - INFO - Processing output 1/2 in batch 51
2025-05-08 18:24:40,904 - distillation - INFO - Processing output 2/2 in batch 51
2025-05-08 18:24:41,066 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_100
2025-05-08 18:24:41,067 - distillation - INFO - Processing batch 52/218
2025-05-08 18:25:01,447 - distillation - INFO - Processing output 1/2 in batch 52
2025-05-08 18:25:01,447 - distillation - INFO - Processing output 2/2 in batch 52
2025-05-08 18:25:01,604 - distillation - INFO - Processing batch 53/218
2025-05-08 18:25:34,211 - distillation - INFO - Processing output 1/2 in batch 53
2025-05-08 18:25:34,212 - distillation - INFO - Processing output 2/2 in batch 53
2025-05-08 18:25:34,372 - distillation - INFO - Processing batch 54/218
2025-05-08 18:26:07,314 - distillation - INFO - Processing output 1/2 in batch 54
2025-05-08 18:26:07,314 - distillation - INFO - Processing output 2/2 in batch 54
2025-05-08 18:26:07,468 - distillation - INFO - Processing batch 55/218
2025-05-08 18:26:48,807 - distillation - INFO - Processing output 1/2 in batch 55
2025-05-08 18:26:48,807 - distillation - INFO - Processing output 2/2 in batch 55
2025-05-08 18:26:48,964 - distillation - INFO - Processing batch 56/218
2025-05-08 18:27:29,019 - distillation - INFO - Processing output 1/2 in batch 56
2025-05-08 18:27:29,020 - distillation - INFO - Processing output 2/2 in batch 56
2025-05-08 18:27:29,179 - distillation - INFO - Processing batch 57/218
2025-05-08 18:28:09,687 - distillation - INFO - Processing output 1/2 in batch 57
2025-05-08 18:28:09,688 - distillation - INFO - Processing output 2/2 in batch 57
2025-05-08 18:28:09,840 - distillation - INFO - Processing batch 58/218
2025-05-08 18:28:43,049 - distillation - INFO - Processing output 1/2 in batch 58
2025-05-08 18:28:43,050 - distillation - INFO - Processing output 2/2 in batch 58
2025-05-08 18:28:43,210 - distillation - INFO - Processing batch 59/218
2025-05-08 18:29:21,470 - distillation - INFO - Processing output 1/2 in batch 59
2025-05-08 18:29:21,471 - distillation - INFO - Processing output 2/2 in batch 59
2025-05-08 18:29:21,632 - distillation - INFO - Processing batch 60/218
2025-05-08 18:29:44,567 - distillation - INFO - Processing output 1/2 in batch 60
2025-05-08 18:29:44,568 - distillation - INFO - Processing output 2/2 in batch 60
2025-05-08 18:29:44,728 - distillation - INFO - Processing batch 61/218
2025-05-08 18:30:31,003 - distillation - INFO - Processing output 1/2 in batch 61
2025-05-08 18:30:31,004 - distillation - INFO - Processing output 2/2 in batch 61
2025-05-08 18:30:31,164 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_120
2025-05-08 18:30:31,165 - distillation - INFO - Processing batch 62/218
2025-05-08 18:31:17,890 - distillation - INFO - Processing output 1/2 in batch 62
2025-05-08 18:31:17,891 - distillation - INFO - Processing output 2/2 in batch 62
2025-05-08 18:31:18,053 - distillation - INFO - Processing batch 63/218
2025-05-08 18:31:48,440 - distillation - INFO - Processing output 1/2 in batch 63
2025-05-08 18:31:48,441 - distillation - INFO - Processing output 2/2 in batch 63
2025-05-08 18:31:48,598 - distillation - INFO - Processing batch 64/218
2025-05-08 18:32:29,783 - distillation - INFO - Processing output 1/2 in batch 64
2025-05-08 18:32:29,785 - distillation - INFO - Processing output 2/2 in batch 64
2025-05-08 18:32:29,940 - distillation - INFO - Processing batch 65/218
2025-05-08 18:33:06,213 - distillation - INFO - Processing output 1/2 in batch 65
2025-05-08 18:33:06,214 - distillation - INFO - Processing output 2/2 in batch 65
2025-05-08 18:33:06,378 - distillation - INFO - Processing batch 66/218
2025-05-08 18:33:52,291 - distillation - INFO - Processing output 1/2 in batch 66
2025-05-08 18:33:52,292 - distillation - INFO - Processing output 2/2 in batch 66
2025-05-08 18:33:52,456 - distillation - INFO - Processing batch 67/218
2025-05-08 18:34:38,488 - distillation - INFO - Processing output 1/2 in batch 67
2025-05-08 18:34:38,489 - distillation - INFO - Processing output 2/2 in batch 67
2025-05-08 18:34:38,645 - distillation - INFO - Processing batch 68/218
2025-05-08 18:35:21,152 - distillation - INFO - Processing output 1/2 in batch 68
2025-05-08 18:35:21,153 - distillation - INFO - Processing output 2/2 in batch 68
2025-05-08 18:35:21,308 - distillation - INFO - Processing batch 69/218
2025-05-08 18:35:57,795 - distillation - INFO - Processing output 1/2 in batch 69
2025-05-08 18:35:57,796 - distillation - INFO - Processing output 2/2 in batch 69
2025-05-08 18:35:57,960 - distillation - INFO - Processing batch 70/218
2025-05-08 18:36:38,226 - distillation - INFO - Processing output 1/2 in batch 70
2025-05-08 18:36:38,228 - distillation - INFO - Processing output 2/2 in batch 70
2025-05-08 18:36:38,391 - distillation - INFO - Processing batch 71/218
2025-05-08 18:37:15,432 - distillation - INFO - Processing output 1/2 in batch 71
2025-05-08 18:37:15,433 - distillation - INFO - Processing output 2/2 in batch 71
2025-05-08 18:37:15,596 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_140
2025-05-08 18:37:15,596 - distillation - INFO - Processing batch 72/218
2025-05-08 18:37:57,810 - distillation - INFO - Processing output 1/2 in batch 72
2025-05-08 18:37:57,811 - distillation - INFO - Processing output 2/2 in batch 72
2025-05-08 18:37:57,972 - distillation - INFO - Processing batch 73/218
2025-05-08 18:38:41,330 - distillation - INFO - Processing output 1/2 in batch 73
2025-05-08 18:38:41,331 - distillation - INFO - Processing output 2/2 in batch 73
2025-05-08 18:38:41,489 - distillation - INFO - Processing batch 74/218
2025-05-08 18:39:29,260 - distillation - INFO - Processing output 1/2 in batch 74
2025-05-08 18:39:29,261 - distillation - INFO - Processing output 2/2 in batch 74
2025-05-08 18:39:29,422 - distillation - INFO - Processing batch 75/218
2025-05-08 18:40:16,498 - distillation - INFO - Processing output 1/2 in batch 75
2025-05-08 18:40:16,499 - distillation - INFO - Processing output 2/2 in batch 75
2025-05-08 18:40:16,664 - distillation - INFO - Processing batch 76/218
2025-05-08 18:40:54,620 - distillation - INFO - Processing output 1/2 in batch 76
2025-05-08 18:40:54,620 - distillation - INFO - Processing output 2/2 in batch 76
2025-05-08 18:40:54,779 - distillation - INFO - Processing batch 77/218
2025-05-08 18:41:33,357 - distillation - INFO - Processing output 1/2 in batch 77
2025-05-08 18:41:33,358 - distillation - INFO - Processing output 2/2 in batch 77
2025-05-08 18:41:33,522 - distillation - INFO - Processing batch 78/218
2025-05-08 18:42:09,354 - distillation - INFO - Processing output 1/2 in batch 78
2025-05-08 18:42:09,356 - distillation - INFO - Processing output 2/2 in batch 78
2025-05-08 18:42:09,524 - distillation - INFO - Processing batch 79/218
2025-05-08 18:42:54,009 - distillation - INFO - Processing output 1/2 in batch 79
2025-05-08 18:42:54,010 - distillation - INFO - Processing output 2/2 in batch 79
2025-05-08 18:42:54,175 - distillation - INFO - Processing batch 80/218
2025-05-08 18:43:37,302 - distillation - INFO - Processing output 1/2 in batch 80
2025-05-08 18:43:37,303 - distillation - INFO - Processing output 2/2 in batch 80
2025-05-08 18:43:37,463 - distillation - INFO - Processing batch 81/218
2025-05-08 18:44:24,210 - distillation - INFO - Processing output 1/2 in batch 81
2025-05-08 18:44:24,211 - distillation - INFO - Processing output 2/2 in batch 81
2025-05-08 18:44:24,380 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_160
2025-05-08 18:44:24,380 - distillation - INFO - Processing batch 82/218
2025-05-08 18:45:11,895 - distillation - INFO - Processing output 1/2 in batch 82
2025-05-08 18:45:11,896 - distillation - INFO - Processing output 2/2 in batch 82
2025-05-08 18:45:12,060 - distillation - INFO - Processing batch 83/218
2025-05-08 18:45:57,856 - distillation - INFO - Processing output 1/2 in batch 83
2025-05-08 18:45:57,857 - distillation - INFO - Processing output 2/2 in batch 83
2025-05-08 18:45:58,018 - distillation - INFO - Processing batch 84/218
2025-05-08 18:46:44,739 - distillation - INFO - Processing output 1/2 in batch 84
2025-05-08 18:46:44,740 - distillation - INFO - Processing output 2/2 in batch 84
2025-05-08 18:46:44,897 - distillation - INFO - Processing batch 85/218
2025-05-08 18:47:13,473 - distillation - INFO - Processing output 1/2 in batch 85
2025-05-08 18:47:13,474 - distillation - INFO - Processing output 2/2 in batch 85
2025-05-08 18:47:13,639 - distillation - INFO - Processing batch 86/218
2025-05-08 18:47:49,512 - distillation - INFO - Processing output 1/2 in batch 86
2025-05-08 18:47:49,512 - distillation - INFO - Processing output 2/2 in batch 86
2025-05-08 18:47:49,677 - distillation - INFO - Processing batch 87/218
2025-05-08 18:48:38,402 - distillation - INFO - Processing output 1/2 in batch 87
2025-05-08 18:48:38,402 - distillation - INFO - Processing output 2/2 in batch 87
2025-05-08 18:48:38,570 - distillation - INFO - Processing batch 88/218
2025-05-08 18:49:12,874 - distillation - INFO - Processing output 1/2 in batch 88
2025-05-08 18:49:12,874 - distillation - INFO - Processing output 2/2 in batch 88
2025-05-08 18:49:13,036 - distillation - INFO - Processing batch 89/218
2025-05-08 18:49:43,197 - distillation - INFO - Processing output 1/2 in batch 89
2025-05-08 18:49:43,198 - distillation - INFO - Processing output 2/2 in batch 89
2025-05-08 18:49:43,362 - distillation - INFO - Processing batch 90/218
2025-05-08 18:50:21,496 - distillation - INFO - Processing output 1/2 in batch 90
2025-05-08 18:50:21,497 - distillation - INFO - Processing output 2/2 in batch 90
2025-05-08 18:50:21,661 - distillation - INFO - Processing batch 91/218
2025-05-08 18:51:03,001 - distillation - INFO - Processing output 1/2 in batch 91
2025-05-08 18:51:03,002 - distillation - INFO - Processing output 2/2 in batch 91
2025-05-08 18:51:03,161 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_180
2025-05-08 18:51:03,161 - distillation - INFO - Processing batch 92/218
2025-05-08 18:51:49,213 - distillation - INFO - Processing output 1/2 in batch 92
2025-05-08 18:51:49,214 - distillation - INFO - Processing output 2/2 in batch 92
2025-05-08 18:51:49,385 - distillation - INFO - Processing batch 93/218
2025-05-08 18:52:35,278 - distillation - INFO - Processing output 1/2 in batch 93
2025-05-08 18:52:35,279 - distillation - INFO - Processing output 2/2 in batch 93
2025-05-08 18:52:35,451 - distillation - INFO - Processing batch 94/218
2025-05-08 18:53:08,858 - distillation - INFO - Processing output 1/2 in batch 94
2025-05-08 18:53:08,859 - distillation - INFO - Processing output 2/2 in batch 94
2025-05-08 18:53:09,030 - distillation - INFO - Processing batch 95/218
2025-05-08 18:54:00,064 - distillation - INFO - Processing output 1/2 in batch 95
2025-05-08 18:54:00,065 - distillation - INFO - Processing output 2/2 in batch 95
2025-05-08 18:54:00,341 - distillation - INFO - Processing batch 96/218
2025-05-08 18:55:01,450 - distillation - INFO - Processing output 1/2 in batch 96
2025-05-08 18:55:01,454 - distillation - INFO - Processing output 2/2 in batch 96
2025-05-08 18:55:01,707 - distillation - INFO - Processing batch 97/218
2025-05-08 18:56:06,403 - distillation - INFO - Processing output 1/2 in batch 97
2025-05-08 18:56:06,404 - distillation - INFO - Processing output 2/2 in batch 97
2025-05-08 18:56:06,634 - distillation - INFO - Processing batch 98/218
2025-05-08 18:57:04,442 - distillation - INFO - Processing output 1/2 in batch 98
2025-05-08 18:57:04,443 - distillation - INFO - Processing output 2/2 in batch 98
2025-05-08 18:57:04,738 - distillation - INFO - Processing batch 99/218
2025-05-08 18:57:59,261 - distillation - INFO - Processing output 1/2 in batch 99
2025-05-08 18:57:59,262 - distillation - INFO - Processing output 2/2 in batch 99
2025-05-08 18:57:59,431 - distillation - INFO - Processing batch 100/218
2025-05-08 18:58:43,044 - distillation - INFO - Processing output 1/2 in batch 100
2025-05-08 18:58:43,045 - distillation - INFO - Processing output 2/2 in batch 100
2025-05-08 18:58:43,207 - distillation - INFO - Processing batch 101/218
2025-05-08 18:59:14,099 - distillation - INFO - Processing output 1/2 in batch 101
2025-05-08 18:59:14,101 - distillation - INFO - Processing output 2/2 in batch 101
2025-05-08 18:59:14,276 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_200
2025-05-08 18:59:14,277 - distillation - INFO - Processing batch 102/218
2025-05-08 18:59:58,864 - distillation - INFO - Processing output 1/2 in batch 102
2025-05-08 18:59:58,866 - distillation - INFO - Processing output 2/2 in batch 102
2025-05-08 18:59:59,032 - distillation - INFO - Processing batch 103/218
2025-05-08 19:00:42,770 - distillation - INFO - Processing output 1/2 in batch 103
2025-05-08 19:00:42,773 - distillation - INFO - Processing output 2/2 in batch 103
2025-05-08 19:00:42,936 - distillation - INFO - Processing batch 104/218
2025-05-08 19:01:20,971 - distillation - INFO - Processing output 1/2 in batch 104
2025-05-08 19:01:20,972 - distillation - INFO - Processing output 2/2 in batch 104
2025-05-08 19:01:21,127 - distillation - INFO - Processing batch 105/218
2025-05-08 19:01:56,380 - distillation - INFO - Processing output 1/2 in batch 105
2025-05-08 19:01:56,381 - distillation - INFO - Processing output 2/2 in batch 105
2025-05-08 19:01:56,539 - distillation - INFO - Processing batch 106/218
2025-05-08 19:02:36,029 - distillation - INFO - Processing output 1/2 in batch 106
2025-05-08 19:02:36,029 - distillation - INFO - Processing output 2/2 in batch 106
2025-05-08 19:02:36,202 - distillation - INFO - Processing batch 107/218
2025-05-08 19:03:12,309 - distillation - INFO - Processing output 1/2 in batch 107
2025-05-08 19:03:12,310 - distillation - INFO - Processing output 2/2 in batch 107
2025-05-08 19:03:12,481 - distillation - INFO - Processing batch 108/218
2025-05-08 19:03:42,029 - distillation - INFO - Processing output 1/2 in batch 108
2025-05-08 19:03:42,030 - distillation - INFO - Processing output 2/2 in batch 108
2025-05-08 19:03:42,187 - distillation - INFO - Processing batch 109/218
2025-05-08 19:04:17,961 - distillation - INFO - Processing output 1/2 in batch 109
2025-05-08 19:04:17,962 - distillation - INFO - Processing output 2/2 in batch 109
2025-05-08 19:04:18,127 - distillation - INFO - Processing batch 110/218
2025-05-08 19:05:00,934 - distillation - INFO - Processing output 1/2 in batch 110
2025-05-08 19:05:00,936 - distillation - INFO - Processing output 2/2 in batch 110
2025-05-08 19:05:01,114 - distillation - INFO - Processing batch 111/218
2025-05-08 19:05:46,334 - distillation - INFO - Processing output 1/2 in batch 111
2025-05-08 19:05:46,334 - distillation - INFO - Processing output 2/2 in batch 111
2025-05-08 19:05:46,511 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_220
2025-05-08 19:05:46,512 - distillation - INFO - Processing batch 112/218
2025-05-08 19:06:30,940 - distillation - INFO - Processing output 1/2 in batch 112
2025-05-08 19:06:30,941 - distillation - INFO - Processing output 2/2 in batch 112
2025-05-08 19:06:31,097 - distillation - INFO - Processing batch 113/218
2025-05-08 19:07:02,346 - distillation - INFO - Processing output 1/2 in batch 113
2025-05-08 19:07:02,346 - distillation - INFO - Processing output 2/2 in batch 113
2025-05-08 19:07:02,498 - distillation - INFO - Processing batch 114/218
2025-05-08 19:07:38,965 - distillation - INFO - Processing output 1/2 in batch 114
2025-05-08 19:07:38,967 - distillation - INFO - Processing output 2/2 in batch 114
2025-05-08 19:07:39,129 - distillation - INFO - Processing batch 115/218
2025-05-08 19:08:23,832 - distillation - INFO - Processing output 1/2 in batch 115
2025-05-08 19:08:23,833 - distillation - INFO - Processing output 2/2 in batch 115
2025-05-08 19:08:23,990 - distillation - INFO - Processing batch 116/218
2025-05-08 19:09:08,105 - distillation - INFO - Processing output 1/2 in batch 116
2025-05-08 19:09:08,106 - distillation - INFO - Processing output 2/2 in batch 116
2025-05-08 19:09:08,263 - distillation - INFO - Processing batch 117/218
2025-05-08 19:09:34,245 - distillation - INFO - Processing output 1/2 in batch 117
2025-05-08 19:09:34,246 - distillation - INFO - Processing output 2/2 in batch 117
2025-05-08 19:09:34,413 - distillation - INFO - Processing batch 118/218
2025-05-08 19:10:07,047 - distillation - INFO - Processing output 1/2 in batch 118
2025-05-08 19:10:07,047 - distillation - INFO - Processing output 2/2 in batch 118
2025-05-08 19:10:07,217 - distillation - INFO - Processing batch 119/218
2025-05-08 19:10:52,180 - distillation - INFO - Processing output 1/2 in batch 119
2025-05-08 19:10:52,181 - distillation - INFO - Processing output 2/2 in batch 119
2025-05-08 19:10:52,354 - distillation - INFO - Processing batch 120/218
2025-05-08 19:11:37,023 - distillation - INFO - Processing output 1/2 in batch 120
2025-05-08 19:11:37,023 - distillation - INFO - Processing output 2/2 in batch 120
2025-05-08 19:11:37,177 - distillation - INFO - Processing batch 121/218
2025-05-08 19:12:21,767 - distillation - INFO - Processing output 1/2 in batch 121
2025-05-08 19:12:21,767 - distillation - INFO - Processing output 2/2 in batch 121
2025-05-08 19:12:21,932 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_240
2025-05-08 19:12:21,932 - distillation - INFO - Processing batch 122/218
2025-05-08 19:13:07,101 - distillation - INFO - Processing output 1/2 in batch 122
2025-05-08 19:13:07,101 - distillation - INFO - Processing output 2/2 in batch 122
2025-05-08 19:13:07,265 - distillation - INFO - Processing batch 123/218
2025-05-08 19:13:52,547 - distillation - INFO - Processing output 1/2 in batch 123
2025-05-08 19:13:52,547 - distillation - INFO - Processing output 2/2 in batch 123
2025-05-08 19:13:52,707 - distillation - INFO - Processing batch 124/218
2025-05-08 19:14:37,289 - distillation - INFO - Processing output 1/2 in batch 124
2025-05-08 19:14:37,290 - distillation - INFO - Processing output 2/2 in batch 124
2025-05-08 19:14:37,467 - distillation - INFO - Processing batch 125/218
2025-05-08 19:15:05,291 - distillation - INFO - Processing output 1/2 in batch 125
2025-05-08 19:15:05,292 - distillation - INFO - Processing output 2/2 in batch 125
2025-05-08 19:15:05,459 - distillation - INFO - Processing batch 126/218
2025-05-08 19:15:39,045 - distillation - INFO - Processing output 1/2 in batch 126
2025-05-08 19:15:39,046 - distillation - INFO - Processing output 2/2 in batch 126
2025-05-08 19:15:39,219 - distillation - INFO - Processing batch 127/218
2025-05-08 19:16:25,135 - distillation - INFO - Processing output 1/2 in batch 127
2025-05-08 19:16:25,136 - distillation - INFO - Processing output 2/2 in batch 127
2025-05-08 19:16:25,311 - distillation - INFO - Processing batch 128/218
2025-05-08 19:16:59,810 - distillation - INFO - Processing output 1/2 in batch 128
2025-05-08 19:16:59,811 - distillation - INFO - Processing output 2/2 in batch 128
2025-05-08 19:16:59,976 - distillation - INFO - Processing batch 129/218
2025-05-08 19:17:45,872 - distillation - INFO - Processing output 1/2 in batch 129
2025-05-08 19:17:45,873 - distillation - INFO - Processing output 2/2 in batch 129
2025-05-08 19:17:46,031 - distillation - INFO - Processing batch 130/218
2025-05-08 19:18:28,567 - distillation - INFO - Processing output 1/2 in batch 130
2025-05-08 19:18:28,570 - distillation - INFO - Processing output 2/2 in batch 130
2025-05-08 19:18:28,753 - distillation - INFO - Processing batch 131/218
2025-05-08 19:19:12,726 - distillation - INFO - Processing output 1/2 in batch 131
2025-05-08 19:19:12,727 - distillation - INFO - Processing output 2/2 in batch 131
2025-05-08 19:19:12,893 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_260
2025-05-08 19:19:12,893 - distillation - INFO - Processing batch 132/218
2025-05-08 19:19:57,431 - distillation - INFO - Processing output 1/2 in batch 132
2025-05-08 19:19:57,431 - distillation - INFO - Processing output 2/2 in batch 132
2025-05-08 19:19:57,612 - distillation - INFO - Processing batch 133/218
2025-05-08 19:20:33,305 - distillation - INFO - Processing output 1/2 in batch 133
2025-05-08 19:20:33,305 - distillation - INFO - Processing output 2/2 in batch 133
2025-05-08 19:20:33,472 - distillation - INFO - Processing batch 134/218
2025-05-08 19:21:17,648 - distillation - INFO - Processing output 1/2 in batch 134
2025-05-08 19:21:17,651 - distillation - INFO - Processing output 2/2 in batch 134
2025-05-08 19:21:17,831 - distillation - INFO - Processing batch 135/218
2025-05-08 19:22:03,292 - distillation - INFO - Processing output 1/2 in batch 135
2025-05-08 19:22:03,295 - distillation - INFO - Processing output 2/2 in batch 135
2025-05-08 19:22:03,477 - distillation - INFO - Processing batch 136/218
2025-05-08 19:22:49,249 - distillation - INFO - Processing output 1/2 in batch 136
2025-05-08 19:22:49,249 - distillation - INFO - Processing output 2/2 in batch 136
2025-05-08 19:22:49,407 - distillation - INFO - Processing batch 137/218
2025-05-08 19:23:35,088 - distillation - INFO - Processing output 1/2 in batch 137
2025-05-08 19:23:35,089 - distillation - INFO - Processing output 2/2 in batch 137
2025-05-08 19:23:35,265 - distillation - INFO - Processing batch 138/218
2025-05-08 19:24:18,293 - distillation - INFO - Processing output 1/2 in batch 138
2025-05-08 19:24:18,294 - distillation - INFO - Processing output 2/2 in batch 138
2025-05-08 19:24:18,451 - distillation - INFO - Processing batch 139/218
2025-05-08 19:24:57,376 - distillation - INFO - Processing output 1/2 in batch 139
2025-05-08 19:24:57,378 - distillation - INFO - Processing output 2/2 in batch 139
2025-05-08 19:24:57,534 - distillation - INFO - Processing batch 140/218
2025-05-08 19:25:34,198 - distillation - INFO - Processing output 1/2 in batch 140
2025-05-08 19:25:34,199 - distillation - INFO - Processing output 2/2 in batch 140
2025-05-08 19:25:34,368 - distillation - INFO - Processing batch 141/218
2025-05-08 19:26:11,596 - distillation - INFO - Processing output 1/2 in batch 141
2025-05-08 19:26:11,597 - distillation - INFO - Processing output 2/2 in batch 141
2025-05-08 19:26:11,763 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_280
2025-05-08 19:26:11,764 - distillation - INFO - Processing batch 142/218
2025-05-08 19:26:53,669 - distillation - INFO - Processing output 1/2 in batch 142
2025-05-08 19:26:53,672 - distillation - INFO - Processing output 2/2 in batch 142
2025-05-08 19:26:53,850 - distillation - INFO - Processing batch 143/218
2025-05-08 19:27:38,595 - distillation - INFO - Processing output 1/2 in batch 143
2025-05-08 19:27:38,598 - distillation - INFO - Processing output 2/2 in batch 143
2025-05-08 19:27:38,760 - distillation - INFO - Processing batch 144/218
2025-05-08 19:28:23,278 - distillation - INFO - Processing output 1/2 in batch 144
2025-05-08 19:28:23,280 - distillation - INFO - Processing output 2/2 in batch 144
2025-05-08 19:28:23,467 - distillation - INFO - Processing batch 145/218
2025-05-08 19:29:09,875 - distillation - INFO - Processing output 1/2 in batch 145
2025-05-08 19:29:09,876 - distillation - INFO - Processing output 2/2 in batch 145
2025-05-08 19:29:10,045 - distillation - INFO - Processing batch 146/218
2025-05-08 19:29:43,061 - distillation - INFO - Processing output 1/2 in batch 146
2025-05-08 19:29:43,063 - distillation - INFO - Processing output 2/2 in batch 146
2025-05-08 19:29:43,220 - distillation - INFO - Processing batch 147/218
2025-05-08 19:30:28,401 - distillation - INFO - Processing output 1/2 in batch 147
2025-05-08 19:30:28,402 - distillation - INFO - Processing output 2/2 in batch 147
2025-05-08 19:30:28,559 - distillation - INFO - Processing batch 148/218
2025-05-08 19:31:13,511 - distillation - INFO - Processing output 1/2 in batch 148
2025-05-08 19:31:13,512 - distillation - INFO - Processing output 2/2 in batch 148
2025-05-08 19:31:13,685 - distillation - INFO - Processing batch 149/218
2025-05-08 19:31:39,955 - distillation - INFO - Processing output 1/2 in batch 149
2025-05-08 19:31:39,956 - distillation - INFO - Processing output 2/2 in batch 149
2025-05-08 19:31:40,127 - distillation - INFO - Processing batch 150/218
2025-05-08 19:32:09,092 - distillation - INFO - Processing output 1/2 in batch 150
2025-05-08 19:32:09,094 - distillation - INFO - Processing output 2/2 in batch 150
2025-05-08 19:32:09,255 - distillation - INFO - Processing batch 151/218
2025-05-08 19:32:55,631 - distillation - INFO - Processing output 1/2 in batch 151
2025-05-08 19:32:55,632 - distillation - INFO - Processing output 2/2 in batch 151
2025-05-08 19:32:55,800 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_300
2025-05-08 19:32:55,800 - distillation - INFO - Processing batch 152/218
2025-05-08 19:33:32,630 - distillation - INFO - Processing output 1/2 in batch 152
2025-05-08 19:33:32,631 - distillation - INFO - Processing output 2/2 in batch 152
2025-05-08 19:33:32,786 - distillation - INFO - Processing batch 153/218
2025-05-08 19:33:59,412 - distillation - INFO - Processing output 1/2 in batch 153
2025-05-08 19:33:59,413 - distillation - INFO - Processing output 2/2 in batch 153
2025-05-08 19:33:59,570 - distillation - INFO - Processing batch 154/218
2025-05-08 19:34:28,105 - distillation - INFO - Processing output 1/2 in batch 154
2025-05-08 19:34:28,107 - distillation - INFO - Processing output 2/2 in batch 154
2025-05-08 19:34:28,260 - distillation - INFO - Processing batch 155/218
2025-05-08 19:35:02,373 - distillation - INFO - Processing output 1/2 in batch 155
2025-05-08 19:35:02,375 - distillation - INFO - Processing output 2/2 in batch 155
2025-05-08 19:35:02,543 - distillation - INFO - Processing batch 156/218
2025-05-08 19:35:43,270 - distillation - INFO - Processing output 1/2 in batch 156
2025-05-08 19:35:43,270 - distillation - INFO - Processing output 2/2 in batch 156
2025-05-08 19:35:43,430 - distillation - INFO - Processing batch 157/218
2025-05-08 19:36:05,921 - distillation - INFO - Processing output 1/2 in batch 157
2025-05-08 19:36:05,921 - distillation - INFO - Processing output 2/2 in batch 157
2025-05-08 19:36:06,071 - distillation - INFO - Processing batch 158/218
2025-05-08 19:36:42,167 - distillation - INFO - Processing output 1/2 in batch 158
2025-05-08 19:36:42,168 - distillation - INFO - Processing output 2/2 in batch 158
2025-05-08 19:36:42,327 - distillation - INFO - Processing batch 159/218
2025-05-08 19:37:22,769 - distillation - INFO - Processing output 1/2 in batch 159
2025-05-08 19:37:22,769 - distillation - INFO - Processing output 2/2 in batch 159
2025-05-08 19:37:22,939 - distillation - INFO - Processing batch 160/218
2025-05-08 19:38:07,938 - distillation - INFO - Processing output 1/2 in batch 160
2025-05-08 19:38:07,940 - distillation - INFO - Processing output 2/2 in batch 160
2025-05-08 19:38:08,102 - distillation - INFO - Processing batch 161/218
2025-05-08 19:38:52,910 - distillation - INFO - Processing output 1/2 in batch 161
2025-05-08 19:38:52,910 - distillation - INFO - Processing output 2/2 in batch 161
2025-05-08 19:38:53,096 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_320
2025-05-08 19:38:53,097 - distillation - INFO - Processing batch 162/218
2025-05-08 19:39:31,771 - distillation - INFO - Processing output 1/2 in batch 162
2025-05-08 19:39:31,771 - distillation - INFO - Processing output 2/2 in batch 162
2025-05-08 19:39:31,933 - distillation - INFO - Processing batch 163/218
2025-05-08 19:40:16,938 - distillation - INFO - Processing output 1/2 in batch 163
2025-05-08 19:40:16,939 - distillation - INFO - Processing output 2/2 in batch 163
2025-05-08 19:40:17,105 - distillation - INFO - Processing batch 164/218
2025-05-08 19:40:52,993 - distillation - INFO - Processing output 1/2 in batch 164
2025-05-08 19:40:52,993 - distillation - INFO - Processing output 2/2 in batch 164
2025-05-08 19:40:53,173 - distillation - INFO - Processing batch 165/218
2025-05-08 19:41:30,928 - distillation - INFO - Processing output 1/2 in batch 165
2025-05-08 19:41:30,929 - distillation - INFO - Processing output 2/2 in batch 165
2025-05-08 19:41:31,102 - distillation - INFO - Processing batch 166/218
2025-05-08 19:42:01,674 - distillation - INFO - Processing output 1/2 in batch 166
2025-05-08 19:42:01,674 - distillation - INFO - Processing output 2/2 in batch 166
2025-05-08 19:42:01,839 - distillation - INFO - Processing batch 167/218
2025-05-08 19:42:38,208 - distillation - INFO - Processing output 1/2 in batch 167
2025-05-08 19:42:38,208 - distillation - INFO - Processing output 2/2 in batch 167
2025-05-08 19:42:38,365 - distillation - INFO - Processing batch 168/218
2025-05-08 19:43:24,122 - distillation - INFO - Processing output 1/2 in batch 168
2025-05-08 19:43:24,124 - distillation - INFO - Processing output 2/2 in batch 168
2025-05-08 19:43:24,289 - distillation - INFO - Processing batch 169/218
2025-05-08 19:44:07,510 - distillation - INFO - Processing output 1/2 in batch 169
2025-05-08 19:44:07,511 - distillation - INFO - Processing output 2/2 in batch 169
2025-05-08 19:44:07,673 - distillation - INFO - Processing batch 170/218
2025-05-08 19:44:50,275 - distillation - INFO - Processing output 1/2 in batch 170
2025-05-08 19:44:50,275 - distillation - INFO - Processing output 2/2 in batch 170
2025-05-08 19:44:50,437 - distillation - INFO - Processing batch 171/218
2025-05-08 19:45:36,162 - distillation - INFO - Processing output 1/2 in batch 171
2025-05-08 19:45:36,163 - distillation - INFO - Processing output 2/2 in batch 171
2025-05-08 19:45:36,341 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_340
2025-05-08 19:45:36,342 - distillation - INFO - Processing batch 172/218
2025-05-08 19:46:17,599 - distillation - INFO - Processing output 1/2 in batch 172
2025-05-08 19:46:17,600 - distillation - INFO - Processing output 2/2 in batch 172
2025-05-08 19:46:17,766 - distillation - INFO - Processing batch 173/218
2025-05-08 19:47:03,821 - distillation - INFO - Processing output 1/2 in batch 173
2025-05-08 19:47:03,822 - distillation - INFO - Processing output 2/2 in batch 173
2025-05-08 19:47:04,005 - distillation - INFO - Processing batch 174/218
2025-05-08 19:47:44,663 - distillation - INFO - Processing output 1/2 in batch 174
2025-05-08 19:47:44,664 - distillation - INFO - Processing output 2/2 in batch 174
2025-05-08 19:47:44,836 - distillation - INFO - Processing batch 175/218
2025-05-08 19:48:16,207 - distillation - INFO - Processing output 1/2 in batch 175
2025-05-08 19:48:16,208 - distillation - INFO - Processing output 2/2 in batch 175
2025-05-08 19:48:16,369 - distillation - INFO - Processing batch 176/218
2025-05-08 19:49:01,612 - distillation - INFO - Processing output 1/2 in batch 176
2025-05-08 19:49:01,614 - distillation - INFO - Processing output 2/2 in batch 176
2025-05-08 19:49:01,777 - distillation - INFO - Processing batch 177/218
2025-05-08 19:49:47,666 - distillation - INFO - Processing output 1/2 in batch 177
2025-05-08 19:49:47,667 - distillation - INFO - Processing output 2/2 in batch 177
2025-05-08 19:49:47,826 - distillation - INFO - Processing batch 178/218
2025-05-08 19:50:14,251 - distillation - INFO - Processing output 1/2 in batch 178
2025-05-08 19:50:14,253 - distillation - INFO - Processing output 2/2 in batch 178
2025-05-08 19:50:14,429 - distillation - INFO - Processing batch 179/218
2025-05-08 19:50:53,204 - distillation - INFO - Processing output 1/2 in batch 179
2025-05-08 19:50:53,204 - distillation - INFO - Processing output 2/2 in batch 179
2025-05-08 19:50:53,365 - distillation - INFO - Processing batch 180/218
2025-05-08 19:51:24,664 - distillation - INFO - Processing output 1/2 in batch 180
2025-05-08 19:51:24,665 - distillation - INFO - Processing output 2/2 in batch 180
2025-05-08 19:51:24,832 - distillation - INFO - Processing batch 181/218
2025-05-08 19:52:05,093 - distillation - INFO - Processing output 1/2 in batch 181
2025-05-08 19:52:05,095 - distillation - INFO - Processing output 2/2 in batch 181
2025-05-08 19:52:05,260 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_360
2025-05-08 19:52:05,260 - distillation - INFO - Processing batch 182/218
2025-05-08 19:52:43,210 - distillation - INFO - Processing output 1/2 in batch 182
2025-05-08 19:52:43,211 - distillation - INFO - Processing output 2/2 in batch 182
2025-05-08 19:52:43,374 - distillation - INFO - Processing batch 183/218
2025-05-08 19:53:29,635 - distillation - INFO - Processing output 1/2 in batch 183
2025-05-08 19:53:29,636 - distillation - INFO - Processing output 2/2 in batch 183
2025-05-08 19:53:29,808 - distillation - INFO - Processing batch 184/218
2025-05-08 19:54:15,608 - distillation - INFO - Processing output 1/2 in batch 184
2025-05-08 19:54:15,609 - distillation - INFO - Processing output 2/2 in batch 184
2025-05-08 19:54:15,773 - distillation - INFO - Processing batch 185/218
2025-05-08 19:54:53,375 - distillation - INFO - Processing output 1/2 in batch 185
2025-05-08 19:54:53,375 - distillation - INFO - Processing output 2/2 in batch 185
2025-05-08 19:54:53,553 - distillation - INFO - Processing batch 186/218
2025-05-08 19:55:26,815 - distillation - INFO - Processing output 1/2 in batch 186
2025-05-08 19:55:26,816 - distillation - INFO - Processing output 2/2 in batch 186
2025-05-08 19:55:26,977 - distillation - INFO - Processing batch 187/218
2025-05-08 19:55:59,298 - distillation - INFO - Processing output 1/2 in batch 187
2025-05-08 19:55:59,300 - distillation - INFO - Processing output 2/2 in batch 187
2025-05-08 19:55:59,467 - distillation - INFO - Processing batch 188/218
2025-05-08 19:56:36,980 - distillation - INFO - Processing output 1/2 in batch 188
2025-05-08 19:56:36,982 - distillation - INFO - Processing output 2/2 in batch 188
2025-05-08 19:56:37,145 - distillation - INFO - Processing batch 189/218
2025-05-08 19:57:14,045 - distillation - INFO - Processing output 1/2 in batch 189
2025-05-08 19:57:14,046 - distillation - INFO - Processing output 2/2 in batch 189
2025-05-08 19:57:14,211 - distillation - INFO - Processing batch 190/218
2025-05-08 19:57:59,305 - distillation - INFO - Processing output 1/2 in batch 190
2025-05-08 19:57:59,306 - distillation - INFO - Processing output 2/2 in batch 190
2025-05-08 19:57:59,466 - distillation - INFO - Processing batch 191/218
2025-05-08 19:58:44,706 - distillation - INFO - Processing output 1/2 in batch 191
2025-05-08 19:58:44,708 - distillation - INFO - Processing output 2/2 in batch 191
2025-05-08 19:58:44,880 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_380
2025-05-08 19:58:44,881 - distillation - INFO - Processing batch 192/218
2025-05-08 19:59:29,622 - distillation - INFO - Processing output 1/2 in batch 192
2025-05-08 19:59:29,624 - distillation - INFO - Processing output 2/2 in batch 192
2025-05-08 19:59:29,784 - distillation - INFO - Processing batch 193/218
2025-05-08 20:00:06,324 - distillation - INFO - Processing output 1/2 in batch 193
2025-05-08 20:00:06,325 - distillation - INFO - Processing output 2/2 in batch 193
2025-05-08 20:00:06,484 - distillation - INFO - Processing batch 194/218
2025-05-08 20:00:36,880 - distillation - INFO - Processing output 1/2 in batch 194
2025-05-08 20:00:36,881 - distillation - INFO - Processing output 2/2 in batch 194
2025-05-08 20:00:37,046 - distillation - INFO - Processing batch 195/218
2025-05-08 20:01:22,915 - distillation - INFO - Processing output 1/2 in batch 195
2025-05-08 20:01:22,915 - distillation - INFO - Processing output 2/2 in batch 195
2025-05-08 20:01:23,092 - distillation - INFO - Processing batch 196/218
2025-05-08 20:01:57,889 - distillation - INFO - Processing output 1/2 in batch 196
2025-05-08 20:01:57,890 - distillation - INFO - Processing output 2/2 in batch 196
2025-05-08 20:01:58,061 - distillation - INFO - Processing batch 197/218
2025-05-08 20:02:30,025 - distillation - INFO - Processing output 1/2 in batch 197
2025-05-08 20:02:30,027 - distillation - INFO - Processing output 2/2 in batch 197
2025-05-08 20:02:30,199 - distillation - INFO - Processing batch 198/218
2025-05-08 20:03:03,874 - distillation - INFO - Processing output 1/2 in batch 198
2025-05-08 20:03:03,875 - distillation - INFO - Processing output 2/2 in batch 198
2025-05-08 20:03:04,042 - distillation - INFO - Processing batch 199/218
2025-05-08 20:03:41,971 - distillation - INFO - Processing output 1/2 in batch 199
2025-05-08 20:03:41,971 - distillation - INFO - Processing output 2/2 in batch 199
2025-05-08 20:03:42,142 - distillation - INFO - Processing batch 200/218
2025-05-08 20:04:27,699 - distillation - INFO - Processing output 1/2 in batch 200
2025-05-08 20:04:27,701 - distillation - INFO - Processing output 2/2 in batch 200
2025-05-08 20:04:27,882 - distillation - INFO - Processing batch 201/218
2025-05-08 20:05:08,277 - distillation - INFO - Processing output 1/2 in batch 201
2025-05-08 20:05:08,278 - distillation - INFO - Processing output 2/2 in batch 201
2025-05-08 20:05:08,442 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_400
2025-05-08 20:05:08,443 - distillation - INFO - Processing batch 202/218
2025-05-08 20:05:53,557 - distillation - INFO - Processing output 1/2 in batch 202
2025-05-08 20:05:53,558 - distillation - INFO - Processing output 2/2 in batch 202
2025-05-08 20:05:53,730 - distillation - INFO - Processing batch 203/218
2025-05-08 20:06:38,440 - distillation - INFO - Processing output 1/2 in batch 203
2025-05-08 20:06:38,442 - distillation - INFO - Processing output 2/2 in batch 203
2025-05-08 20:06:38,610 - distillation - INFO - Processing batch 204/218
2025-05-08 20:07:25,116 - distillation - INFO - Processing output 1/2 in batch 204
2025-05-08 20:07:25,119 - distillation - INFO - Processing output 2/2 in batch 204
2025-05-08 20:07:25,279 - distillation - INFO - Processing batch 205/218
2025-05-08 20:08:10,948 - distillation - INFO - Processing output 1/2 in batch 205
2025-05-08 20:08:10,949 - distillation - INFO - Processing output 2/2 in batch 205
2025-05-08 20:08:11,111 - distillation - INFO - Processing batch 206/218
2025-05-08 20:08:50,048 - distillation - INFO - Processing output 1/2 in batch 206
2025-05-08 20:08:50,050 - distillation - INFO - Processing output 2/2 in batch 206
2025-05-08 20:08:50,211 - distillation - INFO - Processing batch 207/218
2025-05-08 20:09:32,250 - distillation - INFO - Processing output 1/2 in batch 207
2025-05-08 20:09:32,250 - distillation - INFO - Processing output 2/2 in batch 207
2025-05-08 20:09:32,417 - distillation - INFO - Processing batch 208/218
2025-05-08 20:10:16,819 - distillation - INFO - Processing output 1/2 in batch 208
2025-05-08 20:10:16,822 - distillation - INFO - Processing output 2/2 in batch 208
2025-05-08 20:10:16,989 - distillation - INFO - Processing batch 209/218
2025-05-08 20:10:53,339 - distillation - INFO - Processing output 1/2 in batch 209
2025-05-08 20:10:53,341 - distillation - INFO - Processing output 2/2 in batch 209
2025-05-08 20:10:53,508 - distillation - INFO - Processing batch 210/218
2025-05-08 20:11:33,781 - distillation - INFO - Processing output 1/2 in batch 210
2025-05-08 20:11:33,782 - distillation - INFO - Processing output 2/2 in batch 210
2025-05-08 20:11:33,946 - distillation - INFO - Processing batch 211/218
2025-05-08 20:12:19,062 - distillation - INFO - Processing output 1/2 in batch 211
2025-05-08 20:12:19,063 - distillation - INFO - Processing output 2/2 in batch 211
2025-05-08 20:12:19,242 - distillation - INFO - Saved progress to temporary file: models\train_data.json.temp_420
2025-05-08 20:12:19,242 - distillation - INFO - Processing batch 212/218
2025-05-08 20:13:01,710 - distillation - INFO - Processing output 1/2 in batch 212
2025-05-08 20:13:01,710 - distillation - INFO - Processing output 2/2 in batch 212
2025-05-08 20:13:01,862 - distillation - INFO - Processing batch 213/218
2025-05-08 20:13:37,652 - distillation - INFO - Processing output 1/2 in batch 213
2025-05-08 20:13:37,652 - distillation - INFO - Processing output 2/2 in batch 213
2025-05-08 20:13:37,809 - distillation - INFO - Processing batch 214/218
2025-05-08 20:14:15,221 - distillation - INFO - Processing output 1/2 in batch 214
2025-05-08 20:14:15,221 - distillation - INFO - Processing output 2/2 in batch 214
2025-05-08 20:14:15,381 - distillation - INFO - Processing batch 215/218
2025-05-08 20:14:53,454 - distillation - INFO - Processing output 1/2 in batch 215
2025-05-08 20:14:53,455 - distillation - INFO - Processing output 2/2 in batch 215
2025-05-08 20:14:53,610 - distillation - INFO - Processing batch 216/218
2025-05-08 20:15:36,680 - distillation - INFO - Processing output 1/2 in batch 216
2025-05-08 20:15:36,681 - distillation - INFO - Processing output 2/2 in batch 216
2025-05-08 20:15:36,837 - distillation - INFO - Processing batch 217/218
2025-05-08 20:16:19,017 - distillation - INFO - Processing output 1/2 in batch 217
2025-05-08 20:16:19,018 - distillation - INFO - Processing output 2/2 in batch 217
2025-05-08 20:16:19,174 - distillation - INFO - Processing batch 218/218
2025-05-08 20:17:04,655 - distillation - INFO - Processing output 1/2 in batch 218
2025-05-08 20:17:04,657 - distillation - INFO - Processing output 2/2 in batch 218
2025-05-08 20:17:04,837 - distillation - INFO - Distillation data saved to models\train_data.json with 436 examples
2025-05-08 20:17:04,838 - distillation - INFO - Total processing time: 2:24:47
2025-05-08 20:17:04,839 - distillation - INFO - Validation data saved to models\train_data_val.json with 43 examples
2025-05-08 20:17:04,840 - __main__ - INFO - Starting distillation
2025-05-08 20:17:04,840 - distillation - INFO - Starting distillation process
2025-05-08 20:17:04,841 - distillation - WARNING - 訓練データまたは検証データが見つかりません: train=True, val=False
2025-05-08 20:17:04,841 - distillation - INFO - 蒸留データを自動生成します
2025-05-08 20:17:04,841 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 20:17:04,842 - distillation - INFO - 読み込んだ質問数: 508
2025-05-08 20:17:04,842 - distillation - INFO - フィルタリングに使用するキーワード数: 76
2025-05-08 20:17:04,844 - distillation - INFO - Filtered questions: 436 out of 508 original questions
2025-05-08 20:17:04,844 - distillation - INFO - フィルタリング後の質問数: 436
2025-05-08 20:17:04,844 - distillation - INFO - Processing batch 1/218
2025-05-08 20:17:50,513 - distillation - INFO - Processing output 1/2 in batch 1
2025-05-08 20:17:50,513 - distillation - INFO - Processing output 2/2 in batch 1
2025-05-08 20:17:50,677 - distillation - INFO - Processing batch 2/218
2025-05-08 20:18:24,190 - distillation - INFO - Processing output 1/2 in batch 2
2025-05-08 20:18:24,191 - distillation - INFO - Processing output 2/2 in batch 2
2025-05-08 20:18:24,353 - distillation - INFO - Processing batch 3/218
2025-05-08 20:18:55,648 - distillation - INFO - Processing output 1/2 in batch 3
2025-05-08 20:18:55,648 - distillation - INFO - Processing output 2/2 in batch 3
2025-05-08 20:18:55,808 - distillation - INFO - Processing batch 4/218
2025-05-08 20:19:32,875 - distillation - INFO - Processing output 1/2 in batch 4
2025-05-08 20:19:32,875 - distillation - INFO - Processing output 2/2 in batch 4
2025-05-08 20:19:33,035 - distillation - INFO - Processing batch 5/218
2025-05-08 20:20:19,066 - distillation - INFO - Processing output 1/2 in batch 5
2025-05-08 20:20:19,067 - distillation - INFO - Processing output 2/2 in batch 5
2025-05-08 20:20:19,228 - distillation - INFO - Processing batch 6/218
2025-05-08 20:21:00,751 - distillation - INFO - Processing output 1/2 in batch 6
2025-05-08 20:21:00,752 - distillation - INFO - Processing output 2/2 in batch 6
2025-05-08 20:21:00,907 - distillation - INFO - Processing batch 7/218
2025-05-08 20:21:42,698 - distillation - INFO - Processing output 1/2 in batch 7
2025-05-08 20:21:42,699 - distillation - INFO - Processing output 2/2 in batch 7
2025-05-08 20:21:42,860 - distillation - INFO - Processing batch 8/218
2025-05-08 20:22:19,552 - distillation - INFO - Processing output 1/2 in batch 8
2025-05-08 20:22:19,553 - distillation - INFO - Processing output 2/2 in batch 8
2025-05-08 20:22:19,714 - distillation - INFO - Processing batch 9/218
2025-05-08 20:22:53,795 - distillation - INFO - Processing output 1/2 in batch 9
2025-05-08 20:22:53,796 - distillation - INFO - Processing output 2/2 in batch 9
2025-05-08 20:22:53,976 - distillation - INFO - Processing batch 10/218
2025-05-08 20:23:36,567 - distillation - INFO - Processing output 1/2 in batch 10
2025-05-08 20:23:36,569 - distillation - INFO - Processing output 2/2 in batch 10
2025-05-08 20:23:36,739 - distillation - INFO - Processing batch 11/218
2025-05-08 20:24:17,316 - distillation - INFO - Processing output 1/2 in batch 11
2025-05-08 20:24:17,316 - distillation - INFO - Processing output 2/2 in batch 11
2025-05-08 20:24:17,479 - distillation - INFO - Saved progress to temporary file: temp_distillation_data.json.temp_20
2025-05-08 20:24:17,480 - distillation - INFO - Processing batch 12/218
2025-05-08 20:24:50,450 - distillation - INFO - Processing output 1/2 in batch 12
2025-05-08 20:24:50,451 - distillation - INFO - Processing output 2/2 in batch 12
2025-05-08 20:24:50,615 - distillation - INFO - Processing batch 13/218
2025-05-08 20:25:25,705 - distillation - INFO - Processing output 1/2 in batch 13
2025-05-08 20:25:25,706 - distillation - INFO - Processing output 2/2 in batch 13
2025-05-08 20:25:25,865 - distillation - INFO - Processing batch 14/218
2025-05-08 20:25:52,430 - distillation - INFO - Processing output 1/2 in batch 14
2025-05-08 20:25:52,430 - distillation - INFO - Processing output 2/2 in batch 14
2025-05-08 20:25:52,589 - distillation - INFO - Processing batch 15/218
2025-05-08 20:26:33,654 - distillation - INFO - Processing output 1/2 in batch 15
2025-05-08 20:26:33,655 - distillation - INFO - Processing output 2/2 in batch 15
2025-05-08 20:26:33,834 - distillation - INFO - Processing batch 16/218
2025-05-08 20:27:10,661 - distillation - INFO - Processing output 1/2 in batch 16
2025-05-08 20:27:10,662 - distillation - INFO - Processing output 2/2 in batch 16
2025-05-08 20:27:10,823 - distillation - INFO - Processing batch 17/218
2025-05-08 20:27:55,264 - distillation - INFO - Processing output 1/2 in batch 17
2025-05-08 20:27:55,265 - distillation - INFO - Processing output 2/2 in batch 17
2025-05-08 20:27:55,431 - distillation - INFO - Processing batch 18/218
2025-05-08 20:28:33,048 - distillation - INFO - Processing output 1/2 in batch 18
2025-05-08 20:28:33,050 - distillation - INFO - Processing output 2/2 in batch 18
2025-05-08 20:28:33,224 - distillation - INFO - Processing batch 19/218
2025-05-08 20:29:13,991 - distillation - INFO - Processing output 1/2 in batch 19
2025-05-08 20:29:13,992 - distillation - INFO - Processing output 2/2 in batch 19
2025-05-08 20:29:14,158 - distillation - INFO - Processing batch 20/218
2025-05-08 20:29:59,468 - distillation - INFO - Processing output 1/2 in batch 20
2025-05-08 20:29:59,469 - distillation - INFO - Processing output 2/2 in batch 20
2025-05-08 20:29:59,620 - distillation - INFO - Processing batch 21/218
2025-05-08 20:30:26,524 - distillation - INFO - Processing output 1/2 in batch 21
2025-05-08 20:30:26,524 - distillation - INFO - Processing output 2/2 in batch 21
2025-05-08 20:30:26,689 - distillation - INFO - Saved progress to temporary file: temp_distillation_data.json.temp_40
2025-05-08 20:30:26,689 - distillation - INFO - Processing batch 22/218
2025-05-08 20:31:09,052 - distillation - INFO - Processing output 1/2 in batch 22
2025-05-08 20:31:09,053 - distillation - INFO - Processing output 2/2 in batch 22
2025-05-08 20:31:09,222 - distillation - INFO - Processing batch 23/218
2025-05-08 20:31:54,727 - distillation - INFO - Processing output 1/2 in batch 23
2025-05-08 20:31:54,727 - distillation - INFO - Processing output 2/2 in batch 23
2025-05-08 20:31:54,898 - distillation - INFO - Processing batch 24/218
2025-05-08 20:32:22,633 - distillation - INFO - Processing output 1/2 in batch 24
2025-05-08 20:32:22,635 - distillation - INFO - Processing output 2/2 in batch 24
2025-05-08 20:32:22,809 - distillation - INFO - Processing batch 25/218
2025-05-08 20:32:51,267 - distillation - INFO - Processing output 1/2 in batch 25
2025-05-08 20:32:51,268 - distillation - INFO - Processing output 2/2 in batch 25
2025-05-08 20:32:51,433 - distillation - INFO - Processing batch 26/218
2025-05-08 20:33:26,750 - distillation - INFO - Processing output 1/2 in batch 26
2025-05-08 20:33:26,753 - distillation - INFO - Processing output 2/2 in batch 26
2025-05-08 20:33:26,921 - distillation - INFO - Processing batch 27/218
2025-05-08 20:33:58,902 - distillation - INFO - Processing output 1/2 in batch 27
2025-05-08 20:33:58,902 - distillation - INFO - Processing output 2/2 in batch 27
2025-05-08 20:33:59,081 - distillation - INFO - Processing batch 28/218
2025-05-08 20:34:40,119 - distillation - INFO - Processing output 1/2 in batch 28
2025-05-08 20:34:40,120 - distillation - INFO - Processing output 2/2 in batch 28
2025-05-08 20:34:40,278 - distillation - INFO - Processing batch 29/218
2025-05-08 20:35:25,255 - distillation - INFO - Processing output 1/2 in batch 29
2025-05-08 20:35:25,256 - distillation - INFO - Processing output 2/2 in batch 29
2025-05-08 20:35:25,415 - distillation - INFO - Processing batch 30/218
2025-05-08 20:36:00,252 - distillation - INFO - Processing output 1/2 in batch 30
2025-05-08 20:36:00,253 - distillation - INFO - Processing output 2/2 in batch 30
2025-05-08 20:36:00,412 - distillation - INFO - Processing batch 31/218
2025-05-08 20:36:31,335 - distillation - INFO - Processing output 1/2 in batch 31
2025-05-08 20:36:31,337 - distillation - INFO - Processing output 2/2 in batch 31
2025-05-08 20:36:31,503 - distillation - INFO - Saved progress to temporary file: temp_distillation_data.json.temp_60
2025-05-08 20:36:31,503 - distillation - INFO - Processing batch 32/218
2025-05-08 20:37:04,844 - distillation - INFO - Processing output 1/2 in batch 32
2025-05-08 20:37:04,845 - distillation - INFO - Processing output 2/2 in batch 32
2025-05-08 20:37:05,005 - distillation - INFO - Processing batch 33/218
2025-05-08 20:37:09,624 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 20:38:28,217 - __main__ - INFO - RAM: 使用中 27.0GB / 95.4GB (28.3%)
2025-05-08 20:38:28,239 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 20:38:28,239 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 20:38:28,239 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 20:38:28,239 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 20:41:25,674 - __main__ - INFO - 重点分野: highschool, electronics, it
2025-05-08 20:41:25,674 - __main__ - INFO - Using device: cuda
2025-05-08 20:41:25,674 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 20:41:25,674 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 20:41:25,675 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 20:41:25,675 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 20:41:26,527 - __main__ - INFO - パッドトークンがないため、EOSトークンをパッドトークンとして設定しました
2025-05-08 20:41:26,536 - __main__ - INFO - 語彙サイズ: 128256
2025-05-08 20:41:26,536 - __main__ - INFO - Initializing student model
2025-05-08 20:41:27,774 - __main__ - INFO - Initializing 8-bit quantization
2025-05-08 20:41:27,775 - __main__ - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 20:41:33,947 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-05-08 20:41:43,717 - __main__ - INFO - 教師モデルのロードが完了しました
2025-05-08 20:41:43,718 - distillation - INFO - トークナイザーのパディング方向を左側に設定します
2025-05-08 20:41:43,728 - distillation - INFO - RAM使用状況: 30.1% (使用中: 28.7GB, 空き: 66.7GB)
2025-05-08 20:41:43,729 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool, electronics, it
2025-05-08 20:41:43,730 - __main__ - INFO - Starting distillation
2025-05-08 20:41:43,730 - distillation - INFO - Starting distillation process
2025-05-08 20:41:43,730 - distillation - INFO - Loading data from models\train_data.json
2025-05-08 20:41:43,733 - distillation - INFO - Loaded 436 examples for distillation
2025-05-08 20:41:43,733 - distillation - INFO - Loading data from models\val_data.json
2025-05-08 20:41:43,734 - distillation - INFO - Loaded 43 examples for distillation
2025-05-08 20:41:43,735 - distillation - INFO - Loaded training dataset with 436 examples
2025-05-08 20:41:43,735 - distillation - INFO - Loaded validation dataset with 43 examples
2025-05-08 20:41:43,736 - distillation - INFO - Starting training loop
2025-05-08 20:41:43,770 - distillation - ERROR - Distillation process failed: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
2025-05-08 20:41:43,785 - distillation - ERROR - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\brain_model.py", line 155, in forward
    embedded = self.embedding(input_ids)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\sparse.py", line 190, in forward
    return F.embedding(
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\functional.py", line 2551, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 872, in distill
    outputs = self.student_model(
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\brain_model.py", line 200, in forward
    embedded = self.embedding(input_ids)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\sparse.py", line 190, in forward
    return F.embedding(
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\functional.py", line 2551, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)

2025-05-08 20:41:43,786 - __main__ - ERROR - 知識蒸留に失敗しました。モデル保存をスキップします。
2025-05-08 20:41:43,787 - __main__ - INFO - Saving model in Hugging Face format as lorinta/lal_v3
2025-05-08 20:41:43,788 - distillation - ERROR - モデルファイルが存在しません: None
2025-05-08 20:41:43,788 - distillation - INFO - 代わりに現在の学生モデルを保存します
2025-05-08 20:41:43,912 - distillation - ERROR - モデルのHugging Face形式での保存に失敗: 'BrainModel' object has no attribute 'save_pretrained'
2025-05-08 20:41:43,913 - distillation - ERROR - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 993, in save_model_hf_format
    self.student_model.save_pretrained(output_dir)
  File "C:\Users\s-rin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'BrainModel' object has no attribute 'save_pretrained'

2025-05-08 20:41:43,917 - __main__ - ERROR - Failed to save model in Hugging Face format
2025-05-08 20:41:44,086 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 20:41:44,339 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
