2025-05-04 13:09:06,815 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:09:06,819 - __main__ - INFO - RAM: 使用中 12.8GB / 95.4GB (13.4%)
2025-05-04 13:09:06,892 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:09:06,892 - __main__ - INFO - Using device: cuda
2025-05-04 13:09:06,892 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:09:06,893 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:09:06,893 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:09:06,893 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:16:02,776 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:16:02,787 - __main__ - INFO - RAM: 使用中 13.7GB / 95.4GB (14.4%)
2025-05-04 13:16:02,830 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:16:02,831 - __main__ - INFO - Using device: cuda
2025-05-04 13:16:02,831 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:16:02,832 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:16:02,832 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:16:02,832 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:16:03,721 - __main__ - INFO - Initializing student model
2025-05-04 13:16:04,650 - distillation - INFO - RAM使用状況: 15.5% (使用中: 14.8GB, 空き: 80.6GB)
2025-05-04 13:16:04,651 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:16:04,651 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:16:04,653 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:16:06,061 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:29:48,824 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:29:48,835 - __main__ - INFO - RAM: 使用中 14.3GB / 95.4GB (15.0%)
2025-05-04 13:29:48,836 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:29:48,836 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:29:56,484 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:29:56,484 - __main__ - INFO - Using device: cpu
2025-05-04 13:29:56,485 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:29:56,978 - __main__ - INFO - Initializing student model
2025-05-04 13:29:57,810 - distillation - INFO - RAM使用状況: 16.0% (使用中: 15.2GB, 空き: 80.2GB)
2025-05-04 13:29:57,811 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:29:57,812 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:29:57,869 - xformers - WARNING - WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.6.0+cpu)
    Python  3.10.11 (you have 3.10.6)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-05-04 13:30:01,264 - distillation - INFO - xformersが使用可能です
2025-05-04 13:30:01,265 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:30:01,273 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:30:01,274 - distillation - INFO - Qwen2モデル用にxformersでパフォーマンス最適化
2025-05-04 13:30:02,057 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
operator torchvision::nms does not exist
2025-05-04 13:30:02,058 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
operator torchvision::nms does not exist
2025-05-04 13:42:02,715 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:42:02,720 - __main__ - INFO - RAM: 使用中 14.7GB / 95.4GB (15.4%)
2025-05-04 13:42:02,789 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:42:02,789 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:42:02,790 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:42:20,169 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:42:20,169 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:42:20,169 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-04 13:42:20,170 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-04 13:42:23,020 - __main__ - INFO - Using device: cuda
2025-05-04 13:42:23,020 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:42:23,020 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:42:23,020 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:42:23,020 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:42:24,009 - __main__ - INFO - Initializing student model
2025-05-04 13:42:24,888 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:42:24,899 - distillation - INFO - RAM使用状況: 16.4% (使用中: 15.6GB, 空き: 79.8GB)
2025-05-04 13:42:24,900 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:42:24,900 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 13:42:24,906 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:42:29,070 - distillation - INFO - xformersが使用可能です
2025-05-04 13:42:29,071 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:42:29,080 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:42:29,513 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:42:29,513 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 13:42:29,514 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 13:42:29,515 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 13:42:29,515 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:42:29,516 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:42:29,517 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 13:42:59,736 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:42:59,736 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 13:42:59,736 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 13:43:05,919 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 13:43:05,920 - __main__ - INFO - 代替モデルの候補:
2025-05-04 13:43:05,920 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:43:05,920 - __main__ - INFO - 2. meta-llama/Llama-2-7b-hf
2025-05-04 13:43:05,920 - __main__ - INFO - 3. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:43:16,861 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:43:16,867 - __main__ - INFO - RAM: 使用中 14.6GB / 95.4GB (15.3%)
2025-05-04 13:43:16,926 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:43:16,926 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:43:16,927 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:43:19,199 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:43:19,199 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:43:19,199 - __main__ - INFO - Using device: cuda
2025-05-04 13:43:19,200 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:43:19,200 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:43:19,200 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:43:19,200 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:43:19,709 - __main__ - INFO - Initializing student model
2025-05-04 13:43:20,576 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:43:20,580 - distillation - INFO - RAM使用状況: 16.4% (使用中: 15.6GB, 空き: 79.7GB)
2025-05-04 13:43:20,580 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:43:20,581 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 13:43:20,581 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:43:21,804 - distillation - INFO - xformersが使用可能です
2025-05-04 13:43:21,804 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:43:21,810 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:43:22,202 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:43:22,202 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 13:43:22,203 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 13:43:22,203 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 13:43:22,204 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:43:22,204 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:43:22,204 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 13:45:59,750 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:45:59,751 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 13:45:59,751 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 13:46:01,458 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 13:46:01,458 - __main__ - INFO - 代替モデルの候補:
2025-05-04 13:46:01,458 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:46:01,458 - __main__ - INFO - 2. meta-llama/Llama-2-7b-hf
2025-05-04 13:46:01,458 - __main__ - INFO - 3. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:54:20,894 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 13:54:20,898 - __main__ - INFO - RAM: 使用中 15.5GB / 95.4GB (16.2%)
2025-05-04 13:54:20,925 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 13:54:20,925 - __main__ - INFO - Windows環境を検出しました
2025-05-04 13:54:20,926 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 13:54:34,799 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 13:54:34,799 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:54:34,799 - __main__ - INFO - Using device: cuda
2025-05-04 13:54:34,800 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 13:54:34,800 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 13:54:34,800 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 13:54:34,800 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:54:35,492 - __main__ - INFO - Initializing student model
2025-05-04 13:54:36,394 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 13:54:36,399 - distillation - INFO - RAM使用状況: 17.3% (使用中: 16.5GB, 空き: 78.9GB)
2025-05-04 13:54:36,399 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 13:54:36,399 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 13:54:36,400 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 13:54:37,562 - distillation - INFO - xformersが使用可能です
2025-05-04 13:54:37,562 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 13:54:37,568 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 13:54:37,955 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 13:54:37,955 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 13:54:37,956 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 13:54:37,957 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 13:54:37,958 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 13:54:37,958 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 13:54:37,959 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:00:10,236 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:00:10,246 - __main__ - INFO - RAM: 使用中 15.8GB / 95.4GB (16.6%)
2025-05-04 14:00:10,305 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:00:10,305 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:00:10,306 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 14:00:14,455 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 14:00:14,456 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:00:14,456 - __main__ - INFO - Using device: cuda
2025-05-04 14:00:14,456 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:00:14,456 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:00:14,456 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:00:14,457 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:00:15,291 - __main__ - INFO - Initializing student model
2025-05-04 14:00:16,128 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:00:16,141 - distillation - INFO - RAM使用状況: 17.6% (使用中: 16.8GB, 空き: 78.6GB)
2025-05-04 14:00:16,141 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:00:16,142 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:00:16,142 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:00:17,285 - distillation - INFO - xformersが使用可能です
2025-05-04 14:00:17,286 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:00:17,294 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:00:17,686 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:00:17,687 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:00:17,687 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:00:17,687 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:00:17,688 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:00:17,688 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:00:17,688 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:00:47,649 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:00:47,649 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:00:47,649 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:00:48,025 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:00:48,025 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:00:48,025 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:00:48,026 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:00:48,026 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:00:48,026 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:00:48,026 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:00:48,027 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:00:48,027 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:00:48,027 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 14:00:57,897 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:00:57,902 - __main__ - INFO - RAM: 使用中 15.7GB / 95.4GB (16.5%)
2025-05-04 14:00:57,966 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:00:57,966 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:00:57,967 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 14:01:02,685 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 14:01:02,685 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:02,686 - __main__ - INFO - Using device: cuda
2025-05-04 14:01:02,686 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:01:02,686 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:01:02,686 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:01:02,687 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:01:03,227 - __main__ - INFO - Initializing student model
2025-05-04 14:01:04,106 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:04,110 - distillation - INFO - RAM使用状況: 17.6% (使用中: 16.8GB, 空き: 78.6GB)
2025-05-04 14:01:04,111 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 14:01:04,111 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:01:04,112 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:01:05,280 - distillation - INFO - xformersが使用可能です
2025-05-04 14:01:05,281 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:01:05,282 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:01:05,676 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:05,676 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:01:05,677 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:01:05,677 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:01:05,678 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:05,678 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:05,679 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:01:08,356 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:08,357 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:01:08,357 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:01:10,095 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:01:10,095 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:01:10,096 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:10,096 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:01:10,096 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:01:10,096 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:01:10,096 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:01:10,096 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:01:10,097 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:10,097 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 14:01:25,878 - __main__ - INFO - 代替モデル microsoft/Phi-4-reasoning を使用します
2025-05-04 14:01:25,878 - __main__ - INFO - Phi-4-reasoning用に設定を最適化しています
2025-05-04 14:01:32,252 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:01:32,258 - __main__ - INFO - RAM: 使用中 16.9GB / 95.4GB (17.7%)
2025-05-04 14:01:32,259 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:01:32,259 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:01:32,260 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:32,260 - __main__ - INFO - Using device: cuda
2025-05-04 14:01:32,260 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:01:32,260 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:01:32,261 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:01:32,261 - __main__ - INFO - Loading tokenizer for microsoft/Phi-4-reasoning
2025-05-04 14:01:36,381 - __main__ - INFO - Initializing student model
2025-05-04 14:01:37,000 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:01:37,011 - distillation - INFO - RAM使用状況: 18.5% (使用中: 17.6GB, 空き: 77.8GB)
2025-05-04 14:01:37,011 - distillation - INFO - Loading teacher model: microsoft/Phi-4-reasoning
2025-05-04 14:01:37,011 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:01:37,012 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:01:37,013 - distillation - INFO - xformersが使用可能です
2025-05-04 14:01:37,013 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:01:37,015 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:01:37,520 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.phi3.modeling_phi3 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:37,520 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:01:37,520 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:01:37,520 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:01:37,522 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:37,522 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:37,522 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:01:53,566 - __main__ - ERROR - Error initializing distiller with microsoft/Phi-4-reasoning: Failed to import transformers.models.phi3.modeling_phi3 because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:01:53,566 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:01:53,567 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:01:55,276 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:01:55,277 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:01:55,277 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:01:55,277 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:01:55,277 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:01:55,278 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:01:55,278 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:01:55,278 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:01:55,278 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:01:55,278 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 14:01:58,564 - __main__ - INFO - 代替モデル elyza/Llama-3-ELYZA-JP-8B を使用します
2025-05-04 14:01:58,564 - __main__ - INFO - Llama-3-ELYZA-JP-8B用に設定を最適化しています
2025-05-04 14:02:04,151 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 14:02:04,157 - __main__ - INFO - RAM: 使用中 17.6GB / 95.4GB (18.5%)
2025-05-04 14:02:04,157 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 14:02:04,158 - __main__ - INFO - Windows環境を検出しました
2025-05-04 14:02:04,158 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:02:04,158 - __main__ - INFO - Using device: cuda
2025-05-04 14:02:04,159 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 14:02:04,159 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 14:02:04,159 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 14:02:04,159 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:02:04,679 - __main__ - INFO - Initializing student model
2025-05-04 14:02:05,410 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 14:02:05,422 - distillation - INFO - RAM使用状況: 19.4% (使用中: 18.5GB, 空き: 76.9GB)
2025-05-04 14:02:05,423 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:02:05,424 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 14:02:05,424 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 14:02:05,425 - distillation - INFO - xformersが使用可能です
2025-05-04 14:02:05,426 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 14:02:05,427 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-04 14:02:05,665 - distillation - ERROR - Failed to load teacher model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:02:05,665 - distillation - ERROR - tritonモジュールが見つかりません。PyTorch nightlyを使用している可能性があります。
2025-05-04 14:02:05,666 - distillation - INFO - --use_direct_gpu オプションを使用するか、別のモデルを選択してください。
2025-05-04 14:02:05,666 - distillation - INFO - 以下のモデルはWindows環境でも動作します:
2025-05-04 14:02:05,667 - distillation - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:02:05,667 - distillation - INFO - 2. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:02:05,668 - distillation - INFO - 3. cyberagent/calm2-7b
2025-05-04 14:02:15,418 - __main__ - ERROR - Error initializing distiller with elyza/Llama-3-ELYZA-JP-8B: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
No module named 'triton'
2025-05-04 14:02:15,419 - __main__ - ERROR - triton または flash-attn モジュールがインストールされていません
2025-05-04 14:02:15,419 - __main__ - INFO - 必要なパッケージをインストールするか、代替モデルを使用してください
2025-05-04 14:02:17,484 - __main__ - INFO - 代替の教師モデルを使用するか、--use_cpu_only オプションを使用してみてください
2025-05-04 14:02:17,484 - __main__ - INFO - 代替モデルの候補:
2025-05-04 14:02:17,484 - __main__ - INFO - 1. elyza/elyza-japanese-llama-2-7b
2025-05-04 14:02:17,484 - __main__ - INFO -    - バランスの取れた日本語対応モデル
2025-05-04 14:02:17,484 - __main__ - INFO - 2. microsoft/Phi-4-reasoning
2025-05-04 14:02:17,485 - __main__ - INFO -    - 小型で高性能な推論特化モデル
2025-05-04 14:02:17,485 - __main__ - INFO - 3. elyza/Llama-3-ELYZA-JP-8B
2025-05-04 14:02:17,485 - __main__ - INFO -    - Llama-3ベースの高性能日本語モデル
2025-05-04 14:02:17,485 - __main__ - INFO - 4. stabilityai/stablelm-base-alpha-7b
2025-05-04 14:02:17,485 - __main__ - INFO -    - 安定した軽量ベースモデル
2025-05-04 23:55:20,349 - __main__ - INFO - Starting knowledge distillation process
2025-05-04 23:55:20,362 - __main__ - INFO - RAM: 使用中 17.7GB / 95.4GB (18.5%)
2025-05-04 23:55:20,566 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-04 23:55:20,566 - __main__ - INFO - Windows環境を検出しました
2025-05-04 23:55:20,568 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-04 23:56:56,869 - __main__ - INFO - windows_modeを有効にしました
2025-05-04 23:56:56,870 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 23:56:56,871 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-04 23:56:56,872 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-04 23:59:39,194 - __main__ - INFO - Using device: cuda
2025-05-04 23:59:39,194 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-04 23:59:39,194 - __main__ - INFO - CUDA Version: 12.8
2025-05-04 23:59:39,195 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-04 23:59:39,195 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 23:59:39,832 - __main__ - INFO - Initializing student model
2025-05-04 23:59:40,700 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-04 23:59:40,705 - distillation - INFO - RAM使用状況: 19.6% (使用中: 18.7GB, 空き: 76.7GB)
2025-05-04 23:59:40,706 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-04 23:59:40,706 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-04 23:59:40,707 - distillation - INFO - Windows互換モードで実行しています
2025-05-04 23:59:43,072 - distillation - INFO - xformersが使用可能です
2025-05-04 23:59:43,072 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-04 23:59:43,074 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 00:00:33,056 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 00:01:10,430 - distillation - INFO - Teacher model loaded successfully
2025-05-05 00:01:10,431 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 00:01:10,433 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 00:01:10,434 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 00:09:49,643 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 00:09:49,655 - __main__ - INFO - RAM: 使用中 17.4GB / 95.4GB (18.2%)
2025-05-05 00:09:49,720 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 00:09:49,721 - __main__ - INFO - Windows環境を検出しました
2025-05-05 00:09:49,722 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 00:10:41,676 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 00:10:41,677 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:10:41,677 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 00:10:41,677 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 00:10:58,617 - __main__ - INFO - Using device: cuda
2025-05-05 00:10:58,617 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 00:10:58,617 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 00:10:58,617 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 00:10:58,618 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:10:59,225 - __main__ - INFO - Initializing student model
2025-05-05 00:11:00,125 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:11:00,129 - distillation - INFO - RAM使用状況: 19.2% (使用中: 18.3GB, 空き: 77.0GB)
2025-05-05 00:11:00,130 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:11:00,130 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 00:11:00,131 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 00:11:01,919 - distillation - INFO - xformersが使用可能です
2025-05-05 00:11:01,920 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 00:11:01,927 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 00:11:48,156 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 00:12:27,918 - distillation - INFO - Teacher model loaded successfully
2025-05-05 00:12:27,921 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 00:12:27,922 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 00:12:27,923 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 00:16:31,872 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 00:16:31,879 - __main__ - INFO - RAM: 使用中 17.4GB / 95.4GB (18.3%)
2025-05-05 00:16:31,942 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 00:16:31,942 - __main__ - INFO - Windows環境を検出しました
2025-05-05 00:16:31,943 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 00:17:39,959 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 00:17:39,959 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:17:39,959 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 00:17:39,960 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 00:17:42,620 - __main__ - INFO - Using device: cuda
2025-05-05 00:17:42,621 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 00:17:42,621 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 00:17:42,621 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 00:17:42,621 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:17:43,225 - __main__ - INFO - Initializing student model
2025-05-05 00:17:44,137 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 00:17:44,142 - distillation - INFO - RAM使用状況: 19.4% (使用中: 18.5GB, 空き: 76.9GB)
2025-05-05 00:17:44,142 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 00:17:44,142 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 00:17:44,143 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 00:17:45,808 - distillation - INFO - xformersが使用可能です
2025-05-05 00:17:45,808 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 00:17:45,810 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 00:18:33,005 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 00:19:12,726 - distillation - INFO - Teacher model loaded successfully
2025-05-05 00:19:12,728 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 00:19:12,728 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 00:19:12,729 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 00:19:12,730 - distillation - INFO - Generating 4000 questions for distillation
2025-05-05 00:19:12,730 - distillation - INFO - Generated 0 questions
2025-05-05 00:19:12,743 - distillation - INFO - Generated 100 questions
2025-05-05 00:19:12,745 - distillation - INFO - Generated 200 questions
2025-05-05 00:19:12,752 - distillation - INFO - Generated 300 questions
2025-05-05 00:19:12,754 - distillation - INFO - Generated 400 questions
2025-05-05 00:19:12,756 - distillation - INFO - Generated 500 questions
2025-05-05 00:19:12,758 - distillation - INFO - Generated 600 questions
2025-05-05 00:19:12,760 - distillation - INFO - Generated 700 questions
2025-05-05 00:19:12,762 - distillation - INFO - Generated 800 questions
2025-05-05 00:19:12,764 - distillation - INFO - Generated 900 questions
2025-05-05 00:19:12,765 - distillation - INFO - Generated 1000 questions
2025-05-05 00:19:12,767 - distillation - INFO - Generated 1100 questions
2025-05-05 00:19:12,769 - distillation - INFO - Generated 1200 questions
2025-05-05 00:19:12,770 - distillation - INFO - Generated 1300 questions
2025-05-05 00:19:12,772 - distillation - INFO - Generated 1400 questions
2025-05-05 00:19:12,774 - distillation - INFO - Generated 1500 questions
2025-05-05 00:19:12,775 - distillation - INFO - Generated 1600 questions
2025-05-05 00:19:12,777 - distillation - INFO - Generated 1700 questions
2025-05-05 00:19:12,779 - distillation - INFO - Generated 1800 questions
2025-05-05 00:19:12,780 - distillation - INFO - Generated 1900 questions
2025-05-05 00:19:12,782 - distillation - INFO - Generated 2000 questions
2025-05-05 00:19:12,784 - distillation - INFO - Generated 2100 questions
2025-05-05 00:19:12,785 - distillation - INFO - Generated 2200 questions
2025-05-05 00:19:12,787 - distillation - INFO - Generated 2300 questions
2025-05-05 00:19:12,789 - distillation - INFO - Generated 2400 questions
2025-05-05 00:19:12,791 - distillation - INFO - Generated 2500 questions
2025-05-05 00:19:12,792 - distillation - INFO - Generated 2600 questions
2025-05-05 00:19:12,794 - distillation - INFO - Generated 2700 questions
2025-05-05 00:19:12,796 - distillation - INFO - Generated 2800 questions
2025-05-05 00:19:12,798 - distillation - INFO - Generated 2900 questions
2025-05-05 00:19:12,799 - distillation - INFO - Generated 3000 questions
2025-05-05 00:19:12,801 - distillation - INFO - Generated 3100 questions
2025-05-05 00:19:12,803 - distillation - INFO - Generated 3200 questions
2025-05-05 00:19:12,804 - distillation - INFO - Generated 3300 questions
2025-05-05 00:19:12,806 - distillation - INFO - Generated 3400 questions
2025-05-05 00:19:12,808 - distillation - INFO - Generated 3500 questions
2025-05-05 00:19:12,810 - distillation - INFO - Generated 3600 questions
2025-05-05 00:19:12,811 - distillation - INFO - Generated 3700 questions
2025-05-05 00:19:12,813 - distillation - INFO - Generated 3800 questions
2025-05-05 00:19:12,816 - distillation - INFO - Generated 3900 questions
2025-05-05 00:19:12,820 - distillation - INFO - Generated questions saved to questions.txt
2025-05-05 00:19:12,821 - distillation - INFO - Using PyTorch nightly version 2.7.0.dev20250309+cu128 for data generation
2025-05-05 00:19:12,821 - distillation - INFO - Processing batch 1/1000
2025-05-05 00:38:38,045 - distillation - INFO - Processing batch 2/1000
2025-05-05 19:14:17,645 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 19:14:17,650 - __main__ - INFO - RAM: 使用中 14.1GB / 95.4GB (14.8%)
2025-05-05 19:14:17,837 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 19:14:17,837 - __main__ - INFO - Windows環境を検出しました
2025-05-05 19:14:17,837 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 19:14:25,984 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 19:14:25,985 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:14:25,985 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 19:14:25,985 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 19:14:30,172 - __main__ - INFO - Using device: cuda
2025-05-05 19:14:30,172 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 19:14:30,172 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 19:14:30,173 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 19:14:30,173 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:14:30,741 - __main__ - INFO - Initializing student model
2025-05-05 19:14:31,582 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:14:31,588 - distillation - INFO - RAM使用状況: 15.9% (使用中: 15.2GB, 空き: 80.2GB)
2025-05-05 19:14:31,588 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:14:31,588 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 19:14:31,589 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 19:14:33,962 - distillation - INFO - xformersが使用可能です
2025-05-05 19:14:33,963 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 19:14:33,971 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 19:15:20,838 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 19:16:03,972 - distillation - INFO - Teacher model loaded successfully
2025-05-05 19:16:03,974 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 19:16:03,975 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 19:16:03,976 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-05 19:16:03,977 - distillation - INFO - Generating 4000 questions for distillation
2025-05-05 19:16:03,977 - distillation - INFO - Generated 0 questions
2025-05-05 19:16:03,982 - distillation - INFO - Generated 100 questions
2025-05-05 19:16:03,984 - distillation - INFO - Generated 200 questions
2025-05-05 19:16:03,986 - distillation - INFO - Generated 300 questions
2025-05-05 19:16:03,989 - distillation - INFO - Generated 400 questions
2025-05-05 19:16:03,991 - distillation - INFO - Generated 500 questions
2025-05-05 19:16:03,994 - distillation - INFO - Generated 600 questions
2025-05-05 19:16:03,995 - distillation - INFO - Generated 700 questions
2025-05-05 19:16:03,997 - distillation - INFO - Generated 800 questions
2025-05-05 19:16:04,000 - distillation - INFO - Generated 900 questions
2025-05-05 19:16:04,002 - distillation - INFO - Generated 1000 questions
2025-05-05 19:16:04,004 - distillation - INFO - Generated 1100 questions
2025-05-05 19:16:04,006 - distillation - INFO - Generated 1200 questions
2025-05-05 19:16:04,007 - distillation - INFO - Generated 1300 questions
2025-05-05 19:16:04,009 - distillation - INFO - Generated 1400 questions
2025-05-05 19:16:04,015 - distillation - INFO - Generated 1500 questions
2025-05-05 19:16:04,017 - distillation - INFO - Generated 1600 questions
2025-05-05 19:16:04,019 - distillation - INFO - Generated 1700 questions
2025-05-05 19:16:04,022 - distillation - INFO - Generated 1800 questions
2025-05-05 19:16:04,024 - distillation - INFO - Generated 1900 questions
2025-05-05 19:16:04,026 - distillation - INFO - Generated 2000 questions
2025-05-05 19:16:04,028 - distillation - INFO - Generated 2100 questions
2025-05-05 19:16:04,034 - distillation - INFO - Generated 2200 questions
2025-05-05 19:16:04,036 - distillation - INFO - Generated 2300 questions
2025-05-05 19:16:04,037 - distillation - INFO - Generated 2400 questions
2025-05-05 19:16:04,039 - distillation - INFO - Generated 2500 questions
2025-05-05 19:16:04,041 - distillation - INFO - Generated 2600 questions
2025-05-05 19:16:04,043 - distillation - INFO - Generated 2700 questions
2025-05-05 19:16:04,045 - distillation - INFO - Generated 2800 questions
2025-05-05 19:16:04,046 - distillation - INFO - Generated 2900 questions
2025-05-05 19:16:04,048 - distillation - INFO - Generated 3000 questions
2025-05-05 19:16:04,050 - distillation - INFO - Generated 3100 questions
2025-05-05 19:16:04,052 - distillation - INFO - Generated 3200 questions
2025-05-05 19:16:04,054 - distillation - INFO - Generated 3300 questions
2025-05-05 19:16:04,056 - distillation - INFO - Generated 3400 questions
2025-05-05 19:16:04,058 - distillation - INFO - Generated 3500 questions
2025-05-05 19:16:04,061 - distillation - INFO - Generated 3600 questions
2025-05-05 19:16:04,062 - distillation - INFO - Generated 3700 questions
2025-05-05 19:16:04,064 - distillation - INFO - Generated 3800 questions
2025-05-05 19:16:04,065 - distillation - INFO - Generated 3900 questions
2025-05-05 19:16:04,069 - distillation - INFO - Generated questions saved to questions.txt
2025-05-05 19:16:04,070 - distillation - INFO - Using PyTorch nightly version 2.7.0.dev20250309+cu128 for data generation
2025-05-05 19:16:04,070 - distillation - INFO - Processing batch 1/1000
2025-05-05 19:18:38,923 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-05 19:37:24,506 - __main__ - INFO - Starting knowledge distillation process
2025-05-05 19:37:24,517 - __main__ - INFO - RAM: 使用中 10.4GB / 95.4GB (10.9%)
2025-05-05 19:37:24,541 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-05 19:37:24,541 - __main__ - INFO - Windows環境を検出しました
2025-05-05 19:37:24,542 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-05 19:37:39,543 - __main__ - INFO - windows_modeを有効にしました
2025-05-05 19:37:39,543 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:37:39,544 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-05 19:37:39,544 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-05 19:37:43,980 - __main__ - INFO - Using device: cuda
2025-05-05 19:37:43,980 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-05 19:37:43,981 - __main__ - INFO - CUDA Version: 12.8
2025-05-05 19:37:43,981 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-05 19:37:43,981 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:37:44,867 - __main__ - INFO - Initializing student model
2025-05-05 19:37:45,706 - distillation - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-05 19:37:45,717 - distillation - INFO - RAM使用状況: 12.0% (使用中: 11.5GB, 空き: 83.9GB)
2025-05-05 19:37:45,718 - distillation - INFO - Loading teacher model: elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-05 19:37:45,718 - distillation - INFO - GPU直接アクセスモードで実行します（最適化ライブラリ未使用）
2025-05-05 19:37:45,719 - distillation - INFO - Windows互換モードで実行しています
2025-05-05 19:37:47,683 - distillation - INFO - xformersが使用可能です
2025-05-05 19:37:47,684 - distillation - INFO - Configuring 4-bit quantization for teacher model
2025-05-05 19:37:47,686 - distillation - INFO - Using automatic device mapping with CPU offloading
2025-05-05 19:38:27,984 - distillation - INFO - モデルをcudaに手動で移動します
2025-05-05 19:39:11,408 - distillation - INFO - Teacher model loaded successfully
2025-05-05 19:39:11,421 - __main__ - INFO - Generating 5000 distillation examples
2025-05-05 19:39:11,422 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-05 19:39:11,428 - distillation - INFO - Using PyTorch nightly version 2.7.0.dev20250309+cu128 for data generation
2025-05-05 19:39:11,428 - distillation - INFO - Processing batch 1/1000
2025-05-05 19:57:18,123 - distillation - INFO - Processing batch 2/1000
2025-05-05 20:15:05,750 - distillation - INFO - Processing batch 3/1000
2025-05-05 20:32:52,518 - distillation - INFO - Processing batch 4/1000
2025-05-05 20:50:39,044 - distillation - INFO - Processing batch 5/1000
2025-05-05 21:08:24,119 - distillation - INFO - Processing batch 6/1000
2025-05-05 21:26:09,554 - distillation - INFO - Processing batch 7/1000
2025-05-05 21:44:03,229 - distillation - INFO - Processing batch 8/1000
2025-05-05 22:01:57,091 - distillation - INFO - Processing batch 9/1000
2025-05-05 22:19:02,389 - distillation - INFO - Processing batch 10/1000
2025-05-05 22:37:00,271 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_40
2025-05-05 22:37:00,272 - distillation - INFO - Processing batch 11/1000
2025-05-05 22:55:04,181 - distillation - INFO - Processing batch 12/1000
2025-05-05 23:13:06,050 - distillation - INFO - Processing batch 13/1000
2025-05-05 23:31:00,684 - distillation - INFO - Processing batch 14/1000
2025-05-05 23:49:00,736 - distillation - INFO - Processing batch 15/1000
2025-05-06 00:07:04,566 - distillation - INFO - Processing batch 16/1000
2025-05-06 00:25:07,218 - distillation - INFO - Processing batch 17/1000
2025-05-06 00:43:08,792 - distillation - INFO - Processing batch 18/1000
2025-05-06 01:01:12,135 - distillation - INFO - Processing batch 19/1000
2025-05-06 01:19:10,856 - distillation - INFO - Processing batch 20/1000
2025-05-06 01:37:10,916 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_80
2025-05-06 01:37:10,916 - distillation - INFO - Processing batch 21/1000
2025-05-06 01:55:11,228 - distillation - INFO - Processing batch 22/1000
2025-05-06 02:13:01,497 - distillation - INFO - Processing batch 23/1000
2025-05-06 02:31:00,071 - distillation - INFO - Processing batch 24/1000
2025-05-06 02:49:03,913 - distillation - INFO - Processing batch 25/1000
2025-05-06 03:06:59,700 - distillation - INFO - Processing batch 26/1000
2025-05-06 03:25:02,820 - distillation - INFO - Processing batch 27/1000
2025-05-06 03:43:02,734 - distillation - INFO - Processing batch 28/1000
2025-05-06 04:01:01,968 - distillation - INFO - Processing batch 29/1000
2025-05-06 04:19:03,270 - distillation - INFO - Processing batch 30/1000
2025-05-06 04:37:02,355 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_120
2025-05-06 04:37:02,356 - distillation - INFO - Processing batch 31/1000
2025-05-06 04:54:59,146 - distillation - INFO - Processing batch 32/1000
2025-05-06 05:12:50,872 - distillation - INFO - Processing batch 33/1000
2025-05-06 05:30:46,946 - distillation - INFO - Processing batch 34/1000
2025-05-06 05:48:38,766 - distillation - INFO - Processing batch 35/1000
2025-05-06 06:06:32,509 - distillation - INFO - Processing batch 36/1000
2025-05-06 06:24:28,888 - distillation - INFO - Processing batch 37/1000
2025-05-06 06:42:24,129 - distillation - INFO - Processing batch 38/1000
2025-05-06 07:00:20,255 - distillation - INFO - Processing batch 39/1000
2025-05-06 07:18:15,042 - distillation - INFO - Processing batch 40/1000
2025-05-06 07:36:08,224 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_160
2025-05-06 07:36:08,224 - distillation - INFO - Processing batch 41/1000
2025-05-06 07:54:03,341 - distillation - INFO - Processing batch 42/1000
2025-05-06 08:11:58,898 - distillation - INFO - Processing batch 43/1000
2025-05-06 08:29:58,144 - distillation - INFO - Processing batch 44/1000
2025-05-06 08:47:54,606 - distillation - INFO - Processing batch 45/1000
2025-05-06 09:05:49,136 - distillation - INFO - Processing batch 46/1000
2025-05-06 09:23:38,117 - distillation - INFO - Processing batch 47/1000
2025-05-06 09:41:49,085 - distillation - INFO - Processing batch 48/1000
2025-05-06 10:00:04,392 - distillation - INFO - Processing batch 49/1000
2025-05-06 10:17:59,313 - distillation - INFO - Processing batch 50/1000
2025-05-06 10:35:54,840 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_200
2025-05-06 10:35:54,840 - distillation - INFO - Processing batch 51/1000
2025-05-06 10:53:45,497 - distillation - INFO - Processing batch 52/1000
2025-05-06 11:11:28,831 - distillation - INFO - Processing batch 53/1000
2025-05-06 11:29:22,668 - distillation - INFO - Processing batch 54/1000
2025-05-06 11:47:17,464 - distillation - INFO - Processing batch 55/1000
2025-05-06 12:05:15,315 - distillation - INFO - Processing batch 56/1000
2025-05-06 12:23:22,105 - distillation - INFO - Processing batch 57/1000
2025-05-06 12:41:22,010 - distillation - INFO - Processing batch 58/1000
2025-05-06 12:59:17,226 - distillation - INFO - Processing batch 59/1000
2025-05-06 13:17:12,326 - distillation - INFO - Processing batch 60/1000
2025-05-06 13:35:15,258 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_240
2025-05-06 13:35:15,258 - distillation - INFO - Processing batch 61/1000
2025-05-06 13:53:14,368 - distillation - INFO - Processing batch 62/1000
2025-05-06 14:11:34,199 - distillation - INFO - Processing batch 63/1000
2025-05-06 14:30:00,707 - distillation - INFO - Processing batch 64/1000
2025-05-06 14:48:23,825 - distillation - INFO - Processing batch 65/1000
2025-05-06 15:06:47,462 - distillation - INFO - Processing batch 66/1000
2025-05-06 15:25:04,602 - distillation - INFO - Processing batch 67/1000
2025-05-06 15:43:28,362 - distillation - INFO - Processing batch 68/1000
2025-05-06 16:01:49,094 - distillation - INFO - Processing batch 69/1000
2025-05-06 16:20:08,201 - distillation - INFO - Processing batch 70/1000
2025-05-06 16:38:17,477 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_280
2025-05-06 16:38:17,477 - distillation - INFO - Processing batch 71/1000
2025-05-06 16:56:31,797 - distillation - INFO - Processing batch 72/1000
2025-05-06 17:14:50,958 - distillation - INFO - Processing batch 73/1000
2025-05-06 17:33:00,482 - distillation - INFO - Processing batch 74/1000
2025-05-06 17:51:15,509 - distillation - INFO - Processing batch 75/1000
2025-05-06 18:09:28,788 - distillation - INFO - Processing batch 76/1000
2025-05-06 18:27:33,361 - distillation - INFO - Processing batch 77/1000
2025-05-06 18:45:40,977 - distillation - INFO - Processing batch 78/1000
2025-05-06 19:03:46,914 - distillation - INFO - Processing batch 79/1000
2025-05-06 19:21:56,799 - distillation - INFO - Processing batch 80/1000
2025-05-06 19:40:11,666 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_320
2025-05-06 19:40:11,666 - distillation - INFO - Processing batch 81/1000
2025-05-06 19:58:24,708 - distillation - INFO - Processing batch 82/1000
2025-05-06 20:16:36,995 - distillation - INFO - Processing batch 83/1000
2025-05-06 20:34:42,898 - distillation - INFO - Processing batch 84/1000
2025-05-06 20:52:50,202 - distillation - INFO - Processing batch 85/1000
2025-05-06 21:10:59,550 - distillation - INFO - Processing batch 86/1000
2025-05-06 21:29:01,038 - distillation - INFO - Processing batch 87/1000
2025-05-06 21:46:56,073 - distillation - INFO - Processing batch 88/1000
2025-05-06 22:04:45,909 - distillation - INFO - Processing batch 89/1000
2025-05-06 22:22:47,506 - distillation - INFO - Processing batch 90/1000
2025-05-06 22:40:36,106 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_360
2025-05-06 22:40:36,106 - distillation - INFO - Processing batch 91/1000
2025-05-06 22:58:29,342 - distillation - INFO - Processing batch 92/1000
2025-05-06 23:16:19,824 - distillation - INFO - Processing batch 93/1000
2025-05-06 23:34:17,410 - distillation - INFO - Processing batch 94/1000
2025-05-06 23:52:11,085 - distillation - INFO - Processing batch 95/1000
2025-05-07 00:10:09,091 - distillation - INFO - Processing batch 96/1000
2025-05-07 00:28:01,853 - distillation - INFO - Processing batch 97/1000
2025-05-07 00:45:52,383 - distillation - INFO - Processing batch 98/1000
2025-05-07 01:03:48,757 - distillation - INFO - Processing batch 99/1000
2025-05-07 01:21:45,692 - distillation - INFO - Processing batch 100/1000
2025-05-07 01:39:42,952 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_400
2025-05-07 01:39:42,952 - distillation - INFO - Processing batch 101/1000
2025-05-07 01:57:34,036 - distillation - INFO - Processing batch 102/1000
2025-05-07 02:15:26,233 - distillation - INFO - Processing batch 103/1000
2025-05-07 02:33:21,448 - distillation - INFO - Processing batch 104/1000
2025-05-07 02:51:09,351 - distillation - INFO - Processing batch 105/1000
2025-05-07 03:09:00,313 - distillation - INFO - Processing batch 106/1000
2025-05-07 03:26:57,110 - distillation - INFO - Processing batch 107/1000
2025-05-07 03:44:50,271 - distillation - INFO - Processing batch 108/1000
2025-05-07 04:02:40,550 - distillation - INFO - Processing batch 109/1000
2025-05-07 04:20:36,906 - distillation - INFO - Processing batch 110/1000
2025-05-07 04:38:30,735 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_440
2025-05-07 04:38:30,735 - distillation - INFO - Processing batch 111/1000
2025-05-07 04:56:24,160 - distillation - INFO - Processing batch 112/1000
2025-05-07 05:14:18,763 - distillation - INFO - Processing batch 113/1000
2025-05-07 05:32:12,446 - distillation - INFO - Processing batch 114/1000
2025-05-07 05:50:00,351 - distillation - INFO - Processing batch 115/1000
2025-05-07 06:07:51,306 - distillation - INFO - Processing batch 116/1000
2025-05-07 06:25:47,696 - distillation - INFO - Processing batch 117/1000
2025-05-07 06:43:40,886 - distillation - INFO - Processing batch 118/1000
2025-05-07 07:02:13,096 - distillation - INFO - Processing batch 119/1000
2025-05-07 07:20:36,455 - distillation - INFO - Processing batch 120/1000
2025-05-07 07:38:58,804 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_480
2025-05-07 07:38:58,805 - distillation - INFO - Processing batch 121/1000
2025-05-07 07:57:25,406 - distillation - INFO - Processing batch 122/1000
2025-05-07 08:15:50,119 - distillation - INFO - Processing batch 123/1000
2025-05-07 08:34:14,433 - distillation - INFO - Processing batch 124/1000
2025-05-07 08:52:33,865 - distillation - INFO - Processing batch 125/1000
2025-05-07 09:10:34,732 - distillation - INFO - Processing batch 126/1000
2025-05-07 09:28:33,676 - distillation - INFO - Processing batch 127/1000
2025-05-07 09:46:38,534 - distillation - INFO - Processing batch 128/1000
2025-05-07 10:04:41,866 - distillation - INFO - Processing batch 129/1000
2025-05-07 10:22:42,467 - distillation - INFO - Processing batch 130/1000
2025-05-07 10:40:42,439 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_520
2025-05-07 10:40:42,439 - distillation - INFO - Processing batch 131/1000
2025-05-07 10:58:42,599 - distillation - INFO - Processing batch 132/1000
2025-05-07 11:16:46,743 - distillation - INFO - Processing batch 133/1000
2025-05-07 11:35:55,770 - distillation - INFO - Processing batch 134/1000
2025-05-07 12:08:20,362 - distillation - INFO - Processing batch 135/1000
2025-05-07 12:35:58,525 - distillation - INFO - Processing batch 136/1000
2025-05-07 13:03:44,376 - distillation - INFO - Processing batch 137/1000
2025-05-07 13:31:36,560 - distillation - INFO - Processing batch 138/1000
2025-05-07 14:00:22,402 - distillation - INFO - Processing batch 139/1000
2025-05-07 14:28:32,200 - distillation - INFO - Processing batch 140/1000
2025-05-07 14:57:37,643 - distillation - INFO - Intermediate data saved to ./models\train_data.json.temp_560
2025-05-07 14:57:37,644 - distillation - INFO - Processing batch 141/1000
2025-05-07 15:20:30,480 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 19:52:06,120 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 19:52:06,125 - __main__ - INFO - RAM: 使用中 26.3GB / 95.4GB (27.6%)
2025-05-07 19:52:06,161 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 19:52:06,161 - __main__ - WARNING - LMstudioと通常のteacher_modelの両方が指定されています。LMstudioが優先されます。
2025-05-07 19:52:06,162 - __main__ - INFO - Windows環境を検出しました
2025-05-07 19:52:06,162 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 19:52:12,225 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 19:52:12,225 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 19:52:12,226 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 19:52:12,229 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 19:52:14,777 - __main__ - INFO - Using device: cuda
2025-05-07 19:52:14,777 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 19:52:14,777 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 19:52:14,778 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 19:52:14,778 - __main__ - INFO - Loading tokenizer for elyza/ELYZA-Thinking-1.0-Qwen-32B
2025-05-07 19:52:15,543 - __main__ - INFO - Initializing student model
2025-05-07 19:52:16,575 - distillation - INFO - RAM使用状況: 28.6% (使用中: 27.3GB, 空き: 68.1GB)
2025-05-07 19:52:16,739 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 19:52:18,769 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 19:52:18,770 - __main__ - ERROR - Error initializing distiller with elyza/ELYZA-Thinking-1.0-Qwen-32B: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 19:52:18,830 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 20:34:34,990 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 20:34:34,995 - __main__ - INFO - RAM: 使用中 27.5GB / 95.4GB (28.8%)
2025-05-07 20:34:35,046 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 20:34:35,047 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 20:34:35,047 - __main__ - INFO - Windows環境を検出しました
2025-05-07 20:34:35,047 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 20:34:41,657 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 20:34:41,658 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 20:34:41,658 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 20:34:41,658 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 20:34:45,754 - __main__ - INFO - Using device: cuda
2025-05-07 20:34:45,754 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 20:34:45,755 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 20:34:45,755 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 20:34:50,144 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 20:34:50,144 - __main__ - INFO - Initializing student model
2025-05-07 20:34:50,461 - distillation - INFO - RAM使用状況: 29.3% (使用中: 28.0GB, 空き: 67.4GB)
2025-05-07 20:34:50,462 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 20:34:52,495 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 20:34:52,495 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 20:34:52,557 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:03:18,558 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:03:18,564 - __main__ - INFO - RAM: 使用中 31.5GB / 95.4GB (33.0%)
2025-05-07 21:03:18,581 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:03:18,581 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:03:18,582 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:03:18,582 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:03:21,157 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:03:21,157 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:03:21,157 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:03:21,158 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:03:36,734 - __main__ - INFO - Using device: cuda
2025-05-07 21:03:36,734 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:03:36,734 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:03:36,735 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:03:37,198 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:03:37,199 - __main__ - INFO - Initializing student model
2025-05-07 21:03:37,537 - distillation - INFO - RAM使用状況: 33.3% (使用中: 31.8GB, 空き: 63.6GB)
2025-05-07 21:03:37,537 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:03:39,566 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:03:39,566 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:03:39,630 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:06:19,831 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:06:19,835 - __main__ - INFO - RAM: 使用中 31.6GB / 95.4GB (33.1%)
2025-05-07 21:06:19,856 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:06:19,856 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:06:19,856 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:06:19,856 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:06:31,422 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:06:31,422 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:06:31,422 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:06:34,441 - __main__ - INFO - Using device: cuda
2025-05-07 21:06:34,441 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:06:34,442 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:06:34,442 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:06:34,862 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:06:34,863 - __main__ - INFO - Initializing student model
2025-05-07 21:06:35,197 - distillation - INFO - RAM使用状況: 33.6% (使用中: 32.0GB, 空き: 63.3GB)
2025-05-07 21:06:35,197 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:06:37,228 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:06:37,228 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:06:37,287 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:11:00,282 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:11:00,290 - __main__ - INFO - RAM: 使用中 31.7GB / 95.4GB (33.2%)
2025-05-07 21:11:00,314 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:11:00,314 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:11:00,314 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:11:00,315 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:11:29,397 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:11:29,398 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:11:29,398 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:11:29,398 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:11:30,593 - __main__ - INFO - Using device: cuda
2025-05-07 21:11:30,593 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:11:30,593 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:11:30,594 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:11:31,088 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:11:31,088 - __main__ - INFO - Initializing student model
2025-05-07 21:11:31,418 - distillation - INFO - RAM使用状況: 33.5% (使用中: 32.0GB, 空き: 63.4GB)
2025-05-07 21:11:31,418 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:11:33,475 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:11:33,476 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:11:33,535 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:34:06,490 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:34:06,495 - __main__ - INFO - RAM: 使用中 31.9GB / 95.4GB (33.4%)
2025-05-07 21:34:06,553 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:34:06,553 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:34:06,553 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:34:06,553 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:34:10,157 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:34:10,158 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:34:10,158 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:34:13,240 - __main__ - INFO - Using device: cuda
2025-05-07 21:34:13,240 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:34:13,241 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:34:13,241 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:34:13,627 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:34:13,627 - __main__ - INFO - Initializing student model
2025-05-07 21:34:13,968 - distillation - INFO - RAM使用状況: 33.7% (使用中: 32.1GB, 空き: 63.2GB)
2025-05-07 21:34:13,969 - distillation - INFO - Using LMstudio API at http://localhost:1234/v1
2025-05-07 21:34:16,011 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:34:16,012 - __main__ - ERROR - Error initializing distiller with http://localhost:1234/v1: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:34:16,074 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:35:02,313 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:35:02,318 - __main__ - INFO - RAM: 使用中 31.9GB / 95.4GB (33.5%)
2025-05-07 21:35:02,338 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:35:02,338 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:35:02,338 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:35:02,338 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:35:05,086 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:35:05,086 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:35:05,088 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:35:05,088 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:35:06,356 - __main__ - INFO - Using device: cuda
2025-05-07 21:35:06,357 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:35:06,357 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:35:06,357 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:35:07,107 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:35:07,107 - __main__ - INFO - Initializing student model
2025-05-07 21:35:07,431 - distillation - INFO - RAM使用状況: 34.0% (使用中: 32.4GB, 空き: 63.0GB)
2025-05-07 21:35:07,431 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234/v1/chat/completions
2025-05-07 21:35:07,444 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:35:07,444 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234/v1/chat/completions: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:35:07,505 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:36:33,533 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:36:33,538 - __main__ - INFO - RAM: 使用中 31.8GB / 95.4GB (33.4%)
2025-05-07 21:36:33,556 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:36:33,556 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:36:33,556 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:36:33,556 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:36:35,632 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:36:35,632 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:36:35,632 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:36:35,632 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:36:36,644 - __main__ - INFO - Using device: cuda
2025-05-07 21:36:36,645 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:36:36,645 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:36:36,645 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:36:37,055 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:36:37,055 - __main__ - INFO - Initializing student model
2025-05-07 21:36:37,391 - distillation - INFO - RAM使用状況: 33.8% (使用中: 32.3GB, 空き: 63.1GB)
2025-05-07 21:36:37,391 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234
2025-05-07 21:36:37,405 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:36:37,405 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:36:37,465 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:37:42,954 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:37:42,958 - __main__ - INFO - RAM: 使用中 31.8GB / 95.4GB (33.3%)
2025-05-07 21:37:42,979 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:37:42,979 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:37:42,980 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:37:42,980 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:37:45,549 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:37:45,550 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:37:45,550 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:37:45,550 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:37:46,483 - __main__ - INFO - Using device: cuda
2025-05-07 21:37:46,483 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:37:46,483 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:37:46,484 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:37:47,205 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:37:47,205 - __main__ - INFO - Initializing student model
2025-05-07 21:37:47,540 - distillation - INFO - RAM使用状況: 33.8% (使用中: 32.2GB, 空き: 63.2GB)
2025-05-07 21:37:47,540 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234
2025-05-07 21:37:47,549 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:37:47,550 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:37:47,610 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 21:39:34,981 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 21:39:34,986 - __main__ - INFO - RAM: 使用中 31.7GB / 95.4GB (33.2%)
2025-05-07 21:39:35,005 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 21:39:35,006 - __main__ - INFO - LMstudioのAPIを使用して蒸留データを生成します
2025-05-07 21:39:35,006 - __main__ - INFO - Windows環境を検出しました
2025-05-07 21:39:35,006 - __main__ - WARNING - WindowsでQwen2モデルを使用する場合はwindows_modeを推奨します
2025-05-07 21:39:37,028 - __main__ - INFO - windows_modeを有効にしました
2025-05-07 21:39:37,028 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 21:39:37,028 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 21:39:37,028 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 21:39:37,673 - __main__ - INFO - Using device: cuda
2025-05-07 21:39:37,674 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 21:39:37,674 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 21:39:37,674 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 21:39:38,419 - __main__ - INFO - LMstudio用にデフォルトのGPT-2トークナイザーを使用します
2025-05-07 21:39:38,420 - __main__ - INFO - Initializing student model
2025-05-07 21:39:38,775 - distillation - INFO - RAM使用状況: 33.7% (使用中: 32.1GB, 空き: 63.3GB)
2025-05-07 21:39:38,775 - distillation - INFO - Using LMstudio API at http://127.0.0.1:1234
2025-05-07 21:39:38,784 - distillation - ERROR - LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:39:38,785 - __main__ - ERROR - Error initializing distiller with http://127.0.0.1:1234: LMstudio API connection error: 'str' object has no attribute 'get'
2025-05-07 21:39:38,845 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:20:01,467 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:20:01,472 - __main__ - INFO - RAM: 使用中 27.9GB / 95.4GB (29.3%)
2025-05-07 22:20:01,532 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:20:01,533 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:20:01,533 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:20:01,533 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:20:01,533 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:20:01,533 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:20:01,534 - __main__ - INFO - Using device: cuda
2025-05-07 22:20:01,535 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:20:01,535 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:20:01,535 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:20:01,535 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:20:02,548 - __main__ - ERROR - Failed to load tokenizer for GGUF model: The repository for Qwen/Qwen-14B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-14B.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-05-07 22:20:02,610 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:20:19,970 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:20:19,983 - __main__ - INFO - RAM: 使用中 27.9GB / 95.4GB (29.3%)
2025-05-07 22:20:20,029 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:20:20,030 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:20:20,030 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:20:20,030 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:20:20,030 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:20:20,030 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:20:27,110 - __main__ - INFO - Using device: cuda
2025-05-07 22:20:27,110 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:20:27,110 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:20:27,110 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:20:27,110 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:20:27,376 - __main__ - ERROR - Failed to load tokenizer for GGUF model: The repository for Qwen/Qwen-14B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-14B.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-05-07 22:20:27,442 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:29:43,677 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:29:43,686 - __main__ - INFO - RAM: 使用中 28.0GB / 95.4GB (29.4%)
2025-05-07 22:29:43,738 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:29:43,739 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:29:43,739 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:29:43,740 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:29:43,740 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:29:43,740 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:30:15,574 - __main__ - INFO - Using device: cuda
2025-05-07 22:30:15,574 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:30:15,574 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:30:15,574 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:30:15,575 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:30:15,947 - __main__ - ERROR - Failed to load tokenizer for GGUF model: The repository for Qwen/Qwen-14B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-14B.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2025-05-07 22:30:16,010 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:35:40,990 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:35:41,000 - __main__ - INFO - RAM: 使用中 28.3GB / 95.4GB (29.7%)
2025-05-07 22:35:41,047 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:35:41,047 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:35:41,048 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:35:41,048 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:35:41,048 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:35:41,049 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:35:46,721 - __main__ - INFO - Using device: cuda
2025-05-07 22:35:46,721 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:35:46,722 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:35:46,722 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:35:46,722 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:35:49,858 - __main__ - INFO - Initializing student model
2025-05-07 22:35:51,483 - distillation - INFO - RAM使用状況: 30.8% (使用中: 29.4GB, 空き: 66.0GB)
2025-05-07 22:35:51,483 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 22:35:52,684 - distillation - INFO - GGUF model loaded successfully
2025-05-07 22:35:52,684 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 22:35:52,685 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 22:35:52,718 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 22:35:52,719 - distillation - INFO - Processing batch 1/1000
2025-05-07 22:35:52,719 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 22:36:10,368 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:41:20,611 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:41:20,619 - __main__ - INFO - RAM: 使用中 28.5GB / 95.4GB (29.8%)
2025-05-07 22:41:20,673 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:41:20,673 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:41:20,675 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:41:20,675 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:41:20,675 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:41:20,676 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:41:26,420 - __main__ - INFO - Using device: cuda
2025-05-07 22:41:26,421 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:41:26,421 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:41:26,421 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:41:26,421 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:41:27,030 - __main__ - INFO - Initializing student model
2025-05-07 22:41:28,756 - distillation - INFO - RAM使用状況: 30.9% (使用中: 29.5GB, 空き: 65.9GB)
2025-05-07 22:41:28,756 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 22:41:28,756 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1)
2025-05-07 22:41:30,009 - distillation - INFO - GGUF model loaded successfully
2025-05-07 22:41:30,010 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 22:41:30,010 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 22:41:30,034 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 22:41:30,034 - distillation - INFO - Processing batch 1/1000
2025-05-07 22:41:30,035 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 22:41:50,198 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 22:55:16,946 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 22:55:16,950 - __main__ - INFO - RAM: 使用中 29.1GB / 95.4GB (30.5%)
2025-05-07 22:55:17,008 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 22:55:17,008 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 22:55:17,009 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 22:55:17,009 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 22:55:17,010 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 22:55:17,010 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 22:55:20,490 - __main__ - INFO - Using device: cuda
2025-05-07 22:55:20,491 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 22:55:20,491 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 22:55:20,491 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 22:55:20,491 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 22:55:21,119 - __main__ - INFO - Initializing student model
2025-05-07 22:55:23,179 - distillation - INFO - RAM使用状況: 31.7% (使用中: 30.2GB, 空き: 65.2GB)
2025-05-07 22:55:23,180 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 22:55:23,180 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1)
2025-05-07 22:55:24,434 - distillation - INFO - GGUF model loaded successfully
2025-05-07 22:55:24,434 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 22:55:24,435 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 22:55:24,459 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 22:55:24,462 - distillation - INFO - Processing batch 1/1000
2025-05-07 22:55:24,462 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 22:55:29,995 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:02:21,921 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:02:21,930 - __main__ - INFO - RAM: 使用中 29.5GB / 95.4GB (31.0%)
2025-05-07 23:02:21,993 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:02:21,993 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:02:21,993 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:02:21,993 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:02:21,994 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:02:21,994 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:02:24,326 - __main__ - INFO - Using device: cuda
2025-05-07 23:02:24,327 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:24,327 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:02:24,327 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:02:24,327 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-07 23:02:24,328 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:02:25,043 - __main__ - INFO - Initializing student model
2025-05-07 23:02:27,280 - distillation - INFO - RAM使用状況: 32.1% (使用中: 30.6GB, 空き: 64.8GB)
2025-05-07 23:02:27,281 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:02:27,281 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1, n_gpu=1, n_batch=512)
2025-05-07 23:02:27,281 - distillation - INFO - GGUF: 使用GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:30,524 - distillation - INFO - GGUF: GPUモード動作確認完了
2025-05-07 23:02:30,524 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:02:30,525 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:02:30,525 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:02:30,556 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:02:30,556 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:02:30,556 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:02:34,891 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:02:48,102 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:02:48,110 - __main__ - INFO - RAM: 使用中 29.5GB / 95.4GB (31.0%)
2025-05-07 23:02:48,169 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:02:48,169 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:02:48,170 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:02:48,171 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:02:48,171 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:02:48,171 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:02:53,166 - __main__ - INFO - Using device: cuda
2025-05-07 23:02:53,167 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:53,167 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:02:53,167 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:02:53,167 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-07 23:02:53,168 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:02:53,872 - __main__ - INFO - Initializing student model
2025-05-07 23:02:55,609 - distillation - INFO - RAM使用状況: 32.1% (使用中: 30.6GB, 空き: 64.8GB)
2025-05-07 23:02:55,610 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:02:55,610 - distillation - INFO - GGUF: GPUモードを有効にします (n_gpu_layers=-1, n_gpu=1, n_batch=512)
2025-05-07 23:02:55,610 - distillation - INFO - GGUF: 使用GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:02:58,785 - distillation - INFO - GGUF: GPUモード動作確認完了
2025-05-07 23:02:58,785 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:02:58,786 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:02:58,786 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:02:58,815 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:02:58,816 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:02:58,816 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:06:10,187 - distillation - INFO - Generating GGUF response for question 2/4000
2025-05-07 23:07:09,075 - distillation - INFO - Generating GGUF response for question 3/4000
2025-05-07 23:08:28,585 - distillation - INFO - Generating GGUF response for question 4/4000
2025-05-07 23:09:21,577 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:09:44,042 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:09:44,052 - __main__ - INFO - RAM: 使用中 29.6GB / 95.4GB (31.0%)
2025-05-07 23:09:44,108 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:09:44,109 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:09:44,109 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:09:44,109 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:09:44,109 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:09:44,109 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:09:47,119 - __main__ - INFO - Using device: cuda
2025-05-07 23:09:47,120 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:09:47,120 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:09:47,120 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:09:47,120 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-07 23:09:47,121 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:09:47,790 - __main__ - INFO - Initializing student model
2025-05-07 23:09:49,496 - distillation - INFO - RAM使用状況: 32.1% (使用中: 30.6GB, 空き: 64.8GB)
2025-05-07 23:09:49,496 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:09:49,497 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=-1, n_gpu=1, n_batch=512
2025-05-07 23:09:49,497 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:09:49,497 - distillation - INFO - CUDA バージョン: 12.8
2025-05-07 23:09:49,507 - distillation - INFO - cuDNN バージョン: 90701
2025-05-07 23:09:49,507 - distillation - INFO - CUDA デバイス数: 1
2025-05-07 23:09:49,507 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-07 23:09:49,507 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': -1, 'n_batch': 512, 'offload_kqv': True, 'main_gpu': 0}
2025-05-07 23:09:50,745 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-07 23:09:54,380 - distillation - INFO - テスト推論完了: 処理時間 3.63秒
2025-05-07 23:09:54,390 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-07 23:09:54,390 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-07 23:09:54,390 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:09:54,391 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:09:54,391 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000001C7FD665F90>>
2025-05-07 23:09:54,391 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-07 23:09:54,391 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:09:54,391 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:09:54,415 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:09:54,415 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:09:54,415 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:10:01,881 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-07 23:15:15,163 - __main__ - INFO - Starting knowledge distillation process
2025-05-07 23:15:15,174 - __main__ - INFO - RAM: 使用中 29.6GB / 95.4GB (31.0%)
2025-05-07 23:15:15,233 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-07 23:15:15,233 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-07 23:15:15,234 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-07 23:15:15,234 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-07 23:15:15,234 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-07 23:15:15,235 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-07 23:15:20,929 - __main__ - INFO - Using device: cuda
2025-05-07 23:15:20,930 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:15:20,930 - __main__ - INFO - CUDA Version: 12.8
2025-05-07 23:15:20,930 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-07 23:15:20,930 - __main__ - INFO - GGUF GPU設定: GPU層数=40, GPU数=1, バッチサイズ=512
2025-05-07 23:15:20,930 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-07 23:15:21,535 - __main__ - INFO - Initializing student model
2025-05-07 23:15:23,198 - distillation - INFO - RAM使用状況: 32.2% (使用中: 30.7GB, 空き: 64.7GB)
2025-05-07 23:15:23,198 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:15:23,199 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=40, n_gpu=1, n_batch=512
2025-05-07 23:15:23,199 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-07 23:15:23,199 - distillation - INFO - CUDA バージョン: 12.8
2025-05-07 23:15:23,199 - distillation - INFO - cuDNN バージョン: 90701
2025-05-07 23:15:23,199 - distillation - INFO - CUDA デバイス数: 1
2025-05-07 23:15:23,199 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-07 23:15:23,199 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': 40, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-07 23:15:24,391 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-07 23:15:28,047 - distillation - INFO - テスト推論完了: 処理時間 3.66秒
2025-05-07 23:15:28,048 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-07 23:15:28,048 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-07 23:15:28,048 - distillation - INFO - GGUF model loaded successfully
2025-05-07 23:15:28,048 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q6_K.gguf
2025-05-07 23:15:28,048 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000002190DDF5F90>>
2025-05-07 23:15:28,048 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-07 23:15:28,049 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-07 23:15:28,049 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-07 23:15:28,082 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-07 23:15:28,082 - distillation - INFO - Processing batch 1/1000
2025-05-07 23:15:28,082 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-07 23:15:36,354 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:03:51,109 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:03:51,114 - __main__ - INFO - RAM: 使用中 25.6GB / 95.4GB (26.9%)
2025-05-08 08:03:51,163 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:03:51,163 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-08 08:03:51,164 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-08 08:03:51,164 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:03:51,164 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:03:51,164 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:03:53,553 - __main__ - INFO - Using device: cuda
2025-05-08 08:03:53,554 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:03:53,554 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:03:53,554 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:03:53,554 - __main__ - INFO - GGUF GPU設定: GPU層数=40, GPU数=1, バッチサイズ=512
2025-05-08 08:03:53,555 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-08 08:03:54,659 - __main__ - INFO - Initializing student model
2025-05-08 08:03:55,573 - distillation - INFO - RAM使用状況: 28.0% (使用中: 26.7GB, 空き: 68.7GB)
2025-05-08 08:03:55,574 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:03:55,575 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=40, n_gpu=1, n_batch=512
2025-05-08 08:03:55,576 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:03:55,577 - distillation - INFO - CUDA バージョン: 12.8
2025-05-08 08:03:55,578 - distillation - INFO - cuDNN バージョン: 90701
2025-05-08 08:03:55,579 - distillation - INFO - CUDA デバイス数: 1
2025-05-08 08:03:55,579 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-08 08:03:55,579 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': 40, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-08 08:03:56,566 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-08 08:03:58,827 - distillation - INFO - テスト推論完了: 処理時間 2.26秒
2025-05-08 08:03:58,837 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-08 08:03:58,837 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-08 08:03:58,837 - distillation - INFO - GGUF model loaded successfully
2025-05-08 08:03:58,837 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:03:58,837 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x00000262A77F2080>>
2025-05-08 08:03:58,838 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-08 08:03:58,838 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:03:58,838 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:03:58,864 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:03:58,864 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:03:58,864 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-08 08:04:14,282 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:06:55,277 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:06:55,285 - __main__ - INFO - RAM: 使用中 25.9GB / 95.4GB (27.2%)
2025-05-08 08:06:55,307 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:06:55,307 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:06:55,307 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:06:55,307 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:07:03,100 - __main__ - INFO - Using device: cuda
2025-05-08 08:07:03,100 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:07:03,101 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:07:03,101 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:07:03,101 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:07:04,071 - __main__ - INFO - Initializing student model
2025-05-08 08:07:04,821 - distillation - INFO - RAM使用状況: 28.1% (使用中: 26.8GB, 空き: 68.6GB)
2025-05-08 08:07:04,821 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:07:04,824 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:07:04,825 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:07:04,852 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:07:04,852 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:07:05,019 - __main__ - CRITICAL - 予期しないエラーが発生しました: name 'outputs' is not defined
2025-05-08 08:07:05,022 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 491, in prepare_distillation_data
    for j, output in enumerate(outputs):
NameError: name 'outputs' is not defined

2025-05-08 08:07:05,078 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:10:03,281 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:10:03,286 - __main__ - INFO - RAM: 使用中 26.2GB / 95.4GB (27.5%)
2025-05-08 08:10:03,314 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:10:03,315 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:10:03,315 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:10:03,315 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:10:06,158 - __main__ - INFO - Using device: cuda
2025-05-08 08:10:06,158 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:10:06,158 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:10:06,159 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:10:06,159 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:10:07,014 - __main__ - INFO - Initializing student model
2025-05-08 08:10:07,745 - distillation - INFO - RAM使用状況: 28.4% (使用中: 27.1GB, 空き: 68.3GB)
2025-05-08 08:10:07,745 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:10:07,748 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:10:07,749 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:10:07,777 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:10:07,777 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:10:07,915 - __main__ - CRITICAL - 予期しないエラーが発生しました: 'KnowledgeDistiller' object has no attribute 'teacher_model'
2025-05-08 08:10:07,916 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 489, in prepare_distillation_data
    outputs = self.teacher_model.generate(
AttributeError: 'KnowledgeDistiller' object has no attribute 'teacher_model'

2025-05-08 08:10:07,971 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:15:54,727 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:15:54,732 - __main__ - INFO - RAM: 使用中 24.0GB / 95.4GB (25.2%)
2025-05-08 08:15:54,753 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:15:54,753 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:15:54,753 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:15:54,753 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:16:26,566 - __main__ - INFO - Using device: cuda
2025-05-08 08:16:26,567 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:16:26,567 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:16:26,567 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:16:26,567 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:16:27,482 - __main__ - INFO - Initializing student model
2025-05-08 08:16:29,908 - distillation - INFO - RAM使用状況: 26.0% (使用中: 24.8GB, 空き: 70.6GB)
2025-05-08 08:16:29,908 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:16:29,910 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:17:05,420 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:17:05,422 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:17:05,424 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:17:05,454 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:17:05,454 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:17:28,377 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:17:38,764 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:17:38,771 - __main__ - INFO - RAM: 使用中 24.7GB / 95.4GB (25.9%)
2025-05-08 08:17:38,789 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:17:38,789 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-08 08:17:38,789 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-08 08:17:38,789 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:17:38,789 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:17:38,791 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:17:55,853 - __main__ - INFO - Using device: cuda
2025-05-08 08:17:55,853 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:17:55,854 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:17:55,854 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:17:55,854 - __main__ - INFO - GGUF GPU設定: GPU層数=40, GPU数=1, バッチサイズ=512
2025-05-08 08:17:55,854 - __main__ - INFO - DeepSeek-Qwen系のトークナイザーを使用します
2025-05-08 08:17:57,088 - __main__ - INFO - Initializing student model
2025-05-08 08:17:57,914 - distillation - INFO - RAM使用状況: 27.0% (使用中: 25.8GB, 空き: 69.6GB)
2025-05-08 08:17:57,914 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:17:57,916 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=40, n_gpu=1, n_batch=512
2025-05-08 08:17:57,917 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:17:57,917 - distillation - INFO - CUDA バージョン: 12.8
2025-05-08 08:17:57,918 - distillation - INFO - cuDNN バージョン: 90701
2025-05-08 08:17:57,918 - distillation - INFO - CUDA デバイス数: 1
2025-05-08 08:17:57,918 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-08 08:17:57,918 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': 40, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-08 08:17:58,868 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-08 08:18:01,274 - distillation - INFO - テスト推論完了: 処理時間 2.40秒
2025-05-08 08:18:01,275 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-08 08:18:01,275 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-08 08:18:01,275 - distillation - INFO - GGUF model loaded successfully
2025-05-08 08:18:01,275 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf\cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q5_0.gguf
2025-05-08 08:18:01,275 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000002B163D41F30>>
2025-05-08 08:18:01,275 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-08 08:18:01,276 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:18:01,276 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:18:01,307 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:18:01,307 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:18:01,307 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-08 08:18:15,947 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:20:22,430 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:20:22,437 - __main__ - INFO - RAM: 使用中 24.7GB / 95.4GB (25.9%)
2025-05-08 08:20:22,471 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:20:22,472 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:20:22,472 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:20:22,473 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:20:27,321 - __main__ - INFO - Using device: cuda
2025-05-08 08:20:27,321 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:20:27,322 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:20:27,322 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:20:27,322 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:20:28,163 - __main__ - INFO - Initializing student model
2025-05-08 08:20:28,881 - distillation - INFO - RAM使用状況: 26.8% (使用中: 25.6GB, 空き: 69.8GB)
2025-05-08 08:20:28,881 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:20:28,889 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:20:40,800 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:20:40,800 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:20:40,801 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:20:40,826 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:20:40,826 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:21:33,064 - distillation - INFO - Processing output 1/4 in batch 1
2025-05-08 08:21:33,064 - distillation - INFO - Processing output 2/4 in batch 1
2025-05-08 08:21:33,067 - distillation - INFO - Processing output 3/4 in batch 1
2025-05-08 08:21:33,068 - distillation - INFO - Processing output 4/4 in batch 1
2025-05-08 08:21:33,221 - distillation - INFO - Processing batch 2/1000
2025-05-08 08:22:07,485 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:28:34,756 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:28:34,762 - __main__ - INFO - RAM: 使用中 25.3GB / 95.4GB (26.5%)
2025-05-08 08:28:34,791 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:28:34,792 - __main__ - INFO - GGUFモデルを使用して蒸留データを生成します
2025-05-08 08:28:34,793 - __main__ - INFO - 考えてから出力するタイプのLLM向けの訓練を有効化しました
2025-05-08 08:28:34,794 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:28:34,794 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:28:34,795 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:28:39,804 - __main__ - INFO - Using device: cuda
2025-05-08 08:28:39,805 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:28:39,805 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:28:39,805 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:28:39,805 - __main__ - INFO - GGUF GPU設定: GPU層数=-1, GPU数=1, バッチサイズ=512
2025-05-08 08:28:39,805 - __main__ - INFO - デフォルトのLLaMAトークナイザーを使用します
2025-05-08 08:28:43,308 - __main__ - INFO - Initializing student model
2025-05-08 08:28:43,557 - distillation - INFO - RAM使用状況: 26.8% (使用中: 25.6GB, 空き: 69.8GB)
2025-05-08 08:28:43,557 - distillation - INFO - Loading GGUF model: C:\Users\s-rin\.lmstudio\models\mmnga\Llama-3-ELYZA-JP-8B-gguf\Llama-3-ELYZA-JP-8B-Q8_0.gguf
2025-05-08 08:28:43,558 - distillation - INFO - GGUF GPU設定: use_gpu=True, n_gpu_layers=-1, n_gpu=1, n_batch=512
2025-05-08 08:28:43,559 - distillation - INFO - GGUF: GPU使用を試みます: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:28:43,560 - distillation - INFO - CUDA バージョン: 12.8
2025-05-08 08:28:43,561 - distillation - INFO - cuDNN バージョン: 90701
2025-05-08 08:28:43,561 - distillation - INFO - CUDA デバイス数: 1
2025-05-08 08:28:43,561 - distillation - INFO - llama-cpp-python バージョン 0.3.8 を検出
2025-05-08 08:28:43,562 - distillation - INFO - GPUモードでGGUFモデルを初期化します: {'n_gpu_layers': -1, 'n_batch': 512, 'offload_kqv': False, 'main_gpu': 0}
2025-05-08 08:28:46,152 - distillation - INFO - GPUモード: テスト推論を実行...
2025-05-08 08:28:48,002 - distillation - INFO - テスト推論完了: 処理時間 1.85秒
2025-05-08 08:28:48,002 - distillation - INFO - GPUメモリ: 割り当て 0.00GB, 予約 0.00GB
2025-05-08 08:28:48,002 - distillation - INFO - GGUF: GPUモードで正常に初期化されました
2025-05-08 08:28:48,002 - distillation - INFO - GGUF model loaded successfully
2025-05-08 08:28:48,002 - distillation - INFO - GGUF モデルパス: C:\Users\s-rin\.lmstudio\models\mmnga\Llama-3-ELYZA-JP-8B-gguf\Llama-3-ELYZA-JP-8B-Q8_0.gguf
2025-05-08 08:28:48,003 - distillation - INFO - コンテキスト長: <bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000001BB43C97730>>
2025-05-08 08:28:48,003 - distillation - INFO - GGUF: CPUモードで実行中
2025-05-08 08:28:48,003 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:28:48,003 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:28:48,032 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:28:48,032 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:28:48,032 - distillation - INFO - Generating GGUF response for question 1/4000
2025-05-08 08:28:55,907 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:31:05,615 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:31:05,621 - __main__ - INFO - RAM: 使用中 25.3GB / 95.4GB (26.5%)
2025-05-08 08:31:05,650 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:31:05,650 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:31:05,651 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:31:05,651 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:31:11,419 - __main__ - INFO - Using device: cuda
2025-05-08 08:31:11,419 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:31:11,420 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:31:11,420 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:31:11,420 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:31:12,268 - __main__ - INFO - Initializing student model
2025-05-08 08:31:12,973 - distillation - INFO - RAM使用状況: 27.4% (使用中: 26.1GB, 空き: 69.2GB)
2025-05-08 08:31:12,974 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:31:12,977 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:31:25,058 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:31:25,061 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:31:25,062 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:31:25,089 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:31:25,089 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:31:55,173 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:32:13,003 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:32:13,009 - __main__ - INFO - RAM: 使用中 25.3GB / 95.4GB (26.5%)
2025-05-08 08:32:13,031 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:32:13,032 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:32:13,032 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:32:13,033 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:32:14,985 - __main__ - INFO - Using device: cuda
2025-05-08 08:32:14,985 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:32:14,986 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:32:14,986 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:32:14,986 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:32:15,847 - __main__ - INFO - Initializing student model
2025-05-08 08:32:16,564 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.2GB, 空き: 69.2GB)
2025-05-08 08:32:16,564 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:32:16,568 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:32:28,531 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:32:28,534 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:32:28,535 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:32:28,537 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:32:28,537 - __main__ - CRITICAL - 予期しないエラーが発生しました: expected str, bytes or os.PathLike object, not NoneType
2025-05-08 08:32:28,539 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 446, in prepare_distillation_data
    with open(questions_file, 'r', encoding='utf-8') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

2025-05-08 08:32:28,701 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:35:32,619 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:35:32,626 - __main__ - INFO - RAM: 使用中 25.4GB / 95.4GB (26.6%)
2025-05-08 08:35:32,648 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:35:32,648 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:35:32,648 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:35:32,648 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:35:34,297 - __main__ - INFO - Using device: cuda
2025-05-08 08:35:34,298 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:35:34,298 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:35:34,298 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:35:34,298 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:35:34,786 - __main__ - INFO - Initializing student model
2025-05-08 08:35:35,506 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.2GB, 空き: 69.2GB)
2025-05-08 08:35:35,506 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:35:35,509 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:35:47,521 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:35:47,523 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:35:47,524 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:35:47,525 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:35:47,525 - __main__ - CRITICAL - 予期しないエラーが発生しました: expected str, bytes or os.PathLike object, not NoneType
2025-05-08 08:35:47,528 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 446, in prepare_distillation_data
    with open(questions_file, 'r', encoding='utf-8') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

2025-05-08 08:35:47,683 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:36:10,955 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:36:10,961 - __main__ - INFO - RAM: 使用中 25.4GB / 95.4GB (26.6%)
2025-05-08 08:36:10,996 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:36:10,996 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:36:10,996 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:36:10,996 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:36:12,929 - __main__ - INFO - Using device: cuda
2025-05-08 08:36:12,929 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:36:12,930 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:36:12,930 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:36:12,930 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:36:13,475 - __main__ - INFO - Initializing student model
2025-05-08 08:36:14,219 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.2GB, 空き: 69.2GB)
2025-05-08 08:36:14,219 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:36:14,221 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:36:25,977 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:36:25,977 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:36:25,978 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:36:25,978 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:36:25,979 - __main__ - CRITICAL - 予期しないエラーが発生しました: expected str, bytes or os.PathLike object, not NoneType
2025-05-08 08:36:25,981 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 446, in prepare_distillation_data
    with open(questions_file, 'r', encoding='utf-8') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType

2025-05-08 08:36:26,132 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:37:15,608 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:37:15,616 - __main__ - INFO - RAM: 使用中 25.4GB / 95.4GB (26.6%)
2025-05-08 08:37:15,652 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:37:15,653 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:37:15,653 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:37:15,654 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:37:17,479 - __main__ - INFO - Using device: cuda
2025-05-08 08:37:17,479 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:37:17,479 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:37:17,479 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:37:17,479 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:37:18,010 - __main__ - INFO - Initializing student model
2025-05-08 08:37:18,694 - distillation - INFO - RAM使用状況: 27.5% (使用中: 26.3GB, 空き: 69.1GB)
2025-05-08 08:37:18,695 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:37:18,697 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:37:30,446 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:37:30,446 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:37:30,446 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:37:30,488 - distillation - INFO - Filtered questions: 4000 out of 4000 original questions
2025-05-08 08:37:30,489 - distillation - INFO - Processing batch 1/1000
2025-05-08 08:37:59,283 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 08:43:36,439 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 08:43:36,448 - __main__ - INFO - RAM: 使用中 25.5GB / 95.4GB (26.8%)
2025-05-08 08:43:36,499 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 08:43:36,499 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 08:43:36,500 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 08:43:36,500 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 08:43:38,291 - __main__ - INFO - Using device: cuda
2025-05-08 08:43:38,292 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 08:43:38,292 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 08:43:38,292 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 08:43:38,292 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:43:39,182 - __main__ - INFO - Initializing student model
2025-05-08 08:43:39,880 - distillation - INFO - RAM使用状況: 27.7% (使用中: 26.4GB, 空き: 69.0GB)
2025-05-08 08:43:39,880 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 08:43:39,882 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 08:43:51,721 - distillation - INFO - Teacher model loaded successfully
2025-05-08 08:43:51,724 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 08:43:51,725 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 08:43:51,726 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 08:43:51,727 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 12:53:50,580 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 12:54:03,500 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 12:54:03,505 - __main__ - INFO - RAM: 使用中 25.6GB / 95.4GB (26.8%)
2025-05-08 12:54:03,526 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 12:54:03,526 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 12:54:03,526 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 12:54:03,526 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 12:54:05,523 - __main__ - INFO - Using device: cuda
2025-05-08 12:54:05,523 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 12:54:05,523 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 12:54:05,523 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 12:54:05,524 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:54:06,048 - __main__ - INFO - Initializing student model
2025-05-08 12:54:06,750 - distillation - INFO - RAM使用状況: 27.7% (使用中: 26.5GB, 空き: 68.9GB)
2025-05-08 12:54:06,751 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:54:06,752 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 12:54:18,546 - distillation - INFO - Teacher model loaded successfully
2025-05-08 12:54:18,549 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 12:54:18,550 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 12:54:18,551 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 12:54:18,551 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 12:54:51,793 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 12:56:19,563 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 12:56:19,573 - __main__ - INFO - RAM: 使用中 25.5GB / 95.4GB (26.7%)
2025-05-08 12:56:19,606 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 12:56:19,606 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 12:56:19,607 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 12:56:19,607 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 12:56:27,473 - __main__ - INFO - Using device: cuda
2025-05-08 12:56:27,474 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 12:56:27,475 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 12:56:27,475 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 12:56:27,475 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:56:28,039 - __main__ - INFO - Initializing student model
2025-05-08 12:56:29,275 - distillation - INFO - RAM使用状況: 27.7% (使用中: 26.4GB, 空き: 69.0GB)
2025-05-08 12:56:29,275 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:56:29,277 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 12:56:33,295 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 12:56:43,022 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 12:56:43,032 - __main__ - INFO - RAM: 使用中 25.5GB / 95.4GB (26.7%)
2025-05-08 12:56:43,069 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 12:56:43,069 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 12:56:43,070 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 12:56:43,070 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 12:57:23,487 - __main__ - INFO - Using device: cuda
2025-05-08 12:57:23,488 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 12:57:23,488 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 12:57:23,488 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 12:57:23,489 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:57:24,383 - __main__ - INFO - Initializing student model
2025-05-08 12:57:25,482 - distillation - INFO - RAM使用状況: 27.6% (使用中: 26.3GB, 空き: 69.1GB)
2025-05-08 12:57:25,482 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 12:57:25,484 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 12:57:37,506 - distillation - INFO - Teacher model loaded successfully
2025-05-08 12:57:37,508 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 12:57:37,509 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 12:57:37,510 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 12:57:37,511 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 12:57:50,872 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 13:07:50,003 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 13:07:50,012 - __main__ - INFO - RAM: 使用中 27.5GB / 95.4GB (28.8%)
2025-05-08 13:07:50,032 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 13:07:50,033 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 13:07:50,033 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 13:07:50,033 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 13:11:50,045 - __main__ - INFO - Using device: cuda
2025-05-08 13:11:50,046 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 13:11:50,046 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 13:11:50,046 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 13:11:50,046 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:11:50,858 - __main__ - INFO - Initializing student model
2025-05-08 13:11:51,623 - distillation - INFO - RAM使用状況: 29.9% (使用中: 28.5GB, 空き: 66.8GB)
2025-05-08 13:11:51,623 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:11:51,624 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 13:12:05,082 - distillation - INFO - Teacher model loaded successfully
2025-05-08 13:12:05,082 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 13:12:05,083 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 13:12:05,083 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 13:12:05,083 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 13:12:05,083 - distillation - INFO - 基本的な質問テーマ数: 32
2025-05-08 13:16:06,327 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 13:45:49,039 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 13:45:49,046 - __main__ - INFO - RAM: 使用中 26.6GB / 95.4GB (27.9%)
2025-05-08 13:45:49,069 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 13:45:49,069 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 13:45:49,069 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 13:45:49,069 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 13:45:50,824 - __main__ - INFO - Using device: cuda
2025-05-08 13:45:50,825 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 13:45:50,825 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 13:45:50,825 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 13:45:50,825 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:45:51,362 - __main__ - INFO - Initializing student model
2025-05-08 13:45:53,042 - distillation - INFO - RAM使用状況: 28.8% (使用中: 27.5GB, 空き: 67.9GB)
2025-05-08 13:45:53,042 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 13:45:53,044 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 13:46:05,668 - distillation - INFO - Teacher model loaded successfully
2025-05-08 13:46:05,671 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 13:46:05,672 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 13:46:05,672 - distillation - INFO - Questions file questions.txt not found, generating automatically
2025-05-08 13:46:05,672 - distillation - INFO - 自動的に質問データを生成します
2025-05-08 13:46:05,673 - distillation - INFO - 基本的な質問テーマ数: 30
2025-05-08 13:46:05,673 - distillation - INFO - 質問生成パターン数: 3
2025-05-08 13:46:05,673 - distillation - INFO - 基本的な質問テーマから質問を生成中...
2025-05-08 13:46:05,673 - distillation - INFO - 追加の質問を生成します。現在: 90/4000
2025-05-08 13:46:05,674 - distillation - INFO - 質問をさらに生成します。現在: 180/4000
2025-05-08 13:46:05,676 - distillation - INFO - 生成された質問数: 508
2025-05-08 13:46:05,676 - distillation - WARNING - 要求された 4000 個の質問を生成できませんでした。実際の生成数: 508
2025-05-08 13:46:05,677 - distillation - INFO - 質問をファイルに書き込み中: questions.txt
2025-05-08 13:46:05,678 - distillation - INFO - 質問ファイルの書き込みが完了しました: questions.txt
2025-05-08 13:46:05,680 - distillation - INFO - Filtered questions: 356 out of 508 original questions
2025-05-08 13:46:05,680 - distillation - INFO - Processing batch 1/89
2025-05-08 13:47:01,219 - distillation - INFO - Processing output 1/4 in batch 1
2025-05-08 13:47:01,220 - distillation - INFO - Processing output 2/4 in batch 1
2025-05-08 13:47:01,224 - distillation - INFO - Processing output 3/4 in batch 1
2025-05-08 13:47:01,227 - distillation - INFO - Processing output 4/4 in batch 1
2025-05-08 13:47:01,479 - distillation - INFO - Processing batch 2/89
2025-05-08 13:47:44,846 - distillation - INFO - Processing output 1/4 in batch 2
2025-05-08 13:47:44,848 - distillation - INFO - Processing output 2/4 in batch 2
2025-05-08 13:47:44,849 - distillation - INFO - Processing output 3/4 in batch 2
2025-05-08 13:47:44,851 - distillation - INFO - Processing output 4/4 in batch 2
2025-05-08 13:47:45,091 - distillation - INFO - Processing batch 3/89
2025-05-08 13:48:21,678 - distillation - INFO - Processing output 1/4 in batch 3
2025-05-08 13:48:21,682 - distillation - INFO - Processing output 2/4 in batch 3
2025-05-08 13:48:21,685 - distillation - INFO - Processing output 3/4 in batch 3
2025-05-08 13:48:21,688 - distillation - INFO - Processing output 4/4 in batch 3
2025-05-08 13:48:21,897 - distillation - INFO - Processing batch 4/89
2025-05-08 13:49:28,331 - distillation - INFO - Processing output 1/4 in batch 4
2025-05-08 13:49:28,332 - distillation - INFO - Processing output 2/4 in batch 4
2025-05-08 13:49:28,332 - distillation - INFO - Processing output 3/4 in batch 4
2025-05-08 13:49:28,333 - distillation - INFO - Processing output 4/4 in batch 4
2025-05-08 13:49:28,482 - distillation - INFO - Processing batch 5/89
2025-05-08 13:50:38,081 - distillation - INFO - Processing output 1/4 in batch 5
2025-05-08 13:50:38,083 - distillation - INFO - Processing output 2/4 in batch 5
2025-05-08 13:50:38,084 - distillation - INFO - Processing output 3/4 in batch 5
2025-05-08 13:50:38,085 - distillation - INFO - Processing output 4/4 in batch 5
2025-05-08 13:50:38,308 - distillation - INFO - Processing batch 6/89
2025-05-08 13:51:39,498 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 14:03:43,763 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 14:03:43,772 - __main__ - INFO - RAM: 使用中 29.1GB / 95.4GB (30.5%)
2025-05-08 14:03:43,791 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 14:03:43,791 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 14:03:43,792 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 14:03:43,792 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 14:03:51,989 - __main__ - INFO - Using device: cuda
2025-05-08 14:03:51,990 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 14:03:51,991 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 14:03:51,991 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 14:03:51,991 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:03:52,808 - __main__ - INFO - Initializing student model
2025-05-08 14:03:54,206 - distillation - INFO - RAM使用状況: 31.4% (使用中: 30.0GB, 空き: 65.4GB)
2025-05-08 14:03:54,207 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:03:54,208 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 14:04:07,088 - distillation - INFO - Teacher model loaded successfully
2025-05-08 14:04:07,091 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 14:04:07,092 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 14:04:07,099 - distillation - INFO - Filtered questions: 356 out of 508 original questions
2025-05-08 14:04:07,099 - distillation - INFO - Processing batch 1/89
2025-05-08 14:04:13,961 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
2025-05-08 14:06:05,516 - __main__ - INFO - Starting knowledge distillation process
2025-05-08 14:06:05,521 - __main__ - INFO - RAM: 使用中 29.0GB / 95.4GB (30.4%)
2025-05-08 14:06:05,545 - __main__ - INFO - GPU 0 (NVIDIA GeForce RTX 5070 Ti): 確保 0.0GB / 予約 0.0GB / 合計 15.9GB
2025-05-08 14:06:05,546 - __main__ - INFO - PyTorch nightly版を検出: 2.7.0.dev20250309+cu128
2025-05-08 14:06:05,546 - __main__ - WARNING - PyTorch nightly版ではGPU最適化ライブラリが使用できない場合があります
2025-05-08 14:06:05,546 - __main__ - INFO - --use_direct_gpu オプションを使用することを推奨します
2025-05-08 14:06:37,622 - __main__ - INFO - Using device: cuda
2025-05-08 14:06:37,623 - __main__ - INFO - GPU: NVIDIA GeForce RTX 5070 Ti
2025-05-08 14:06:37,623 - __main__ - INFO - CUDA Version: 12.8
2025-05-08 14:06:37,623 - __main__ - INFO - GPU Memory: 15.9 GB
2025-05-08 14:06:37,623 - __main__ - INFO - Loading tokenizer for elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:06:38,479 - __main__ - INFO - Initializing student model
2025-05-08 14:06:39,565 - distillation - INFO - RAM使用状況: 31.4% (使用中: 30.0GB, 空き: 65.4GB)
2025-05-08 14:06:39,566 - distillation - INFO - Loading teacher model: elyza/Llama-3-ELYZA-JP-8B
2025-05-08 14:06:39,567 - distillation - INFO - Loading teacher model with parameters: {'pretrained_model_name_or_path': 'elyza/Llama-3-ELYZA-JP-8B', 'device_map': {'': 0}, 'load_in_8bit': False, 'load_in_4bit': False, 'quantization_config': None, 'torch_dtype': torch.float16, 'trust_remote_code': False}
2025-05-08 14:06:52,100 - distillation - INFO - Teacher model loaded successfully
2025-05-08 14:06:52,101 - __main__ - INFO - Generating 5000 distillation examples focusing on: highschool,electronics,it
2025-05-08 14:06:52,102 - distillation - INFO - Preparing distillation data from questions.txt
2025-05-08 14:06:52,105 - distillation - INFO - Filtered questions: 356 out of 508 original questions
2025-05-08 14:06:52,105 - distillation - INFO - Processing batch 1/89
2025-05-08 14:07:44,344 - distillation - INFO - Processing output 1/4 in batch 1
2025-05-08 14:07:44,345 - distillation - INFO - Processing output 2/4 in batch 1
2025-05-08 14:07:44,346 - distillation - INFO - Processing output 3/4 in batch 1
2025-05-08 14:07:44,346 - distillation - INFO - Processing output 4/4 in batch 1
2025-05-08 14:07:44,519 - distillation - INFO - Processing batch 2/89
2025-05-08 14:08:50,635 - distillation - INFO - Processing output 1/4 in batch 2
2025-05-08 14:08:50,637 - distillation - INFO - Processing output 2/4 in batch 2
2025-05-08 14:08:50,638 - distillation - INFO - Processing output 3/4 in batch 2
2025-05-08 14:08:50,640 - distillation - INFO - Processing output 4/4 in batch 2
2025-05-08 14:08:50,811 - distillation - INFO - Processing batch 3/89
2025-05-08 14:09:22,472 - distillation - INFO - Processing output 1/4 in batch 3
2025-05-08 14:09:22,475 - distillation - INFO - Processing output 2/4 in batch 3
2025-05-08 14:09:22,476 - distillation - INFO - Processing output 3/4 in batch 3
2025-05-08 14:09:22,477 - distillation - INFO - Processing output 4/4 in batch 3
2025-05-08 14:09:22,688 - distillation - INFO - Processing batch 4/89
2025-05-08 14:10:31,277 - distillation - INFO - Processing output 1/4 in batch 4
2025-05-08 14:10:31,278 - distillation - INFO - Processing output 2/4 in batch 4
2025-05-08 14:10:31,278 - distillation - INFO - Processing output 3/4 in batch 4
2025-05-08 14:10:31,279 - distillation - INFO - Processing output 4/4 in batch 4
2025-05-08 14:10:31,430 - distillation - INFO - Processing batch 5/89
2025-05-08 14:11:28,295 - distillation - INFO - Processing output 1/4 in batch 5
2025-05-08 14:11:28,295 - distillation - INFO - Processing output 2/4 in batch 5
2025-05-08 14:11:28,296 - distillation - INFO - Processing output 3/4 in batch 5
2025-05-08 14:11:28,296 - distillation - INFO - Processing output 4/4 in batch 5
2025-05-08 14:11:28,460 - distillation - INFO - Processing batch 6/89
2025-05-08 14:12:27,712 - distillation - INFO - Processing output 1/4 in batch 6
2025-05-08 14:12:27,713 - distillation - INFO - Processing output 2/4 in batch 6
2025-05-08 14:12:27,714 - distillation - INFO - Processing output 3/4 in batch 6
2025-05-08 14:12:27,714 - distillation - INFO - Processing output 4/4 in batch 6
2025-05-08 14:12:27,904 - distillation - INFO - Processing batch 7/89
2025-05-08 14:13:13,704 - distillation - INFO - Processing output 1/4 in batch 7
2025-05-08 14:13:13,705 - distillation - INFO - Processing output 2/4 in batch 7
2025-05-08 14:13:13,705 - distillation - INFO - Processing output 3/4 in batch 7
2025-05-08 14:13:13,705 - distillation - INFO - Processing output 4/4 in batch 7
2025-05-08 14:13:13,862 - distillation - INFO - Processing batch 8/89
2025-05-08 14:14:21,813 - distillation - INFO - Processing output 1/4 in batch 8
2025-05-08 14:14:21,813 - distillation - INFO - Processing output 2/4 in batch 8
2025-05-08 14:14:21,814 - distillation - INFO - Processing output 3/4 in batch 8
2025-05-08 14:14:21,817 - distillation - INFO - Processing output 4/4 in batch 8
2025-05-08 14:14:21,979 - distillation - INFO - Processing batch 9/89
2025-05-08 14:15:17,375 - distillation - INFO - Processing output 1/4 in batch 9
2025-05-08 14:15:17,377 - distillation - INFO - Processing output 2/4 in batch 9
2025-05-08 14:15:17,377 - distillation - INFO - Processing output 3/4 in batch 9
2025-05-08 14:15:17,378 - distillation - INFO - Processing output 4/4 in batch 9
2025-05-08 14:15:17,534 - distillation - INFO - Processing batch 10/89
2025-05-08 14:16:15,176 - distillation - INFO - Processing output 1/4 in batch 10
2025-05-08 14:16:15,177 - distillation - INFO - Processing output 2/4 in batch 10
2025-05-08 14:16:15,177 - distillation - INFO - Processing output 3/4 in batch 10
2025-05-08 14:16:15,178 - distillation - INFO - Processing output 4/4 in batch 10
2025-05-08 14:16:15,345 - distillation - INFO - Processing batch 11/89
2025-05-08 14:17:07,582 - distillation - INFO - Processing output 1/4 in batch 11
2025-05-08 14:17:07,584 - distillation - INFO - Processing output 2/4 in batch 11
2025-05-08 14:17:07,585 - distillation - INFO - Processing output 3/4 in batch 11
2025-05-08 14:17:07,586 - distillation - INFO - Processing output 4/4 in batch 11
2025-05-08 14:17:07,746 - __main__ - CRITICAL - 予期しないエラーが発生しました: local variable 'json' referenced before assignment
2025-05-08 14:17:07,747 - __main__ - CRITICAL - Traceback (most recent call last):
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 645, in main
    run_distillation(args)
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\main.py", line 463, in run_distillation
    distiller.prepare_distillation_data(
  File "C:\Users\s-rin\Documents\GitHub\lal_v3\distillation.py", line 843, in prepare_distillation_data
    json.dump(distillation_data, f, ensure_ascii=False, indent=2)
UnboundLocalError: local variable 'json' referenced before assignment

2025-05-08 14:17:07,909 - __main__ - INFO - ガベージコレクションを実行してメモリを解放しました
